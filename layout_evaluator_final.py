#!/usr/bin/env python3
"""
layout_evaluator_final.py

√âvaluateur de layouts Overcooked qui fait jouer deux GreedyAgent de mani√®re optimis√©e.
Mesure les performances en termes de temps de compl√©tion, FPS, et actions par agent.

OBJECTIF: √âvaluation pr√©cise et fiable des layouts g√©n√©r√©s pour exp√©riences cognitives.
"""

import os
import glob
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

from overcooked_ai_py.agents.agent import GreedyAgent, AgentGroup
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld
from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv
from overcooked_ai_py.mdp.actions import Action
from overcooked_ai_py.planning.planners import NO_COUNTERS_PARAMS


class StayAgent:
    """Agent qui reste immobile (ne fait que STAY) pour les tests."""
    
    def __init__(self):
        self.agent_index = None
        self.mdp = None
    
    def action(self, state):
        """Retourne toujours l'action STAY."""
        return Action.STAY, {}
    
    def set_agent_index(self, agent_index):
        self.agent_index = agent_index
    
    def set_mdp(self, mdp):
        self.mdp = mdp
    
    def reset(self):
        pass


class LayoutEvaluator:
    """
    √âvaluateur de layouts qui fait jouer deux GreedyAgent et mesure leurs performances.
    """
    
    def __init__(self, layouts_directory: str = "./overcooked_ai_py/data/layouts/generation_cesar/", 
                 horizon: int = 600, num_games_per_layout: int = 5, 
                 target_fps: float = 10.0, max_stuck_frames: int = 50, 
                 single_agent: bool = False, greedy_with_stay: bool = False):
        """
        Initialise l'√©valuateur.
        
        Args:
            layouts_directory: R√©pertoire contenant les fichiers .layout
            horizon: Nombre maximum de steps par partie
            num_games_per_layout: Nombre de parties √† jouer par layout
            target_fps: FPS cible pour la simulation
            max_stuck_frames: Nombre max de frames o√π les agents peuvent √™tre bloqu√©s
            single_agent: Si True, fait jouer un seul GreedyAgent (mode solo pur)
            greedy_with_stay: Si True, fait jouer un GreedyAgent + un StayAgent
        """
        self.layouts_directory = layouts_directory
        self.horizon = horizon
        self.num_games_per_layout = num_games_per_layout
        self.target_fps = target_fps
        self.step_duration = 1.0 / target_fps
        self.max_stuck_frames = max_stuck_frames
        self.single_agent = single_agent
        self.greedy_with_stay = greedy_with_stay
        self.results = {}
        
        # D√©terminer le mode
        if single_agent:
            agent_mode = "1x GreedyAgent (SOLO PUR)"
        elif greedy_with_stay:
            agent_mode = "GreedyAgent + StayAgent"
        else:
            agent_mode = "2x GreedyAgent (COOP)"
            
        print(f"üéÆ √âVALUATEUR DE LAYOUTS OVERCOOKED")
        print(f"ü§ñ Mode: {agent_mode}")
        print(f"üìÅ R√©pertoire: {layouts_directory}")
        print(f"‚è±Ô∏è Horizon: {horizon} steps")
        print(f"üéØ Parties par layout: {num_games_per_layout}")
        print(f"üöÄ FPS cible: {target_fps}")
        print(f"üîí Max stuck frames: {max_stuck_frames}")
    
    def discover_layouts(self) -> List[str]:
        """D√©couvre tous les fichiers .layout dans le r√©pertoire."""
        layout_files = glob.glob(os.path.join(self.layouts_directory, "*.layout"))
        layout_names = [os.path.basename(f).replace('.layout', '') for f in layout_files]
        layout_names.sort()
        
        print(f"‚úÖ {len(layout_names)} layouts d√©couverts: {layout_names}")
        return layout_names
    
    def create_agent_group(self, mdp: OvercookedGridworld) -> Tuple[bool, object]:
        """
        Cr√©e un agent ou groupe d'agents GreedyAgent configur√©s pour le MDP donn√©.
        Returns (success, agent_or_group)
        """
        try:
            # D√©terminer le type d'agents √† cr√©er
            if self.single_agent:
                agent_desc = "1x GreedyAgent (solo pur)"
            elif self.greedy_with_stay:
                agent_desc = "GreedyAgent + StayAgent"
            else:
                agent_desc = "2x GreedyAgent (coop)"
                
            print(f"ü§ñ Cr√©ation: {agent_desc}...")
            
            # S'assurer que le r√©pertoire des planners existe
            planners_dir = f"./overcooked_ai_py/data/planners/generation_cesar/"
            os.makedirs(planners_dir, exist_ok=True)
            
            if self.single_agent:
                # Mode agent seul : un seul GreedyAgent
                agent = GreedyAgent(auto_unstuck=True)
                agent.set_mdp(mdp)
                agent.set_agent_index(0)  # Toujours le premier joueur
                
                print("‚úÖ GreedyAgent (solo pur) cr√©√© avec succ√®s")
                return True, agent
                
            elif self.greedy_with_stay:
                # Mode GreedyAgent + StayAgent
                agent_0 = GreedyAgent(auto_unstuck=True)
                agent_1 = StayAgent()
                
                # Cr√©er le groupe d'agents
                agent_group = AgentGroup(agent_0, agent_1)
                agent_group.set_mdp(mdp)
                
                print("‚úÖ GreedyAgent + StayAgent cr√©√©s avec succ√®s")
                return True, agent_group
                
            else:
                # Mode coop√©ratif : deux GreedyAgent
                agent_0 = GreedyAgent(auto_unstuck=True)
                agent_1 = GreedyAgent(auto_unstuck=True)
                
                # Cr√©er le groupe d'agents
                agent_group = AgentGroup(agent_0, agent_1)
                agent_group.set_mdp(mdp)
                
                print("‚úÖ 2x GreedyAgent (coop) cr√©√©s avec succ√®s")
                return True, agent_group
            
        except Exception as e:
            print(f"‚ùå √âchec cr√©ation agents: {e}")
            print(f"   Tentative de fallback avec RandomAgent...")
            
            # Fallback sur RandomAgent si GreedyAgent √©choue
            try:
                from overcooked_ai_py.agents.agent import RandomAgent
                
                if self.single_agent:
                    agent = RandomAgent()
                    agent.set_mdp(mdp)
                    agent.set_agent_index(0)
                    print("‚úÖ RandomAgent (solo) cr√©√© en fallback")
                    return True, agent
                    
                elif self.greedy_with_stay:
                    agent_0 = RandomAgent()
                    agent_1 = StayAgent()
                    agent_group = AgentGroup(agent_0, agent_1)
                    agent_group.set_mdp(mdp)
                    print("‚úÖ RandomAgent + StayAgent cr√©√©s en fallback")
                    return True, agent_group
                    
                else:
                    agent_0 = RandomAgent()
                    agent_1 = RandomAgent()
                    agent_group = AgentGroup(agent_0, agent_1)
                    agent_group.set_mdp(mdp)
                    print("‚úÖ 2x RandomAgent cr√©√©s en fallback")
                    return True, agent_group
                    
            except Exception as e2:
                print(f"‚ùå √âchec total cr√©ation agents: {e2}")
                return False, None
    
    def simulate_single_game(self, mdp: OvercookedGridworld, agent_or_group: object, 
                           game_id: int = 1) -> Dict:
        """
        Simule une seule partie compl√®te avec un GreedyAgent seul ou deux GreedyAgent.
        
        Returns:
            Dict avec les r√©sultats d√©taill√©s de la partie
        """
        print(f"   üéÆ Partie {game_id} - Simulation...")
        
        # Variables de suivi
        game_start_time = time.time()
        step_count = 0
        total_reward = 0
        completed = False
        stuck_frames = 0
        last_positions = None
        
        # Statistiques d√©taill√©es
        agent_actions_count = [defaultdict(int), defaultdict(int)]  # Actions par agent
        fps_measurements = []
        step_times = []
        
        try:
            # √âtat initial - adapt√© selon le mode
            state = mdp.get_standard_start_state()
            
            if self.single_agent:
                print(f"      ü§ñ Mode SOLO: Joueur 0 actif en {state.players[0].position}, Joueur 1 inactif en {state.players[1].position}")
            else:
                print(f"      ü§ñ Mode NORMAL: Joueurs en {[p.position for p in state.players]}")
            
            # Nombre initial de commandes
            initial_orders = len(state.all_orders)
            completed_orders = 0
            
            print(f"      üìã Commandes initiales: {initial_orders}")
            
            # Boucle de simulation principale
            for step in range(self.horizon):
                step_start_time = time.time()
                
                # Obtenir les actions selon le mode
                if self.single_agent:
                    # Mode agent seul : une action pour le joueur 0, STAY pour le joueur 1 (hors carte)
                    action_0, _ = agent_or_group.action(state)
                    joint_action = [action_0, Action.STAY]  # Le joueur 1 reste toujours STAY
                    
                    # Compter les actions (seul l'agent 0 agit vraiment)
                    agent_actions_count[0][action_0] += 1
                    agent_actions_count[1][Action.STAY] += 1
                    
                elif self.greedy_with_stay:
                    # Mode GreedyAgent + StayAgent : utiliser AgentGroup
                    joint_action_and_infos = agent_or_group.joint_action(state)
                    joint_action = [action_info[0] for action_info in joint_action_and_infos]
                    
                    # Compter les actions des deux agents
                    for agent_idx, action in enumerate(joint_action):
                        agent_actions_count[agent_idx][action] += 1
                else:
                    # Mode coop√©ratif : deux agents actifs
                    joint_action_and_infos = agent_or_group.joint_action(state)
                    joint_action = [action_info[0] for action_info in joint_action_and_infos]
                    
                    # Compter les actions des deux agents
                    for agent_idx, action in enumerate(joint_action):
                        agent_actions_count[agent_idx][action] += 1
                
                # Ex√©cuter l'action avec la logique Overcooked
                next_state, info = mdp.get_state_transition(state, joint_action)
                
                # Calculer la r√©compense
                step_reward = sum(info['sparse_reward_by_agent'])
                total_reward += step_reward
                
                # V√©rifier les commandes compl√©t√©es
                current_orders = len(next_state.all_orders)
                if current_orders < len(state.all_orders):
                    completed_orders += (len(state.all_orders) - current_orders)
                    print(f"      ‚úÖ Commande compl√©t√©e! Total: {completed_orders}")
                
                # V√©rifier si toutes les commandes sont compl√©t√©es
                if len(next_state.all_orders) == 0:
                    completed = True
                    step_count = step + 1
                    print(f"      üèÅ Toutes les commandes compl√©t√©es en {step_count} steps!")
                    break
                
                # V√©rifier si les agents sont bloqu√©s
                if self.single_agent:
                    # Mode solo : v√©rifier seulement le joueur 0 (le joueur 1 est hors carte)
                    current_positions = [next_state.players[0].position]
                else:
                    # Mode normal : v√©rifier tous les joueurs
                    current_positions = [player.position for player in next_state.players]
                    
                if current_positions == last_positions:
                    stuck_frames += 1
                    if stuck_frames >= self.max_stuck_frames:
                        print(f"      ‚ö†Ô∏è Agent{'s' if not self.single_agent else ''} bloqu√©{'s' if not self.single_agent else ''} pendant {stuck_frames} frames, arr√™t forc√©")
                        break
                else:
                    stuck_frames = 0
                last_positions = current_positions
                
                # Passer √† l'√©tat suivant
                state = next_state
                step_count = step + 1
                
                # Mesurer le timing
                step_end_time = time.time()
                step_duration = step_end_time - step_start_time
                step_times.append(step_duration)
                
                if step_duration > 0:
                    fps_measurements.append(1.0 / step_duration)
                
                # Simulation du timing r√©el (optionnel)
                if step_duration < self.step_duration:
                    time.sleep(self.step_duration - step_duration)
        
        except Exception as e:
            print(f"      ‚ùå Erreur pendant simulation: {e}")
            step_count = max(1, step_count)
            # Valeurs par d√©faut en cas d'erreur
            initial_orders = 1
            completed_orders = 0
        
        game_end_time = time.time()
        total_game_time = game_end_time - game_start_time
        
        # Calculer les m√©triques finales
        if 'initial_orders' not in locals():
            initial_orders = 1
        if 'completed_orders' not in locals():
            completed_orders = 0
        
        # Statistiques FPS
        avg_fps = np.mean(fps_measurements) if fps_measurements else 0
        actual_fps = step_count / max(0.001, total_game_time)
        
        # Actions par agent
        total_actions_agent_0 = sum(agent_actions_count[0].values())
        total_actions_agent_1 = sum(agent_actions_count[1].values())
        
        # R√©sultats d√©taill√©s
        game_result = {
            'game_id': game_id,
            'completed': completed,
            'steps': step_count,
            'total_reward': total_reward,
            'orders_completed': completed_orders,
            'initial_orders': initial_orders,
            'completion_rate': completed_orders / max(1, initial_orders),
            'single_agent_mode': self.single_agent,
            'timing': {
                'total_time_seconds': total_game_time,
                'simulated_time_seconds': step_count * self.step_duration,
                'average_step_time': np.mean(step_times) if step_times else 0,
                'target_fps': self.target_fps,
                'measured_avg_fps': avg_fps,
                'actual_fps': actual_fps,
                'steps_per_fps': step_count / max(1, avg_fps) if avg_fps > 0 else 0
            },
            'agent_statistics': {
                'agent_0': {
                    'total_actions': total_actions_agent_0,
                    'actions_per_step': total_actions_agent_0 / max(1, step_count),
                    'action_distribution': {str(k): v for k, v in agent_actions_count[0].items()},
                    'active': True,
                    'agent_type': 'GreedyAgent'
                },
                'agent_1': {
                    'total_actions': total_actions_agent_1,
                    'actions_per_step': total_actions_agent_1 / max(1, step_count),
                    'action_distribution': {str(k): v for k, v in agent_actions_count[1].items()},
                    'active': not self.single_agent,
                    'agent_type': 'StayAgent' if self.greedy_with_stay else ('GreedyAgent' if not self.single_agent else 'Inactive_OutOfBounds')
                }
            },
            'stuck_frames': stuck_frames,
            'stuck_forced_stop': stuck_frames >= self.max_stuck_frames
        }
        
        mode_info = "SOLO PUR" if self.single_agent else ("GREEDY+STAY" if self.greedy_with_stay else "COOP")
        print(f"      üìä R√©sultat [{mode_info}]: {step_count} steps, {completed_orders}/{initial_orders} commandes, "
              f"FPS: {actual_fps:.1f}, temps: {total_game_time:.2f}s")
        
        return game_result
    
    def evaluate_single_layout(self, layout_name: str) -> Dict:
        """√âvalue un seul layout avec plusieurs parties."""
        print(f"\nüèóÔ∏è √âvaluation: {layout_name}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # Charger le MDP
            full_layout_path = f"generation_cesar/{layout_name}"
            mdp = OvercookedGridworld.from_layout_name(full_layout_path)
            
            # Analyser la structure
            structure = self._analyze_layout_structure(mdp)
            
            print(f"üìä Layout: {structure['width']}x{structure['height']}, "
                  f"Commandes: {structure['initial_orders']}")
            
            # Cr√©er les agents
            success, agent_or_group = self.create_agent_group(mdp)
            if not success:
                return {
                    'layout_name': layout_name,
                    'viable': False,
                    'error': 'Impossible de cr√©er les agents GreedyAgent',
                    'evaluation_time': time.time() - start_time
                }
            
            agent_info = ("1x GreedyAgent (solo pur)" if self.single_agent else 
                         "GreedyAgent + StayAgent" if self.greedy_with_stay else 
                         "2x GreedyAgent (coop)")
            print(f"ü§ñ Agents: {agent_info}")
            
            # Simuler toutes les parties
            game_results = []
            total_simulation_time = 0
            
            for game_num in range(1, self.num_games_per_layout + 1):
                # Reset des agents entre les parties
                if self.single_agent:
                    agent_or_group.reset()
                    agent_or_group.set_mdp(mdp)
                    agent_or_group.set_agent_index(0)
                else:
                    agent_or_group.reset()
                    agent_or_group.set_mdp(mdp)
                
                game_result = self.simulate_single_game(mdp, agent_or_group, game_num)
                game_results.append(game_result)
                total_simulation_time += game_result['timing']['total_time_seconds']
                
                # Petite pause entre les parties
                time.sleep(0.1)
            
            # Analyser les r√©sultats globaux
            eval_time = time.time() - start_time
            aggregated_results = self._aggregate_game_results(
                layout_name, structure, game_results, eval_time, total_simulation_time
            )
            
            print(f"‚úÖ √âvaluation termin√©e en {eval_time:.1f}s")
            return aggregated_results
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"‚ùå Erreur lors de l'√©valuation: {e}")
            return {
                'layout_name': layout_name,
                'viable': False,
                'error': str(e),
                'evaluation_time': error_time
            }
    
    def _analyze_layout_structure(self, mdp: OvercookedGridworld) -> Dict:
        """Analyse la structure d'un layout."""
        structure = {
            'width': mdp.width,
            'height': mdp.height,
            'area': mdp.width * mdp.height,
            'initial_orders': len(mdp.start_all_orders) if hasattr(mdp, 'start_all_orders') else 1,
            'player_count': len(mdp.start_player_positions)
        }
        
        # Compter les √©l√©ments du terrain
        terrain_counts = defaultdict(int)
        for row in mdp.terrain_mtx:
            for cell in row:
                terrain_counts[cell] += 1
        
        structure.update({
            'tomato_dispensers': terrain_counts.get('T', 0),
            'onion_dispensers': terrain_counts.get('O', 0),
            'dish_dispensers': terrain_counts.get('D', 0),
            'pots': terrain_counts.get('P', 0),
            'serve_areas': terrain_counts.get('S', 0),
            'counters': terrain_counts.get('X', 0),
            'walls': terrain_counts.get('X', 0)
        })
        
        return structure
    
    def _aggregate_game_results(self, layout_name: str, structure: Dict, 
                              game_results: List[Dict], eval_time: float, 
                              total_sim_time: float) -> Dict:
        """Agr√®ge les r√©sultats de toutes les parties pour un layout."""
        
        # S√©parer les parties compl√©t√©es et non compl√©t√©es
        completed_games = [g for g in game_results if g['completed']]
        
        # M√©triques de base
        steps_list = [g['steps'] for g in game_results]
        rewards_list = [g['total_reward'] for g in game_results]
        
        # Timing et FPS
        actual_fps_list = [g['timing']['actual_fps'] for g in game_results]
        measured_fps_list = [g['timing']['measured_avg_fps'] for g in game_results]
        
        # Actions des agents
        agent_0_actions = [g['agent_statistics']['agent_0']['total_actions'] for g in game_results]
        agent_1_actions = [g['agent_statistics']['agent_1']['total_actions'] for g in game_results]
        
        # Pr√©parer les r√©sultats agr√©g√©s
        results = {
            'layout_name': layout_name,
            'viable': True,
            'evaluation_method': 'greedy_agent_simulation',
            'structure': structure,
            'games_played': len(game_results),
            'games_completed': len(completed_games),
            'completion_rate': len(completed_games) / len(game_results),
            'timing': {
                'total_evaluation_time': eval_time,
                'total_simulation_time': total_sim_time,
                'average_game_duration': total_sim_time / len(game_results)
            }
        }
        
        # M√©triques des agents (toujours deux agents, m√™me si un est inactif)
        results['agent_metrics'] = {
            'agent_0': {
                'average_actions_per_game': np.mean(agent_0_actions),
                'total_actions': sum(agent_0_actions),
                'actions_per_step': sum(agent_0_actions) / sum(steps_list)
            },
            'agent_1': {
                'average_actions_per_game': np.mean(agent_1_actions),
                'total_actions': sum(agent_1_actions),
                'actions_per_step': sum(agent_1_actions) / sum(steps_list)
            }
        }
        
        # M√©triques de compl√©tion (M√âTRIQUE PRINCIPALE)
        if completed_games:
            completed_steps = [g['steps'] for g in completed_games]
            completed_times = [g['timing']['total_time_seconds'] for g in completed_games]
            
            results['completion_metrics'] = {
                'average_completion_steps': np.mean(completed_steps),
                'median_completion_steps': np.median(completed_steps),
                'fastest_completion_steps': min(completed_steps),
                'slowest_completion_steps': max(completed_steps),
                'std_completion_steps': np.std(completed_steps),
                'average_completion_time_seconds': np.mean(completed_times),
                'median_completion_time_seconds': np.median(completed_times)
            }
            
            # M√âTRIQUE PRINCIPALE
            results['primary_metric'] = results['completion_metrics']['average_completion_steps']
            results['primary_metric_name'] = "Temps de compl√©tion moyen (steps)"
            
        else:
            results['primary_metric'] = self.horizon + 100
            results['primary_metric_name'] = "Aucune compl√©tion"
        
        # M√©triques de performance
        results['performance_metrics'] = {
            'average_steps': np.mean(steps_list),
            'median_steps': np.median(steps_list),
            'average_reward': np.mean(rewards_list),
            'total_reward': sum(rewards_list),
            'actual_fps': {
                'average': np.mean(actual_fps_list),
                'median': np.median(actual_fps_list),
                'min': min(actual_fps_list),
                'max': max(actual_fps_list)
            },
            'measured_fps': {
                'average': np.mean(measured_fps_list),
                'median': np.median(measured_fps_list)
            }
        }
        
        # D√©tails des parties individuelles
        results['individual_games'] = game_results
        
        # Affichage des r√©sultats
        if completed_games:
            print(f"üèÅ COMPL√âTION: {len(completed_games)}/{len(game_results)} parties")
            print(f"‚è±Ô∏è Temps moyen: {results['completion_metrics']['average_completion_steps']:.1f} steps")
            print(f"üöÄ Plus rapide: {results['completion_metrics']['fastest_completion_steps']} steps")
            print(f"üìä FPS moyen: {results['performance_metrics']['actual_fps']['average']:.1f}")
            
            # Affichage des actions adapt√© au mode
            if self.single_agent:
                print(f"ü§ñ Actions/step: Agent0={results['agent_metrics']['agent_0']['actions_per_step']:.2f} (mode SOLO), "
                      f"Agent1={results['agent_metrics']['agent_1']['actions_per_step']:.2f} (hors carte)")
            else:
                print(f"ü§ñ Actions/step: Agent0={results['agent_metrics']['agent_0']['actions_per_step']:.2f}, "
                      f"Agent1={results['agent_metrics']['agent_1']['actions_per_step']:.2f}")
        else:
            print(f"‚ùå AUCUNE COMPL√âTION r√©ussie sur {len(game_results)} parties")
        
        return results
    
    def evaluate_all_layouts(self) -> Dict:
        """√âvalue tous les layouts du r√©pertoire."""
        layout_names = self.discover_layouts()
        
        if not layout_names:
            print("‚ùå Aucun layout trouv√©")
            return {}
        
        print(f"\nüöÄ D√âBUT √âVALUATION DE {len(layout_names)} LAYOUTS")
        print("=" * 60)
        
        start_time = time.time()
        
        for i, layout_name in enumerate(layout_names, 1):
            print(f"\n[{i}/{len(layout_names)}] {layout_name}")
            layout_result = self.evaluate_single_layout(layout_name)
            self.results[layout_name] = layout_result
        
        total_time = time.time() - start_time
        self.generate_final_report(total_time)
        
        return self.results
    
    def generate_final_report(self, total_evaluation_time: float):
        """G√©n√®re le rapport final avec toutes les m√©triques."""
        print(f"\nüèÜ RAPPORT FINAL - √âVALUATION LAYOUTS")
        print("=" * 60)
        
        viable_layouts = [name for name, data in self.results.items() if data.get('viable', False)]
        completed_layouts = [name for name in viable_layouts if self.results[name].get('completion_rate', 0) > 0]
        
        print(f"üìä Layouts √©valu√©s: {len(self.results)}")
        print(f"‚úÖ Layouts viables: {len(viable_layouts)}")
        print(f"üèÅ Layouts avec compl√©tion: {len(completed_layouts)}")
        print(f"‚è±Ô∏è Temps total √©valuation: {total_evaluation_time:.1f}s")
        
        if completed_layouts:
            # Classement par temps de compl√©tion
            completion_data = []
            for name in completed_layouts:
                if 'primary_metric' in self.results[name]:
                    steps = self.results[name]['primary_metric']
                    completion_rate = self.results[name]['completion_rate']
                    avg_fps = self.results[name]['performance_metrics']['actual_fps']['average']
                    completion_data.append((name, steps, completion_rate, avg_fps))
            
            if completion_data:
                completion_data.sort(key=lambda x: x[1])  # Tri par steps
                
                print(f"\nüèÅ CLASSEMENT PAR TEMPS DE COMPL√âTION:")
                for i, (name, steps, rate, fps) in enumerate(completion_data, 1):
                    medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                    print(f"   {medal} {name}: {steps:.0f} steps "
                          f"({rate*100:.0f}% r√©ussite, {fps:.1f} FPS)")
                
                # Statistiques finales
                all_steps = [steps for _, steps, _, _ in completion_data]
                all_fps = [fps for _, _, _, fps in completion_data]
                
                print(f"\nüìä STATISTIQUES GLOBALES:")
                print(f"   Temps moyen: {np.mean(all_steps):.1f} steps")
                print(f"   Meilleur temps: {min(all_steps):.1f} steps")
                print(f"   FPS moyen: {np.mean(all_fps):.1f}")
                print(f"   Horizon max: {self.horizon} steps")
        
        print(f"\nüíæ R√©sultats pr√™ts pour sauvegarde")
    
    def save_results(self, filename: str = "layout_evaluation_final.json"):
        """Sauvegarde les r√©sultats d√©taill√©s."""
        output_data = {
            'evaluation_config': {
                'layouts_directory': self.layouts_directory,
                'horizon': self.horizon,
                'num_games_per_layout': self.num_games_per_layout,
                'target_fps': self.target_fps,
                'max_stuck_frames': self.max_stuck_frames
            },
            'evaluation_timestamp': time.time(),
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"üíæ R√©sultats sauvegard√©s dans {filename}")


def main():
    """Fonction principale."""
    import sys
    
    # V√©rifier les arguments pour les diff√©rents modes
    single_agent_mode = '--solo' in sys.argv or '--single' in sys.argv
    greedy_with_stay_mode = '--stay' in sys.argv or '--greedy-stay' in sys.argv
    
    # D√©terminer le mode et la description
    if single_agent_mode:
        mode_description = "1x GreedyAgent (SOLO PUR)"
        filename_suffix = "solo"
    elif greedy_with_stay_mode:
        mode_description = "GreedyAgent + StayAgent"
        filename_suffix = "greedy_stay"
    else:
        mode_description = "2x GreedyAgent (COOP)"
        filename_suffix = "coop"
    
    print("üéÆ √âVALUATEUR DE LAYOUTS OVERCOOKED")
    print(f"ü§ñ Mode: {mode_description}")
    print("=" * 60)
    
    # Configuration
    layouts_dir = "./overcooked_ai_py/data/layouts/generation_cesar/"
    
    if not os.path.exists(layouts_dir):
        print(f"‚ùå R√©pertoire {layouts_dir} non trouv√©")
        return
    
    # Cr√©er l'√©valuateur
    evaluator = LayoutEvaluator(
        layouts_directory=layouts_dir,
        horizon=600,  # Horizon raisonnable
        num_games_per_layout=5,  # Plusieurs parties pour moyenner
        target_fps=10.0,  # FPS mod√©r√©
        max_stuck_frames=50,  # √âviter les blocages infinis
        single_agent=single_agent_mode,  # Mode solo pur
        greedy_with_stay=greedy_with_stay_mode  # Mode GreedyAgent + StayAgent
    )
    
    # Lancer l'√©valuation
    results = evaluator.evaluate_all_layouts()
    
    # Sauvegarder avec un nom diff√©rent selon le mode
    filename = f"layout_evaluation_{filename_suffix}.json"
    evaluator.save_results(filename)
    
    print(f"\nüéØ √âVALUATION TERMIN√âE!")
    print(f"   üìä Mode: {mode_description}")
    print(f"   üìä M√©triques compl√®tes disponibles")
    print(f"   üíæ R√©sultats sauvegard√©s dans {filename}")
    
    print(f"\nüí° MODES DISPONIBLES:")
    print(f"   ‚Ä¢ Mode coop√©ratif (d√©faut): python {sys.argv[0]}")
    print(f"   ‚Ä¢ Mode solo pur: python {sys.argv[0]} --solo")
    print(f"   ‚Ä¢ Mode GreedyAgent + StayAgent: python {sys.argv[0]} --stay")


if __name__ == "__main__":
    main()
