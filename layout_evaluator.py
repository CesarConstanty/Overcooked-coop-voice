#!/usr/bin/env python3
"""
layout_evaluator_final.py

√âvaluateur de layouts Overcooked qui fait jouer deux GreedyAgent de mani√®re optimis√©e.
Mesure les performances en termes de temps de compl√©tion, FPS, et actions par agent.

OBJECTIF: √âvaluation pr√©cise et fiable des layouts g√©n√©r√©s pour exp√©riences cognitives.
"""

import os
import glob
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

from overcooked_ai_py.agents.agent import GreedyAgent, AgentGroup
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld
from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv
from overcooked_ai_py.mdp.actions import Action
from overcooked_ai_py.planning.planners import NO_COUNTERS_PARAMS


class StayAgent:
    """Agent qui reste immobile (ne fait que STAY) pour les tests."""
    
    def __init__(self):
        self.agent_index = None
        self.mdp = None
    
    def action(self, state):
        """Retourne toujours l'action STAY."""
        return Action.STAY, {}
    
    def set_agent_index(self, agent_index):
        self.agent_index = agent_index
    
    def set_mdp(self, mdp):
        self.mdp = mdp
    
    def reset(self):
        pass


class LayoutEvaluator:
    """
    √âvaluateur de layouts qui fait jouer deux GreedyAgent et mesure leurs performances.
    """
    
    def __init__(self, layouts_directory: str = "./overcooked_ai_py/data/layouts/generation_cesar/", 
                 horizon: int = 600, num_games_per_layout: int = 5, 
                 target_fps: float = 10.0, max_stuck_frames: int = 50, 
                 single_agent: bool = False, greedy_with_stay: bool = False,
                 parallel_games: bool = False, max_workers: int = None):
        """
        Initialise l'√©valuateur.
        
        Args:
            layouts_directory: R√©pertoire contenant les fichiers .layout
            horizon: Nombre maximum de steps par partie
            num_games_per_layout: Nombre de parties √† jouer par layout
            target_fps: FPS cible pour la simulation (ignor√© si parallel_games=True)
            max_stuck_frames: Nombre max de frames o√π les agents peuvent √™tre bloqu√©s
            single_agent: Si True, fait jouer un seul GreedyAgent (mode solo pur)
            greedy_with_stay: Si True, fait jouer un GreedyAgent + un StayAgent
            parallel_games: Si True, ex√©cute les parties en parall√®le
            max_workers: Nombre max de processus parall√®les (None = auto)
        """
        self.layouts_directory = layouts_directory
        self.horizon = horizon
        self.num_games_per_layout = num_games_per_layout
        self.target_fps = target_fps
        self.step_duration = 1.0 / target_fps if not parallel_games else 0
        self.max_stuck_frames = max_stuck_frames
        self.single_agent = single_agent
        self.greedy_with_stay = greedy_with_stay
        self.parallel_games = parallel_games
        self.max_workers = max_workers or min(num_games_per_layout, os.cpu_count() or 1)
        self.results = {}
        
        # D√©terminer le mode
        if single_agent:
            agent_mode = "1x GreedyAgent (SOLO PUR)"
        elif greedy_with_stay:
            agent_mode = "GreedyAgent + StayAgent"
        else:
            agent_mode = "2x GreedyAgent (COOP)"
            
        parallel_info = f" [PARALL√àLE: {self.max_workers} workers]" if parallel_games else ""
        speed_info = f"üöÄ FPS cible: {target_fps}" if not parallel_games else "üöÄ Mode: Parall√®le (FPS max)"
            
        print(f"üéÆ √âVALUATEUR DE LAYOUTS OVERCOOKED")
        print(f"ü§ñ Mode: {agent_mode}{parallel_info}")
        print(f"üìÅ R√©pertoire: {layouts_directory}")
        print(f"‚è±Ô∏è Horizon: {horizon} steps")
        print(f"üéØ Parties par layout: {num_games_per_layout}")
        print(f"{speed_info}")
        print(f"üîí Max stuck frames: {max_stuck_frames}")
    
    def discover_layouts(self) -> List[str]:
        """D√©couvre tous les fichiers .layout dans le r√©pertoire."""
        layout_files = glob.glob(os.path.join(self.layouts_directory, "*.layout"))
        layout_names = [os.path.basename(f).replace('.layout', '') for f in layout_files]
        layout_names.sort()
        
        print(f"‚úÖ {len(layout_names)} layouts d√©couverts: {layout_names}")
        return layout_names
    
    def create_agent_group(self, mdp: OvercookedGridworld) -> Tuple[bool, object]:
        """
        Cr√©e un agent ou groupe d'agents GreedyAgent configur√©s pour le MDP donn√©.
        Returns (success, agent_or_group)
        """
        try:
            # D√©terminer le type d'agents √† cr√©er
            if self.single_agent:
                agent_desc = "1x GreedyAgent (solo pur)"
            elif self.greedy_with_stay:
                agent_desc = "GreedyAgent + StayAgent"
            else:
                agent_desc = "2x GreedyAgent (coop)"
                
            print(f"ü§ñ Cr√©ation: {agent_desc}...")
            
            # S'assurer que le r√©pertoire des planners existe
            planners_dir = f"./overcooked_ai_py/data/planners/generation_cesar/"
            os.makedirs(planners_dir, exist_ok=True)
            
            if self.single_agent:
                # Mode agent seul : un seul GreedyAgent
                agent = GreedyAgent(auto_unstuck=True)
                agent.set_mdp(mdp)
                agent.set_agent_index(0)  # Toujours le premier joueur
                
                print("‚úÖ GreedyAgent (solo pur) cr√©√© avec succ√®s")
                return True, agent
                
            elif self.greedy_with_stay:
                # Mode GreedyAgent + StayAgent
                agent_0 = GreedyAgent(auto_unstuck=True)
                agent_1 = StayAgent()
                
                # Cr√©er le groupe d'agents
                agent_group = AgentGroup(agent_0, agent_1)
                agent_group.set_mdp(mdp)
                
                print("‚úÖ GreedyAgent + StayAgent cr√©√©s avec succ√®s")
                return True, agent_group
                
            else:
                # Mode coop√©ratif : deux GreedyAgent
                agent_0 = GreedyAgent(auto_unstuck=True)
                agent_1 = GreedyAgent(auto_unstuck=True)
                
                # Cr√©er le groupe d'agents
                agent_group = AgentGroup(agent_0, agent_1)
                agent_group.set_mdp(mdp)
                
                print("‚úÖ 2x GreedyAgent (coop) cr√©√©s avec succ√®s")
                return True, agent_group
            
        except Exception as e:
            print(f"‚ùå √âchec cr√©ation agents: {e}")
            print(f"   Tentative de fallback avec RandomAgent...")
            
            # Fallback sur RandomAgent si GreedyAgent √©choue
            try:
                from overcooked_ai_py.agents.agent import RandomAgent
                
                if self.single_agent:
                    agent = RandomAgent()
                    agent.set_mdp(mdp)
                    agent.set_agent_index(0)
                    print("‚úÖ RandomAgent (solo) cr√©√© en fallback")
                    return True, agent
                    
                elif self.greedy_with_stay:
                    agent_0 = RandomAgent()
                    agent_1 = StayAgent()
                    agent_group = AgentGroup(agent_0, agent_1)
                    agent_group.set_mdp(mdp)
                    print("‚úÖ RandomAgent + StayAgent cr√©√©s en fallback")
                    return True, agent_group
                    
                else:
                    agent_0 = RandomAgent()
                    agent_1 = RandomAgent()
                    agent_group = AgentGroup(agent_0, agent_1)
                    agent_group.set_mdp(mdp)
                    print("‚úÖ 2x RandomAgent cr√©√©s en fallback")
                    return True, agent_group
                    
            except Exception as e2:
                print(f"‚ùå √âchec total cr√©ation agents: {e2}")
                return False, None
    
    def simulate_single_game(self, mdp: OvercookedGridworld, agent_or_group: object, 
                           game_id: int = 1) -> Dict:
        """
        Simule une seule partie compl√®te avec un GreedyAgent seul ou deux GreedyAgent.
        
        Returns:
            Dict avec les r√©sultats d√©taill√©s de la partie
        """
        print(f"   üéÆ Partie {game_id} - Simulation...")
        
        # Variables de suivi
        game_start_time = time.time()
        step_count = 0
        total_reward = 0
        completed = False
        stuck_frames = 0
        last_positions = None
        
        # Statistiques d√©taill√©es
        agent_actions_count = [defaultdict(int), defaultdict(int)]  # Actions par agent
        fps_measurements = []
        step_times = []
        
        # M√©triques comportementales d√©taill√©es (inspir√©es de game.py)
        event_infos_history = []
        trajectory = []  # Pour stocker la trajectoire compl√®te
        behavioral_metrics = {
            'interactions_count': [0, 0],  # Nombre d'interactions par agent
            'stuck_loops': [0, 0],         # Nombre de fois bloqu√© par agent
            'movement_efficiency': [0, 0],  # Efficacit√© de mouvement
            'total_distance_traveled': [0, 0], # Distance totale parcourue
            'task_completion_order': [],    # Ordre de compl√©tion des t√¢ches
            'collaboration_events': 0,      # √âv√©nements de collaboration
        }
        
        try:
            # √âtat initial - adapt√© selon le mode
            state = mdp.get_standard_start_state()
            
            # Stocker une r√©f√©rence au MDP pour les corrections de bugs
            self._current_mdp = mdp
            
            if self.single_agent:
                print(f"      ü§ñ Mode SOLO: Joueur 0 actif en {state.players[0].position}, Joueur 1 inactif en {state.players[1].position}")
            else:
                print(f"      ü§ñ Mode NORMAL: Joueurs en {[p.position for p in state.players]}")
            
            # Nombre initial de commandes
            initial_orders = len(state.all_orders)
            completed_orders = 0
            
            print(f"      üìã Commandes initiales: {initial_orders}")
            
            # Boucle de simulation principale
            for step in range(self.horizon):
                step_start_time = time.time()
                
                # Obtenir les actions selon le mode
                if self.single_agent:
                    # Mode agent seul : une action pour le joueur 0, STAY pour le joueur 1 (hors carte)
                    action_0, _ = agent_or_group.action(state)
                    joint_action = [action_0, Action.STAY]  # Le joueur 1 reste toujours STAY
                    
                    # Compter les actions (seul l'agent 0 agit vraiment)
                    agent_actions_count[0][action_0] += 1
                    agent_actions_count[1][Action.STAY] += 1
                    
                elif self.greedy_with_stay:
                    # Mode GreedyAgent + StayAgent : utiliser AgentGroup
                    joint_action_and_infos = agent_or_group.joint_action(state)
                    joint_action = [action_info[0] for action_info in joint_action_and_infos]
                    
                    # Compter les actions des deux agents
                    for agent_idx, action in enumerate(joint_action):
                        agent_actions_count[agent_idx][action] += 1
                else:
                    # Mode coop√©ratif : deux agents actifs
                    joint_action_and_infos = agent_or_group.joint_action(state)
                    joint_action = [action_info[0] for action_info in joint_action_and_infos]
                    
                    # Compter les actions des deux agents
                    for agent_idx, action in enumerate(joint_action):
                        agent_actions_count[agent_idx][action] += 1
                
                # Ex√©cuter l'action avec la logique Overcooked
                next_state, info = mdp.get_state_transition(state, joint_action)
                
                # Collecter les m√©triques comportementales
                event_infos = info.get('event_infos', {})
                
                # CORRECTION BUG OVERCOOKED AI: Les pickups de tomates ne sont pas logg√©s
                # Nous devons d√©tecter manuellement quand un agent ramasse une tomate
                self._fix_missing_tomato_pickup_events(event_infos, state, next_state)
                
                event_infos_history.append(event_infos)
                
                # Analyser les interactions
                for agent_idx, action in enumerate(joint_action):
                    if action == Action.INTERACT:
                        behavioral_metrics['interactions_count'][agent_idx] += 1
                    
                    # Calculer la distance parcourue
                    if step > 0:  # Pas de calcul au premier step
                        prev_pos = state.players[agent_idx].position
                        new_pos = next_state.players[agent_idx].position
                        distance = abs(new_pos[0] - prev_pos[0]) + abs(new_pos[1] - prev_pos[1])
                        behavioral_metrics['total_distance_traveled'][agent_idx] += distance
                
                # D√©tecter les √©v√©nements de collaboration (transferts d'objets)
                for event_type in ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange']:
                    if event_type in event_infos:
                        if any(event_infos[event_type]):
                            behavioral_metrics['collaboration_events'] += 1
                
                # Calculer la r√©compense
                step_reward = sum(info['sparse_reward_by_agent'])
                total_reward += step_reward
                
                # V√©rifier les commandes compl√©t√©es
                current_orders = len(next_state.all_orders)
                if current_orders < len(state.all_orders):
                    orders_just_completed = len(state.all_orders) - current_orders
                    completed_orders += orders_just_completed
                    print(f"      ‚úÖ {orders_just_completed} commande(s) compl√©t√©e(s)! Total: {completed_orders}")
                
                # V√©rifier si toutes les commandes sont compl√©t√©es
                if len(next_state.all_orders) == 0:
                    completed = True
                    step_count = step + 1
                    print(f"      üèÅ Toutes les commandes compl√©t√©es en {step_count} steps!")
                    break
                
                # V√©rifier si les agents sont bloqu√©s
                if self.single_agent:
                    # Mode solo : v√©rifier seulement le joueur 0 (le joueur 1 est hors carte)
                    current_positions = [next_state.players[0].position]
                else:
                    # Mode normal : v√©rifier tous les joueurs
                    current_positions = [player.position for player in next_state.players]
                    
                if current_positions == last_positions:
                    stuck_frames += 1
                    if stuck_frames >= self.max_stuck_frames:
                        print(f"      ‚ö†Ô∏è Agent{'s' if not self.single_agent else ''} bloqu√©{'s' if not self.single_agent else ''} pendant {stuck_frames} frames, arr√™t forc√©")
                        break
                else:
                    stuck_frames = 0
                last_positions = current_positions
                
                # Passer √† l'√©tat suivant
                state = next_state
                step_count = step + 1
                
                # Stocker la trajectoire
                trajectory_step = {
                    'timestep': step,
                    'state': {
                        'players': [
                            {
                                'position': list(player.position),
                                'held_object': str(player.held_object) if player.held_object else None
                            } for player in state.players
                        ]
                    },
                    'joint_action': [str(action) for action in joint_action],
                    'reward': step_reward,
                    'event_infos': event_infos
                }
                trajectory.append(trajectory_step)
                
                # Mesurer le timing
                step_end_time = time.time()
                step_duration = step_end_time - step_start_time
                step_times.append(step_duration)
                
                if step_duration > 0:
                    fps_measurements.append(1.0 / step_duration)
                
                # Simulation du timing r√©el d√©sactiv√©e pour optimisation
                # if step_duration < self.step_duration:
                #     time.sleep(self.step_duration - step_duration)
        
        except Exception as e:
            print(f"      ‚ùå Erreur pendant simulation: {e}")
            step_count = max(1, step_count)
            # Valeurs par d√©faut en cas d'erreur
            initial_orders = 1
            completed_orders = 0
        
        game_end_time = time.time()
        total_game_time = game_end_time - game_start_time
        
        # Calculer les m√©triques finales
        if 'initial_orders' not in locals():
            initial_orders = 1
        if 'completed_orders' not in locals():
            completed_orders = 0
        
        # Calculer les m√©triques comportementales agr√©g√©es
        behavioral_summary = self._calculate_behavioral_summary(
            event_infos_history, behavioral_metrics, step_count
        )
        
        # Statistiques FPS
        avg_fps = np.mean(fps_measurements) if fps_measurements else 0
        actual_fps = step_count / max(0.001, total_game_time)
        
        # Actions par agent
        total_actions_agent_0 = sum(agent_actions_count[0].values())
        total_actions_agent_1 = sum(agent_actions_count[1].values())
        
        # Calculer les m√©triques suppl√©mentaires compatibles avec 2_0_0.json
        total_interact_actions = behavioral_metrics['interactions_count']
        
        # R√©sultats d√©taill√©s avec compatibilit√© 2_0_0.json
        game_result = {
            'game_id': game_id,
            'completed': completed,
            'steps': step_count,
            'total_reward': total_reward,
            'orders_completed': completed_orders,
            'initial_orders': initial_orders,
            'completion_rate': completed_orders / max(1, initial_orders),
            'single_agent_mode': self.single_agent,
            
            # Section info_sum compatible avec 2_0_0.json
            'info_sum': behavioral_summary.get('event_summary', {}),
            
            # M√©triques d'actions compatibles avec 2_0_0.json
            'agent_action_count': total_actions_agent_0 if self.single_agent else total_actions_agent_0 + total_actions_agent_1,
            'human_action_count': 0,  # Dans notre cas, pas d'humains
            'agent_interact_count': sum(total_interact_actions),
            'human_interact_count': 0,  # Dans notre cas, pas d'humains
            'agent_stuck_loop': stuck_frames,
            'achieved_orders_len': completed_orders,
            
            'timing': {
                'total_time_seconds': total_game_time,
                'simulated_time_seconds': step_count * self.step_duration,
                'average_step_time': np.mean(step_times) if step_times else 0,
                'target_fps': self.target_fps,
                'measured_avg_fps': avg_fps,
                'actual_fps': actual_fps,
                'steps_per_fps': step_count / max(1, avg_fps) if avg_fps > 0 else 0
            },
            'agent_statistics': {
                'agent_0': {
                    'total_actions': total_actions_agent_0,
                    'actions_per_step': total_actions_agent_0 / max(1, step_count),
                    'action_distribution': {str(k): v for k, v in agent_actions_count[0].items()},
                    'active': True,
                    'agent_type': 'GreedyAgent',
                    'interact_count': total_interact_actions[0],
                    'distance_traveled': behavioral_metrics['total_distance_traveled'][0]
                },
                'agent_1': {
                    'total_actions': total_actions_agent_1,
                    'actions_per_step': total_actions_agent_1 / max(1, step_count),
                    'action_distribution': {str(k): v for k, v in agent_actions_count[1].items()},
                    'active': not self.single_agent,
                    'agent_type': 'StayAgent' if self.greedy_with_stay else ('GreedyAgent' if not self.single_agent else 'Inactive_OutOfBounds'),
                    'interact_count': total_interact_actions[1],
                    'distance_traveled': behavioral_metrics['total_distance_traveled'][1]
                }
            },
            'stuck_frames': stuck_frames,
            'stuck_forced_stop': stuck_frames >= self.max_stuck_frames,
            'behavioral_metrics': behavioral_summary,
            'trajectory': trajectory,  # Ajouter la trajectoire compl√®te
            'layout_name': mdp.layout_name if hasattr(mdp, 'layout_name') else 'unknown',
            'mdp_terrain': [[str(cell) for cell in row] for row in mdp.terrain_mtx],  # Grille r√©ellement utilis√©e
            'start_player_positions': mdp.start_player_positions,  # Positions de d√©part r√©elles
            'layout_structure': {
                'width': mdp.width,
                'height': mdp.height,
                'player_count': len(mdp.start_player_positions)
            }
        }
        
        mode_info = "SOLO PUR" if self.single_agent else ("GREEDY+STAY" if self.greedy_with_stay else "COOP")
        print(f"      üìä R√©sultat [{mode_info}]: {step_count} steps, {completed_orders}/{initial_orders} commandes, "
              f"FPS: {actual_fps:.1f}, temps: {total_game_time:.2f}s")
        
        return game_result
    
    def _fix_missing_tomato_pickup_events(self, event_infos: Dict, prev_state, current_state):
        """
        CORRECTION BUG OVERCOOKED AI: Les √©v√©nements tomato_pickup ne sont pas g√©n√©r√©s
        quand un agent ramasse une tomate depuis un distributeur.
        
        Cette m√©thode d√©tecte manuellement quand un agent ramasse une tomate
        en comparant l'√©tat pr√©c√©dent et l'√©tat actuel.
        """
        # Initialiser les √©v√©nements tomate s'ils n'existent pas
        if 'tomato_pickup' not in event_infos:
            event_infos['tomato_pickup'] = [False, False]
        if 'useful_tomato_pickup' not in event_infos:
            event_infos['useful_tomato_pickup'] = [False, False]
        
        # V√©rifier chaque agent
        for agent_idx in range(len(prev_state.players)):
            prev_player = prev_state.players[agent_idx]
            current_player = current_state.players[agent_idx]
            
            # D√©tecter si l'agent a ramass√© une tomate
            prev_has_tomato = (prev_player.held_object is not None and 
                              prev_player.held_object.name == 'tomato')
            current_has_tomato = (current_player.held_object is not None and 
                                 current_player.held_object.name == 'tomato')
            
            # Si l'agent n'avait pas de tomate avant et en a une maintenant
            if not prev_has_tomato and current_has_tomato:
                # V√©rifier si l'agent est sur un distributeur de tomates
                pos = current_player.position
                terrain_type = None
                try:
                    # Acc√©der au terrain via le MDP (si disponible)
                    if hasattr(self, '_current_mdp') and self._current_mdp:
                        terrain_type = self._current_mdp.get_terrain_type(pos)
                    else:
                        # Fallback : d√©tecter bas√© sur la logique (toute acquisition de tomate = pickup)
                        terrain_type = 'T'  # Assumer distributeur de tomates
                except:
                    terrain_type = 'T'  # Fallback s√©curis√©
                
                if terrain_type == 'T' or current_has_tomato:  # Distributeur ou d√©tection g√©n√©rale
                    # D√©clencher l'√©v√©nement tomato_pickup
                    event_infos['tomato_pickup'][agent_idx] = True
                    
                    # Pour simplifier, consid√©rer tous les pickups de tomates comme utiles
                    # (dans la vraie logique d'Overcooked, cela d√©pend de l'√©tat des pots)
                    event_infos['useful_tomato_pickup'][agent_idx] = True
    
    def _calculate_behavioral_summary(self, event_infos_history: List[Dict], 
                                    behavioral_metrics: Dict, step_count: int) -> Dict:
        """
        Calcule un r√©sum√© des m√©triques comportementales bas√© sur l'historique des √©v√©nements.
        Compatible avec le format 2_0_0.json: retourne 'event_summary' qui correspond √† 'info_sum'
        
        IMPORTANT: En mode solo, tous les √©v√©nements sont attribu√©s √† l'agent 0 uniquement.
        """
        # Agr√©ger tous les event_infos comme dans game.py et 2_0_0.json
        event_summary = {}
        
        # Initialiser avec tous les types d'√©v√©nements (compatible 2_0_0.json)
        from overcooked_ai_py.mdp.overcooked_mdp import EVENT_TYPES
        for event_type in EVENT_TYPES:
            event_summary[event_type] = [0, 0]  # [agent_0, agent_1]
        
        # Compter les √©v√©nements pour chaque agent
        for event_info in event_infos_history:
            for event_type, agent_bools in event_info.items():
                if event_type in event_summary:
                    for agent_idx, occurred in enumerate(agent_bools):
                        if occurred and agent_idx < 2:
                            # Attribution normale : chaque agent garde ses propres √©v√©nements
                            event_summary[event_type][agent_idx] += 1
        
        # Calculer des m√©triques d√©riv√©es
        efficiency_metrics = self._calculate_efficiency_metrics(event_summary, behavioral_metrics, step_count)
        
        return {
            'event_summary': event_summary,  # Ce champ sera utilis√© pour 'info_sum'
            'interactions_count': behavioral_metrics['interactions_count'],
            'total_distance_traveled': behavioral_metrics['total_distance_traveled'],
            'collaboration_events': behavioral_metrics['collaboration_events'],
            'efficiency_metrics': efficiency_metrics
        }
    
    def _calculate_efficiency_metrics(self, event_summary: Dict, 
                                    behavioral_metrics: Dict, step_count: int) -> Dict:
        """
        Calcule des m√©triques d'efficacit√© comportementale.
        """
        efficiency_metrics = {}
        
        for agent_idx in range(2):
            # Efficacit√© des pickups (utiles vs totaux)
            total_pickups = (event_summary.get('tomato_pickup', [0, 0])[agent_idx] + 
                           event_summary.get('onion_pickup', [0, 0])[agent_idx] + 
                           event_summary.get('dish_pickup', [0, 0])[agent_idx] + 
                           event_summary.get('soup_pickup', [0, 0])[agent_idx])
            
            useful_pickups = (event_summary.get('useful_tomato_pickup', [0, 0])[agent_idx] + 
                            event_summary.get('useful_onion_pickup', [0, 0])[agent_idx] + 
                            event_summary.get('useful_dish_pickup', [0, 0])[agent_idx])
            
            pickup_efficiency = useful_pickups / max(1, total_pickups) if total_pickups > 0 else 0
            
            # Efficacit√© de potting (optimal + viable vs total)
            total_potting = (event_summary.get('potting_tomato', [0, 0])[agent_idx] + 
                           event_summary.get('potting_onion', [0, 0])[agent_idx])
            
            optimal_potting = (event_summary.get('optimal_tomato_potting', [0, 0])[agent_idx] + 
                             event_summary.get('optimal_onion_potting', [0, 0])[agent_idx])
            
            viable_potting = (event_summary.get('viable_tomato_potting', [0, 0])[agent_idx] + 
                            event_summary.get('viable_onion_potting', [0, 0])[agent_idx])
            
            potting_efficiency = (optimal_potting + viable_potting) / max(1, total_potting) if total_potting > 0 else 0
            
            # Actions par step
            actions_per_step = behavioral_metrics['interactions_count'][agent_idx] / max(1, step_count)
            
            # Mouvement par step
            movement_per_step = behavioral_metrics['total_distance_traveled'][agent_idx] / max(1, step_count)
            
            efficiency_metrics[f'agent_{agent_idx}'] = {
                'pickup_efficiency': pickup_efficiency,
                'potting_efficiency': potting_efficiency,
                'actions_per_step': actions_per_step,
                'movement_per_step': movement_per_step,
                'total_pickups': total_pickups,
                'useful_pickups': useful_pickups,
                'total_potting': total_potting,
                'optimal_potting': optimal_potting,
                'soup_deliveries': event_summary.get('soup_delivery', [0, 0])[agent_idx]
            }
        
        # M√©triques globales d'√©quipe
        total_soup_deliveries = sum(event_summary.get('soup_delivery', [0, 0]))
        total_collaboration = behavioral_metrics['collaboration_events']
        
        efficiency_metrics['team'] = {
            'total_soup_deliveries': total_soup_deliveries,
            'collaboration_events': total_collaboration,
            'collaboration_per_delivery': total_collaboration / max(1, total_soup_deliveries)
        }
        
        return efficiency_metrics
    
    def _aggregate_behavioral_metrics(self, game_results: List[Dict]) -> Dict:
        """
        Caract√©rise pr√©cis√©ment comment les GreedyAgent(s) compl√®tent ce layout sp√©cifique.
        Focus sur les patterns comportementaux caract√©ristiques plut√¥t que sur l'agr√©gation.
        """
        from overcooked_ai_py.mdp.overcooked_mdp import EVENT_TYPES
        
        # Analyser seulement les parties compl√©t√©es pour comprendre les strat√©gies gagnantes
        completed_games = [g for g in game_results if g.get('completed', False)]
        
        if not completed_games:
            return {
                'completion_analysis': 'no_successful_completion',
                'layout_solvability': 'unsolvable_with_current_agents',
                'total_attempts': len(game_results)
            }
        
        # ANALYSE DES PATTERNS DE COMPLETION R√âUSSIS
        completion_patterns = self._analyze_completion_patterns(completed_games)
        
        # CARACT√âRISATION DU LAYOUT
        layout_characteristics = self._characterize_layout_behavior(completed_games)
        
        # IDENTIFICATION DES STRAT√âGIES OPTIMALES
        optimal_strategies = self._identify_optimal_strategies(completed_games)
        
        # M√âTRIQUES DE COH√âRENCE (toutes les parties compl√©t√©es utilisent-elles la m√™me strat√©gie ?)
        strategy_consistency = self._analyze_strategy_consistency(completed_games)
        
        return {
            'completion_analysis': 'successful_completion_found',
            'total_attempts': len(game_results),
            'successful_completions': len(completed_games),
            'success_rate': len(completed_games) / len(game_results),
            
            # Comment ce layout sp√©cifique est r√©solu
            'completion_patterns': completion_patterns,
            'layout_characteristics': layout_characteristics,
            'optimal_strategies': optimal_strategies,
            'strategy_consistency': strategy_consistency,
            
            # Variabilit√© entre les tentatives r√©ussies
            'completion_variability': self._calculate_completion_variability(completed_games)
        }
    
    def _analyze_completion_patterns(self, completed_games: List[Dict]) -> Dict:
        """
        Analyse les patterns sp√©cifiques de compl√©tion pour ce layout.
        Comment les agents r√©solvent-ils ce niveau particulier ?
        """
        patterns = {
            'temporal_progression': [],  # Progression temporelle des √©v√©nements cl√©s
            'critical_events_sequence': [],  # S√©quence d'√©v√©nements critiques
            'coordination_patterns': [],  # Patterns de coordination entre agents
            'efficiency_progression': []  # Evolution de l'efficacit√© au cours du temps
        }
        
        for game in completed_games:
            if 'behavioral_metrics' not in game:
                continue
                
            bm = game['behavioral_metrics']
            
            # Analyser la progression temporelle des livraisons
            if 'event_summary' in bm:
                soup_deliveries = sum(bm['event_summary'].get('soup_delivery', [0, 0]))
                steps = game.get('steps', 1)
                
                # Pattern temporel de ce jeu
                temporal_pattern = {
                    'total_steps': steps,
                    'soup_deliveries': soup_deliveries,
                    'deliveries_per_step': soup_deliveries / steps,
                    'efficiency_score': soup_deliveries / max(steps / 100, 1),  # Normalised
                    'agent_balance': self._calculate_agent_balance(bm['event_summary'])
                }
                patterns['temporal_progression'].append(temporal_pattern)
                
                # Identifier les √©v√©nements critiques
                critical_events = self._identify_critical_event_sequence(bm['event_summary'])
                patterns['critical_events_sequence'].append(critical_events)
                
                # Patterns de coordination
                coordination = self._analyze_coordination_pattern(bm['event_summary'])
                patterns['coordination_patterns'].append(coordination)
        
        # Caract√©riser le pattern dominant pour ce layout
        if patterns['temporal_progression']:
            # Identifier le pattern de compl√©tion le plus repr√©sentatif
            dominant_pattern = self._identify_dominant_completion_pattern(patterns)
            patterns['dominant_strategy'] = dominant_pattern
        
        return patterns
    
    def _characterize_layout_behavior(self, completed_games: List[Dict]) -> Dict:
        """
        Caract√©rise les comportements sp√©cifiques √† ce layout.
        Quelles sont les contraintes et opportunit√©s de ce niveau ?
        """
        characteristics = {
            'layout_difficulty': 'unknown',
            'required_coordination_level': 'unknown', 
            'bottleneck_identification': [],
            'optimal_agent_roles': {},
            'layout_specific_strategies': []
        }
        
        if not completed_games:
            return characteristics
        
        # Analyser la difficult√© bas√©e sur les steps n√©cessaires
        steps_needed = [g.get('steps', 0) for g in completed_games]
        avg_steps = np.mean(steps_needed)
        
        if avg_steps < 200:
            characteristics['layout_difficulty'] = 'easy'
        elif avg_steps < 400:
            characteristics['layout_difficulty'] = 'moderate' 
        else:
            characteristics['layout_difficulty'] = 'hard'
        
        # Analyser le niveau de coordination requis
        coordination_levels = []
        role_specializations = []
        
        for game in completed_games:
            if 'behavioral_metrics' not in game:
                continue
                
            bm = game['behavioral_metrics']
            if 'event_summary' not in bm:
                continue
                
            events = bm['event_summary']
            
            # Mesurer la coordination par les √©changes d'objets
            exchanges = ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange']
            total_exchanges = sum(sum(events.get(ex, [0, 0])) for ex in exchanges)
            coordination_levels.append(total_exchanges)
            
            # Analyser la sp√©cialisation des r√¥les
            agent_0_soups = events.get('soup_delivery', [0, 0])[0]
            agent_1_soups = events.get('soup_delivery', [0, 0])[1]
            total_soups = agent_0_soups + agent_1_soups
            
            if total_soups > 0:
                specialization = abs(agent_0_soups - agent_1_soups) / total_soups
                role_specializations.append(specialization)
                
                # Identifier les r√¥les sp√©cifiques
                if agent_0_soups > agent_1_soups * 1.5:
                    characteristics['optimal_agent_roles']['agent_0'] = 'primary_deliverer'
                    characteristics['optimal_agent_roles']['agent_1'] = 'support_prep'
                elif agent_1_soups > agent_0_soups * 1.5:
                    characteristics['optimal_agent_roles']['agent_0'] = 'support_prep'
                    characteristics['optimal_agent_roles']['agent_1'] = 'primary_deliverer'
                else:
                    characteristics['optimal_agent_roles']['both'] = 'balanced_cooperation'
        
        # D√©terminer le niveau de coordination requis
        if coordination_levels:
            avg_coordination = np.mean(coordination_levels)
            if avg_coordination < 2:
                characteristics['required_coordination_level'] = 'minimal'
            elif avg_coordination < 5:
                characteristics['required_coordination_level'] = 'moderate'
            else:
                characteristics['required_coordination_level'] = 'high'
        
        # Identifier les strat√©gies sp√©cifiques √† ce layout
        characteristics['layout_specific_strategies'] = self._identify_layout_strategies(completed_games)
        
        return characteristics
    
    def _identify_optimal_strategies(self, completed_games: List[Dict]) -> Dict:
        """
        Identifie les strat√©gies optimales pour compl√©ter ce layout sp√©cifique.
        """
        strategies = {
            'fastest_completion_strategy': None,
            'most_efficient_strategy': None,
            'most_consistent_strategy': None,
            'strategy_recommendations': []
        }
        
        if not completed_games:
            return strategies
        
        # Identifier la strat√©gie de compl√©tion la plus rapide
        fastest_game = min(completed_games, key=lambda g: g.get('steps', float('inf')))
        strategies['fastest_completion_strategy'] = self._extract_strategy_profile(fastest_game)
        
        # Identifier la strat√©gie la plus efficace (meilleur ratio r√©compense/steps)
        efficiency_scores = []
        for game in completed_games:
            steps = game.get('steps', 1)
            reward = game.get('total_reward', 0)
            efficiency_scores.append((reward / steps, game))
        
        if efficiency_scores:
            most_efficient_game = max(efficiency_scores, key=lambda x: x[0])[1]
            strategies['most_efficient_strategy'] = self._extract_strategy_profile(most_efficient_game)
        
        # G√©n√©rer des recommandations strat√©giques
        strategies['strategy_recommendations'] = self._generate_strategy_recommendations(completed_games)
        
        return strategies
    
    def _analyze_strategy_consistency(self, completed_games: List[Dict]) -> Dict:
        """
        Analyse la coh√©rence des strat√©gies utilis√©es pour ce layout.
        Les diff√©rentes tentatives r√©ussies utilisent-elles des approches similaires ?
        """
        consistency = {
            'strategy_variance': 0,
            'dominant_pattern_percentage': 0,
            'layout_determinism': 'unknown',
            'alternative_strategies_count': 0
        }
        
        if len(completed_games) < 2:
            consistency['layout_determinism'] = 'insufficient_data'
            return consistency
        
        # Extraire les profils de strat√©gie de chaque jeu
        strategy_profiles = []
        for game in completed_games:
            profile = self._extract_strategy_profile(game)
            strategy_profiles.append(profile)
        
        # Calculer la variance entre les strat√©gies
        consistency['strategy_variance'] = self._calculate_strategy_variance(strategy_profiles)
        
        # Identifier les patterns dominants
        patterns = self._cluster_strategy_patterns(strategy_profiles)
        if patterns:
            dominant_pattern_size = max(len(pattern) for pattern in patterns)
            consistency['dominant_pattern_percentage'] = dominant_pattern_size / len(completed_games)
            consistency['alternative_strategies_count'] = len(patterns)
        
        # D√©terminer le d√©terminisme du layout
        if consistency['strategy_variance'] < 0.2:
            consistency['layout_determinism'] = 'highly_deterministic'
        elif consistency['strategy_variance'] < 0.5:
            consistency['layout_determinism'] = 'moderately_deterministic'
        else:
            consistency['layout_determinism'] = 'multiple_viable_strategies'
        
        return consistency
    
    def _calculate_completion_variability(self, completed_games: List[Dict]) -> Dict:
        """
        Calcule la variabilit√© entre les diff√©rentes tentatives de compl√©tion r√©ussies.
        """
        variability = {
            'steps_variability': 0,
            'strategy_diversity': 0,
            'performance_consistency': 0
        }
        
        if len(completed_games) < 2:
            return variability
        
        # Variabilit√© en nombre de steps
        steps = [g.get('steps', 0) for g in completed_games]
        variability['steps_variability'] = np.std(steps) / np.mean(steps) if np.mean(steps) > 0 else 0
        
        # Diversit√© des strat√©gies
        strategy_profiles = [self._extract_strategy_profile(g) for g in completed_games]
        variability['strategy_diversity'] = self._calculate_strategy_diversity(strategy_profiles)
        
        # Coh√©rence des performances
        rewards = [g.get('total_reward', 0) for g in completed_games]
        variability['performance_consistency'] = 1 - (np.std(rewards) / np.mean(rewards)) if np.mean(rewards) > 0 else 0
        
        return variability
    
    def _calculate_agent_balance(self, event_summary: Dict) -> Dict:
        """Calcule l'√©quilibre entre les agents pour ce layout."""
        soup_deliveries = event_summary.get('soup_delivery', [0, 0])
        total_soups = sum(soup_deliveries)
        
        if total_soups == 0:
            return {'balance_type': 'no_deliveries', 'balance_score': 0}
        
        balance_score = min(soup_deliveries) / max(soup_deliveries) if max(soup_deliveries) > 0 else 0
        
        if balance_score > 0.8:
            balance_type = 'highly_balanced'
        elif balance_score > 0.5:
            balance_type = 'moderately_balanced'
        else:
            balance_type = 'specialized_roles'
            
        return {
            'balance_type': balance_type,
            'balance_score': balance_score,
            'agent_0_contribution': soup_deliveries[0] / total_soups,
            'agent_1_contribution': soup_deliveries[1] / total_soups
        }
    
    def _identify_critical_event_sequence(self, event_summary: Dict) -> Dict:
        """Identifie la s√©quence d'√©v√©nements critiques pour ce layout."""
        critical_events = {
            'pickup_phase': sum(event_summary.get('tomato_pickup', [0, 0])) + sum(event_summary.get('onion_pickup', [0, 0])),
            'preparation_phase': sum(event_summary.get('potting_tomato', [0, 0])) + sum(event_summary.get('potting_onion', [0, 0])),
            'cooking_phase': sum(event_summary.get('soup_pickup', [0, 0])),
            'delivery_phase': sum(event_summary.get('soup_delivery', [0, 0]))
        }
        
        # Identifier le goulot d'√©tranglement
        phase_ratios = {}
        total_deliveries = critical_events['delivery_phase']
        if total_deliveries > 0:
            for phase, count in critical_events.items():
                phase_ratios[phase] = count / total_deliveries
        
        return {
            'event_counts': critical_events,
            'phase_efficiency': phase_ratios,
            'bottleneck_phase': min(phase_ratios.items(), key=lambda x: x[1])[0] if phase_ratios else 'unknown'
        }
    
    def _analyze_coordination_pattern(self, event_summary: Dict) -> Dict:
        """Analyse les patterns de coordination sp√©cifiques."""
        exchanges = ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange']
        total_exchanges = sum(sum(event_summary.get(ex, [0, 0])) for ex in exchanges)
        
        deliveries = sum(event_summary.get('soup_delivery', [0, 0]))
        
        if deliveries == 0:
            return {'coordination_type': 'no_completion', 'exchanges_per_delivery': 0}
        
        exchanges_per_delivery = total_exchanges / deliveries
        
        if exchanges_per_delivery < 0.5:
            coordination_type = 'minimal_coordination'
        elif exchanges_per_delivery < 2:
            coordination_type = 'moderate_coordination'
        else:
            coordination_type = 'high_coordination'
            
        return {
            'coordination_type': coordination_type,
            'exchanges_per_delivery': exchanges_per_delivery,
            'total_exchanges': total_exchanges,
            'exchange_breakdown': {ex: sum(event_summary.get(ex, [0, 0])) for ex in exchanges}
        }
    
    def _identify_dominant_completion_pattern(self, patterns: Dict) -> Dict:
        """Identifie le pattern de compl√©tion dominant pour ce layout."""
        if not patterns['temporal_progression']:
            return {'pattern_type': 'no_pattern_found'}
        
        # Analyser les efficacit√©s pour identifier le pattern optimal
        efficiencies = [tp['efficiency_score'] for tp in patterns['temporal_progression']]
        steps = [tp['total_steps'] for tp in patterns['temporal_progression']]
        
        # Le pattern dominant est celui qui combine rapidit√© et efficacit√©
        best_idx = 0
        best_score = 0
        
        for i, (eff, step) in enumerate(zip(efficiencies, steps)):
            # Score combin√© : efficacit√© √©lev√©e ET steps faibles
            combined_score = eff * (1000 / max(step, 100))  # Favorise moins de steps
            if combined_score > best_score:
                best_score = combined_score
                best_idx = i
        
        dominant = patterns['temporal_progression'][best_idx]
        dominant['pattern_identification'] = 'optimal_speed_efficiency_combination'
        
        return dominant
    
    def _identify_layout_strategies(self, completed_games: List[Dict]) -> List[str]:
        """Identifie les strat√©gies sp√©cifiques utilis√©es pour ce layout."""
        strategies = []
        
        for game in completed_games:
            if 'behavioral_metrics' not in game:
                continue
                
            bm = game['behavioral_metrics']
            if 'event_summary' not in bm:
                continue
                
            events = bm['event_summary']
            
            # Identifier les strat√©gies bas√©es sur les patterns d'√©v√©nements
            agent_0_pickups = events.get('tomato_pickup', [0, 0])[0] + events.get('onion_pickup', [0, 0])[0]
            agent_1_pickups = events.get('tomato_pickup', [0, 0])[1] + events.get('onion_pickup', [0, 0])[1]
            
            agent_0_deliveries = events.get('soup_delivery', [0, 0])[0]
            agent_1_deliveries = events.get('soup_delivery', [0, 0])[1]
            
            # Strat√©gie de sp√©cialisation
            if agent_0_deliveries > agent_1_deliveries * 2:
                strategies.append('agent_0_specializes_delivery')
            elif agent_1_deliveries > agent_0_deliveries * 2:
                strategies.append('agent_1_specializes_delivery')
            else:
                strategies.append('balanced_delivery_roles')
            
            # Strat√©gie de pr√©paration
            if agent_0_pickups > agent_1_pickups * 2:
                strategies.append('agent_0_specializes_preparation')
            elif agent_1_pickups > agent_0_pickups * 2:
                strategies.append('agent_1_specializes_preparation')
            else:
                strategies.append('shared_preparation_duties')
            
            # Niveau de coordination
            exchanges = sum(sum(events.get(ex, [0, 0])) for ex in ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange'])
            total_deliveries = sum(events.get('soup_delivery', [0, 0]))
            
            if exchanges / max(total_deliveries, 1) > 1:
                strategies.append('high_coordination_strategy')
            else:
                strategies.append('low_coordination_strategy')
        
        # Retourner les strat√©gies dominantes (les plus fr√©quentes)
        from collections import Counter
        strategy_counts = Counter(strategies)
        dominant_strategies = [strategy for strategy, count in strategy_counts.most_common(3)]
        
        return dominant_strategies
    
    def _extract_strategy_profile(self, game: Dict) -> Dict:
        """Extrait un profil de strat√©gie complet d'un jeu."""
        profile = {
            'completion_time': game.get('steps', 0),
            'total_reward': game.get('total_reward', 0),
            'agent_balance': {},
            'coordination_level': 0,
            'efficiency_metrics': {}
        }
        
        if 'behavioral_metrics' in game and 'event_summary' in game['behavioral_metrics']:
            events = game['behavioral_metrics']['event_summary']
            
            # Balance entre agents
            soup_deliveries = events.get('soup_delivery', [0, 0])
            total_soups = sum(soup_deliveries)
            if total_soups > 0:
                profile['agent_balance'] = {
                    'agent_0_share': soup_deliveries[0] / total_soups,
                    'agent_1_share': soup_deliveries[1] / total_soups,
                    'balance_score': min(soup_deliveries) / max(soup_deliveries) if max(soup_deliveries) > 0 else 0
                }
            
            # Niveau de coordination
            exchanges = sum(sum(events.get(ex, [0, 0])) for ex in ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange'])
            profile['coordination_level'] = exchanges / max(total_soups, 1)
            
            # M√©triques d'efficacit√©
            if 'efficiency_metrics' in game['behavioral_metrics']:
                em = game['behavioral_metrics']['efficiency_metrics']
                profile['efficiency_metrics'] = {
                    'avg_pickup_efficiency': (em.get('agent_0', {}).get('pickup_efficiency', 0) + em.get('agent_1', {}).get('pickup_efficiency', 0)) / 2,
                    'avg_potting_efficiency': (em.get('agent_0', {}).get('potting_efficiency', 0) + em.get('agent_1', {}).get('potting_efficiency', 0)) / 2
                }
        
        return profile
    
    def _calculate_strategy_variance(self, strategy_profiles: List[Dict]) -> float:
        """Calcule la variance entre diff√©rents profils de strat√©gie."""
        if len(strategy_profiles) < 2:
            return 0
        
        # Normaliser et comparer les m√©triques cl√©s
        completion_times = [p['completion_time'] for p in strategy_profiles]
        coordination_levels = [p['coordination_level'] for p in strategy_profiles]
        
        # Variance normalis√©e
        time_variance = np.std(completion_times) / np.mean(completion_times) if np.mean(completion_times) > 0 else 0
        coord_variance = np.std(coordination_levels) / max(np.mean(coordination_levels), 0.1)
        
        return (time_variance + coord_variance) / 2
    
    def _cluster_strategy_patterns(self, strategy_profiles: List[Dict]) -> List[List[Dict]]:
        """Regroupe les profils de strat√©gie similaires."""
        if len(strategy_profiles) < 2:
            return [strategy_profiles] if strategy_profiles else []
        
        # Clustering simple bas√© sur la similarit√© des m√©triques
        clusters = []
        similarity_threshold = 0.3
        
        for profile in strategy_profiles:
            placed = False
            for cluster in clusters:
                # V√©rifier la similarit√© avec le premier √©l√©ment du cluster
                if self._calculate_profile_similarity(profile, cluster[0]) > similarity_threshold:
                    cluster.append(profile)
                    placed = True
                    break
            
            if not placed:
                clusters.append([profile])
        
        return clusters
    
    def _calculate_profile_similarity(self, profile1: Dict, profile2: Dict) -> float:
        """Calcule la similarit√© entre deux profils de strat√©gie."""
        similarity_score = 0
        comparisons = 0
        
        # Comparer les temps de compl√©tion (normalis√©)
        if profile1['completion_time'] > 0 and profile2['completion_time'] > 0:
            time_similarity = 1 - abs(profile1['completion_time'] - profile2['completion_time']) / max(profile1['completion_time'], profile2['completion_time'])
            similarity_score += time_similarity
            comparisons += 1
        
        # Comparer les niveaux de coordination
        coord_diff = abs(profile1['coordination_level'] - profile2['coordination_level'])
        coord_similarity = max(0, 1 - coord_diff)
        similarity_score += coord_similarity
        comparisons += 1
        
        # Comparer la balance des agents
        if profile1['agent_balance'] and profile2['agent_balance']:
            balance_diff = abs(profile1['agent_balance'].get('balance_score', 0) - profile2['agent_balance'].get('balance_score', 0))
            balance_similarity = max(0, 1 - balance_diff)
            similarity_score += balance_similarity
            comparisons += 1
        
        return similarity_score / max(comparisons, 1)
    
    def _calculate_strategy_diversity(self, strategy_profiles: List[Dict]) -> float:
        """Calcule la diversit√© des strat√©gies utilis√©es."""
        if len(strategy_profiles) < 2:
            return 0
        
        total_similarity = 0
        comparisons = 0
        
        for i in range(len(strategy_profiles)):
            for j in range(i + 1, len(strategy_profiles)):
                similarity = self._calculate_profile_similarity(strategy_profiles[i], strategy_profiles[j])
                total_similarity += similarity
                comparisons += 1
        
        avg_similarity = total_similarity / max(comparisons, 1)
        return 1 - avg_similarity  # Diversit√© = 1 - similarit√© moyenne
    
    def _generate_strategy_recommendations(self, completed_games: List[Dict]) -> List[str]:
        """G√©n√®re des recommandations strat√©giques bas√©es sur les parties r√©ussies."""
        recommendations = []
        
        if not completed_games:
            return ['No successful completions found - layout may be too difficult']
        
        # Analyser les patterns de succ√®s
        fastest_steps = min(g.get('steps', float('inf')) for g in completed_games)
        avg_steps = np.mean([g.get('steps', 0) for g in completed_games])
        
        if fastest_steps < avg_steps * 0.8:
            recommendations.append(f'Optimal completion possible in {fastest_steps} steps - focus on speed strategies')
        
        # Analyser les patterns de coordination
        high_coord_games = []
        low_coord_games = []
        
        for game in completed_games:
            if 'behavioral_metrics' in game and 'event_summary' in game['behavioral_metrics']:
                events = game['behavioral_metrics']['event_summary']
                exchanges = sum(sum(events.get(ex, [0, 0])) for ex in ['tomato_exchange', 'onion_exchange', 'dish_exchange', 'soup_exchange'])
                deliveries = sum(events.get('soup_delivery', [0, 0]))
                
                if exchanges / max(deliveries, 1) > 1:
                    high_coord_games.append(game)
                else:
                    low_coord_games.append(game)
        
        if high_coord_games and low_coord_games:
            high_coord_avg = np.mean([g.get('steps', 0) for g in high_coord_games])
            low_coord_avg = np.mean([g.get('steps', 0) for g in low_coord_games])
            
            if high_coord_avg < low_coord_avg:
                recommendations.append('High coordination strategy recommended - agent exchanges improve efficiency')
            else:
                recommendations.append('Independent agent strategy recommended - minimal coordination needed')
        elif high_coord_games:
            recommendations.append('Layout requires high coordination between agents')
        elif low_coord_games:
            recommendations.append('Layout solvable with independent agent strategies')
        
        return recommendations
    
    def _calculate_average_efficiency(self, game_results: List[Dict]) -> Dict:
        """
        Calcule l'efficacit√© moyenne sur toutes les parties.
        """
        efficiency_sums = {
            'agent_0': {'pickup_efficiency': 0, 'potting_efficiency': 0, 'actions_per_step': 0, 'movement_per_step': 0},
            'agent_1': {'pickup_efficiency': 0, 'potting_efficiency': 0, 'actions_per_step': 0, 'movement_per_step': 0},
            'team': {'collaboration_per_delivery': 0}
        }
        
        valid_games = 0
        
        for game in game_results:
            if 'behavioral_metrics' in game and 'efficiency_metrics' in game['behavioral_metrics']:
                em = game['behavioral_metrics']['efficiency_metrics']
                valid_games += 1
                
                for agent in ['agent_0', 'agent_1']:
                    if agent in em:
                        for metric in efficiency_sums[agent]:
                            efficiency_sums[agent][metric] += em[agent].get(metric, 0)
                
                if 'team' in em:
                    efficiency_sums['team']['collaboration_per_delivery'] += em['team'].get('collaboration_per_delivery', 0)
        
        # Calculer les moyennes
        if valid_games > 0:
            for agent in ['agent_0', 'agent_1']:
                for metric in efficiency_sums[agent]:
                    efficiency_sums[agent][metric] /= valid_games
            
            efficiency_sums['team']['collaboration_per_delivery'] /= valid_games
        
        return efficiency_sums
    
    def _generate_behavioral_insights(self, aggregated_events: Dict, total_interactions: List, 
                                    total_distance: List, num_games: int) -> Dict:
        """
        G√©n√®re des insights comportementaux bas√©s sur les donn√©es agr√©g√©es.
        """
        insights = {}
        
        # Analyse de la sp√©cialisation des agents
        agent_0_soups = aggregated_events.get('soup_delivery', [0, 0])[0]
        agent_1_soups = aggregated_events.get('soup_delivery', [0, 0])[1]
        total_soups = agent_0_soups + agent_1_soups
        
        if total_soups > 0:
            insights['task_distribution'] = {
                'agent_0_soup_share': agent_0_soups / total_soups,
                'agent_1_soup_share': agent_1_soups / total_soups,
                'specialization_index': abs(agent_0_soups - agent_1_soups) / total_soups
            }
        
        # Analyse de l'efficacit√© comparative
        if total_interactions[0] > 0 and total_interactions[1] > 0:
            interaction_ratio = total_interactions[0] / total_interactions[1]
            insights['agent_activity'] = {
                'interaction_ratio_0_to_1': interaction_ratio,
                'more_active_agent': 0 if interaction_ratio > 1 else 1,
                'activity_balance': min(interaction_ratio, 1/interaction_ratio)  # Plus proche de 1 = plus √©quilibr√©
            }
        
        # Analyse des types d'actions dominantes
        pickup_events = ['tomato_pickup', 'onion_pickup', 'dish_pickup', 'soup_pickup']
        potting_events = ['potting_tomato', 'potting_onion']
        
        total_pickups = sum(aggregated_events.get(event, [0, 0])[0] + aggregated_events.get(event, [0, 0])[1] 
                           for event in pickup_events)
        total_pottings = sum(aggregated_events.get(event, [0, 0])[0] + aggregated_events.get(event, [0, 0])[1] 
                            for event in potting_events)
        
        insights['action_patterns'] = {
            'total_pickups': total_pickups,
            'total_pottings': total_pottings,
            'pickups_per_game': total_pickups / num_games,
            'pottings_per_game': total_pottings / num_games,
            'pickup_to_potting_ratio': total_pickups / max(1, total_pottings)
        }
        
        return insights
    
    def evaluate_single_layout(self, layout_name: str) -> Dict:
        """√âvalue un seul layout avec plusieurs parties."""
        print(f"\nüèóÔ∏è √âvaluation: {layout_name}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # Charger le MDP
            full_layout_path = f"generation_cesar/{layout_name}"
            mdp = OvercookedGridworld.from_layout_name(full_layout_path)
            
            # V√©rifier la coh√©rence de la grille charg√©e
            grid_verification = self._verify_layout_grid_consistency(layout_name, mdp)
            if grid_verification['has_discrepancy']:
                print(f"‚ö†Ô∏è DIVERGENCE D√âTECT√âE: La grille charg√©e diff√®re du fichier .layout pour {layout_name}")
                for issue in grid_verification['issues']:
                    print(f"   - {issue}")
            
            # Analyser la structure
            structure = self._analyze_layout_structure(mdp)
            
            print(f"üìä Layout: {structure['width']}x{structure['height']}, "
                  f"Commandes: {structure['initial_orders']}")
            
            # Cr√©er les agents
            success, agent_or_group = self.create_agent_group(mdp)
            if not success:
                return {
                    'layout_name': layout_name,
                    'viable': False,
                    'error': 'Impossible de cr√©er les agents GreedyAgent',
                    'evaluation_time': time.time() - start_time
                }
            
            agent_info = ("1x GreedyAgent (solo pur)" if self.single_agent else 
                         "GreedyAgent + StayAgent" if self.greedy_with_stay else 
                         "2x GreedyAgent (coop)")
            print(f"ü§ñ Agents: {agent_info}")
            
            # Simuler toutes les parties
            game_results = []
            total_simulation_time = 0
            
            if self.parallel_games and self.num_games_per_layout > 1:
                # Ex√©cution en parall√®le
                print(f"üöÄ Simulation en parall√®le avec {self.max_workers} workers...")
                
                # Pr√©parer la configuration pour les workers
                evaluator_config = {
                    'layouts_directory': self.layouts_directory,
                    'horizon': self.horizon,
                    'num_games_per_layout': 1,  # Chaque worker fait 1 partie
                    'target_fps': self.target_fps,
                    'max_stuck_frames': self.max_stuck_frames,
                    'single_agent': self.single_agent,
                    'greedy_with_stay': self.greedy_with_stay,
                    'parallel_games': False  # D√©sactiver le parall√©lisme dans les workers
                }
                
                # Ex√©cuter les parties en parall√®le
                with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                    # Soumettre toutes les t√¢ches
                    future_to_game_id = {
                        executor.submit(simulate_game_parallel, layout_name, game_num, evaluator_config): game_num
                        for game_num in range(1, self.num_games_per_layout + 1)
                    }
                    
                    # Collecter les r√©sultats
                    for future in as_completed(future_to_game_id):
                        game_id = future_to_game_id[future]
                        try:
                            game_result = future.result()
                            if 'error' not in game_result:
                                game_results.append(game_result)
                                total_simulation_time += game_result['timing']['total_time_seconds']
                                print(f"   ‚úÖ Partie {game_id} termin√©e: {game_result['steps']} steps")
                            else:
                                print(f"   ‚ùå Partie {game_id} √©chou√©e: {game_result['error']}")
                        except Exception as exc:
                            print(f"   ‚ùå Partie {game_id} a g√©n√©r√© une exception: {exc}")
                
            else:
                # Ex√©cution s√©quentielle (mode classique)
                for game_num in range(1, self.num_games_per_layout + 1):
                    # Reset des agents entre les parties
                    if self.single_agent:
                        agent_or_group.reset()
                        agent_or_group.set_mdp(mdp)
                        agent_or_group.set_agent_index(0)
                    else:
                        agent_or_group.reset()
                        agent_or_group.set_mdp(mdp)
                    
                    game_result = self.simulate_single_game(mdp, agent_or_group, game_num)
                    game_results.append(game_result)
                    total_simulation_time += game_result['timing']['total_time_seconds']
            
            # Analyser les r√©sultats globaux
            eval_time = time.time() - start_time
            aggregated_results = self._aggregate_game_results(
                layout_name, structure, game_results, eval_time, total_simulation_time
            )
            
            print(f"‚úÖ √âvaluation termin√©e en {eval_time:.1f}s")
            return aggregated_results
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"‚ùå Erreur lors de l'√©valuation: {e}")
            return {
                'layout_name': layout_name,
                'viable': False,
                'error': str(e),
                'evaluation_time': error_time
            }
    
    def _analyze_layout_structure(self, mdp: OvercookedGridworld) -> Dict:
        """Analyse la structure d'un layout."""
        structure = {
            'width': mdp.width,
            'height': mdp.height,
            'area': mdp.width * mdp.height,
            'initial_orders': len(mdp.start_all_orders) if hasattr(mdp, 'start_all_orders') else 1,
            'player_count': len(mdp.start_player_positions)
        }
        
        # Compter les √©l√©ments du terrain
        terrain_counts = defaultdict(int)
        for row in mdp.terrain_mtx:
            for cell in row:
                terrain_counts[cell] += 1
        
        structure.update({
            'tomato_dispensers': terrain_counts.get('T', 0),
            'onion_dispensers': terrain_counts.get('O', 0),
            'dish_dispensers': terrain_counts.get('D', 0),
            'pots': terrain_counts.get('P', 0),
            'serve_areas': terrain_counts.get('S', 0),
            'counters': terrain_counts.get('X', 0),
            'walls': terrain_counts.get('X', 0)
        })
        
        return structure
    
    def _aggregate_game_results(self, layout_name: str, structure: Dict, 
                              game_results: List[Dict], eval_time: float, 
                              total_sim_time: float) -> Dict:
        """Agr√®ge les r√©sultats de toutes les parties pour un layout."""
        
        # S√©parer les parties compl√©t√©es et non compl√©t√©es
        completed_games = [g for g in game_results if g['completed']]
        
        # M√©triques de base
        steps_list = [g['steps'] for g in game_results]
        rewards_list = [g['total_reward'] for g in game_results]
        
        # Timing et FPS
        actual_fps_list = [g['timing']['actual_fps'] for g in game_results]
        measured_fps_list = [g['timing']['measured_avg_fps'] for g in game_results]
        
        # Actions des agents
        agent_0_actions = [g['agent_statistics']['agent_0']['total_actions'] for g in game_results]
        agent_1_actions = [g['agent_statistics']['agent_1']['total_actions'] for g in game_results]
        
        # Agr√©gation des m√©triques comportementales
        behavioral_aggregation = self._aggregate_behavioral_metrics(game_results)
        
        # Pr√©parer les r√©sultats agr√©g√©s
        results = {
            'layout_name': layout_name,
            'viable': True,
            'evaluation_method': 'greedy_agent_simulation',
            'structure': structure,
            'games_played': len(game_results),
            'games_completed': len(completed_games),
            'completion_rate': len(completed_games) / len(game_results),
            'timing': {
                'total_evaluation_time': eval_time,
                'total_simulation_time': total_sim_time,
                'average_game_duration': total_sim_time / len(game_results)
            }
        }
        
        # M√©triques des agents (toujours deux agents, m√™me si un est inactif)
        results['agent_metrics'] = {
            'agent_0': {
                'average_actions_per_game': np.mean(agent_0_actions),
                'total_actions': sum(agent_0_actions),
                'actions_per_step': sum(agent_0_actions) / sum(steps_list)
            },
            'agent_1': {
                'average_actions_per_game': np.mean(agent_1_actions),
                'total_actions': sum(agent_1_actions),
                'actions_per_step': sum(agent_1_actions) / sum(steps_list)
            }
        }
        
        # M√©triques de compl√©tion (M√âTRIQUE PRINCIPALE)
        if completed_games:
            completed_steps = [g['steps'] for g in completed_games]
            completed_times = [g['timing']['total_time_seconds'] for g in completed_games]
            
            results['completion_metrics'] = {
                'average_completion_steps': np.mean(completed_steps),
                'median_completion_steps': np.median(completed_steps),
                'fastest_completion_steps': min(completed_steps),
                'slowest_completion_steps': max(completed_steps),
                'std_completion_steps': np.std(completed_steps),
                'average_completion_time_seconds': np.mean(completed_times),
                'median_completion_time_seconds': np.median(completed_times)
            }
            
            # M√âTRIQUE PRINCIPALE
            results['primary_metric'] = results['completion_metrics']['average_completion_steps']
            results['primary_metric_name'] = "Temps de compl√©tion moyen (steps)"
            
        else:
            results['primary_metric'] = self.horizon + 100
            results['primary_metric_name'] = "Aucune compl√©tion"
        
        # M√©triques de performance
        results['performance_metrics'] = {
            'average_steps': np.mean(steps_list),
            'median_steps': np.median(steps_list),
            'average_reward': np.mean(rewards_list),
            'total_reward': sum(rewards_list),
            'actual_fps': {
                'average': np.mean(actual_fps_list),
                'median': np.median(actual_fps_list),
                'min': min(actual_fps_list),
                'max': max(actual_fps_list)
            },
            'measured_fps': {
                'average': np.mean(measured_fps_list),
                'median': np.median(measured_fps_list)
            }
        }
        
        # Ajouter les m√©triques comportementales agr√©g√©es
        results['behavioral_analysis'] = behavioral_aggregation
        
        # D√©tails des parties individuelles (compatible avec 2_0_0.json)
        results['individual_games'] = game_results
        
        # Affichage des r√©sultats
        if completed_games:
            print(f"üèÅ COMPL√âTION: {len(completed_games)}/{len(game_results)} parties")
            print(f"‚è±Ô∏è Temps moyen: {results['completion_metrics']['average_completion_steps']:.1f} steps")
            print(f"üöÄ Plus rapide: {results['completion_metrics']['fastest_completion_steps']} steps")
            print(f"üìä FPS moyen: {results['performance_metrics']['actual_fps']['average']:.1f}")
            
            # Affichage des actions adapt√© au mode
            if self.single_agent:
                print(f"ü§ñ Actions/step: Agent0={results['agent_metrics']['agent_0']['actions_per_step']:.2f} (mode SOLO), "
                      f"Agent1={results['agent_metrics']['agent_1']['actions_per_step']:.2f} (hors carte)")
            else:
                print(f"ü§ñ Actions/step: Agent0={results['agent_metrics']['agent_0']['actions_per_step']:.2f}, "
                      f"Agent1={results['agent_metrics']['agent_1']['actions_per_step']:.2f}")
            
            # Affichage des m√©triques comportementales sp√©cifiques au layout
            if 'behavioral_analysis' in results:
                ba = results['behavioral_analysis']
                print(f"üéØ Analyse comportementale du layout:")
                
                if ba.get('completion_analysis') == 'successful_completion_found':
                    # Strat√©gies optimales identifi√©es
                    if 'optimal_strategies' in ba and 'strategy_recommendations' in ba['optimal_strategies']:
                        recs = ba['optimal_strategies']['strategy_recommendations']
                        if recs:
                            print(f"   üìã Recommandations: {recs[0]}")
                    
                    # Caract√©ristiques du layout
                    if 'layout_characteristics' in ba:
                        lc = ba['layout_characteristics']
                        print(f"   üéÆ Difficult√©: {lc.get('layout_difficulty', 'unknown')}")
                        print(f"   ü§ù Coordination requise: {lc.get('required_coordination_level', 'unknown')}")
                        
                        if 'optimal_agent_roles' in lc:
                            roles = lc['optimal_agent_roles']
                            if 'both' in roles:
                                print(f"   üë• R√¥les optimaux: {roles['both']}")
                            else:
                                print(f"   üë• R√¥les optimaux: Agent0={roles.get('agent_0', 'unknown')}, Agent1={roles.get('agent_1', 'unknown')}")
                    
                    # Patterns de compl√©tion
                    if 'completion_patterns' in ba and 'dominant_strategy' in ba['completion_patterns']:
                        ds = ba['completion_patterns']['dominant_strategy']
                        if 'efficiency_score' in ds:
                            print(f"   ‚ö° Score d'efficacit√© optimal: {ds['efficiency_score']:.2f}")
                        if 'agent_balance' in ds:
                            balance = ds['agent_balance']
                            print(f"   ‚öñÔ∏è √âquilibre optimal: {balance.get('balance_type', 'unknown')} (score: {balance.get('balance_score', 0):.2f})")
                    
                    # Coh√©rence des strat√©gies
                    if 'strategy_consistency' in ba:
                        sc = ba['strategy_consistency']
                        print(f"   üéØ D√©terminisme du layout: {sc.get('layout_determinism', 'unknown')}")
                        if sc.get('alternative_strategies_count', 0) > 1:
                            print(f"   üîÄ Strat√©gies alternatives trouv√©es: {sc['alternative_strategies_count']}")
                        else:
                            print(f"   üéØ Strat√©gie unique identifi√©e")
                
                else:
                    print(f"   ‚ùå Aucune compl√©tion r√©ussie - layout potentiellement trop difficile")
                
                # Statistiques de base toujours affich√©es
                total_soups = 0
                if 'completion_patterns' in ba and ba['completion_patterns'].get('temporal_progression'):
                    tp = ba['completion_patterns']['temporal_progression'][0]  # Premier jeu r√©ussi
                    total_soups = tp.get('soup_deliveries', 0)
                    print(f"   üç≤ Soupes livr√©es (exemple r√©ussi): {total_soups}")
            else:
                # Fallback vers les anciennes m√©triques si les nouvelles ne sont pas disponibles
                print(f"üéØ M√©triques comportementales basiques:")
                print(f"   ‚ö†Ô∏è Analyse comportementale avanc√©e non disponible")
        else:
            print(f"‚ùå AUCUNE COMPL√âTION r√©ussie sur {len(game_results)} parties")
        
        return results
    
    def evaluate_all_layouts(self) -> Dict:
        """√âvalue tous les layouts du r√©pertoire."""
        layout_names = self.discover_layouts()
        
        if not layout_names:
            print("‚ùå Aucun layout trouv√©")
            return {}
        
        print(f"\nüöÄ D√âBUT √âVALUATION DE {len(layout_names)} LAYOUTS")
        print("=" * 60)
        
        start_time = time.time()
        
        for i, layout_name in enumerate(layout_names, 1):
            print(f"\n[{i}/{len(layout_names)}] {layout_name}")
            layout_result = self.evaluate_single_layout(layout_name)
            self.results[layout_name] = layout_result
        
        total_time = time.time() - start_time
        self.generate_final_report(total_time)
        
        return self.results
    
    def generate_final_report(self, total_evaluation_time: float):
        """G√©n√®re le rapport final avec toutes les m√©triques."""
        print(f"\nüèÜ RAPPORT FINAL - √âVALUATION LAYOUTS")
        print("=" * 60)
        
        viable_layouts = [name for name, data in self.results.items() if data.get('viable', False)]
        completed_layouts = [name for name in viable_layouts if self.results[name].get('completion_rate', 0) > 0]
        
        print(f"üìä Layouts √©valu√©s: {len(self.results)}")
        print(f"‚úÖ Layouts viables: {len(viable_layouts)}")
        print(f"üèÅ Layouts avec compl√©tion: {len(completed_layouts)}")
        print(f"‚è±Ô∏è Temps total √©valuation: {total_evaluation_time:.1f}s")
        
        if completed_layouts:
            # Classement par temps de compl√©tion
            completion_data = []
            for name in completed_layouts:
                if 'primary_metric' in self.results[name]:
                    steps = self.results[name]['primary_metric']
                    completion_rate = self.results[name]['completion_rate']
                    avg_fps = self.results[name]['performance_metrics']['actual_fps']['average']
                    completion_data.append((name, steps, completion_rate, avg_fps))
            
            if completion_data:
                completion_data.sort(key=lambda x: x[1])  # Tri par steps
                
                print(f"\nüèÅ CLASSEMENT PAR TEMPS DE COMPL√âTION:")
                for i, (name, steps, rate, fps) in enumerate(completion_data, 1):
                    medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                    print(f"   {medal} {name}: {steps:.0f} steps "
                          f"({rate*100:.0f}% r√©ussite, {fps:.1f} FPS)")
                
                # Statistiques finales
                all_steps = [steps for _, steps, _, _ in completion_data]
                all_fps = [fps for _, _, _, fps in completion_data]
                
                print(f"\nüìä STATISTIQUES GLOBALES:")
                print(f"   Temps moyen: {np.mean(all_steps):.1f} steps")
                print(f"   Meilleur temps: {min(all_steps):.1f} steps")
                print(f"   FPS moyen: {np.mean(all_fps):.1f}")
                print(f"   Horizon max: {self.horizon} steps")
        
        print(f"\nüíæ R√©sultats pr√™ts pour sauvegarde")
    
    def save_results(self, filename: str = "layout_evaluation_final.json", include_individual_games: bool = False):
        """
        Sauvegarde les r√©sultats d√©taill√©s.
        
        Args:
            filename: Nom du fichier de sortie
            include_individual_games: Si True, inclut les d√©tails de chaque partie (fichier volumineux)
                                    Si False, ne sauvegarde que les m√©triques agr√©g√©es par layout
        """
        # Pr√©parer les donn√©es d'output en excluant les parties individuelles si demand√©
        results_to_save = {}
        
        for layout_name, layout_data in self.results.items():
            if layout_data.get('viable', False):
                # Copier toutes les donn√©es sauf individual_games si pas demand√©
                filtered_data = {}
                for key, value in layout_data.items():
                    if key == 'individual_games' and not include_individual_games:
                        continue  # Exclure les d√©tails des parties individuelles
                    filtered_data[key] = value
                
                results_to_save[layout_name] = filtered_data
            else:
                # Garder les layouts non viables tels quels (ils n'ont pas d'individual_games)
                results_to_save[layout_name] = layout_data
        
        output_data = {
            'evaluation_config': {
                'layouts_directory': self.layouts_directory,
                'horizon': self.horizon,
                'num_games_per_layout': self.num_games_per_layout,
                'target_fps': self.target_fps,
                'max_stuck_frames': self.max_stuck_frames,
                'single_agent_mode': self.single_agent,
                'greedy_with_stay_mode': self.greedy_with_stay
            },
            'evaluation_timestamp': time.time(),
            'data_type': 'layout_aggregated_metrics' if not include_individual_games else 'detailed_with_individual_games',
            'results': results_to_save
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        data_type_desc = "m√©triques agr√©g√©es par layout" if not include_individual_games else "donn√©es d√©taill√©es avec parties individuelles"
        print(f"üíæ R√©sultats sauvegard√©s dans {filename} ({data_type_desc})")
        
        # Afficher la taille du fichier
        import os
        file_size = os.path.getsize(filename)
        if file_size > 1024 * 1024:
            print(f"   üì¶ Taille: {file_size / (1024 * 1024):.1f} MB")
        elif file_size > 1024:
            print(f"   üì¶ Taille: {file_size / 1024:.1f} KB")
        else:
            print(f"   üì¶ Taille: {file_size} bytes")
    
    def save_simulation_data_files(self):
        """
        G√©n√®re des fichiers de donn√©es de simulation individuels pour chaque game
        organis√©s dans des dossiers par layout.
        Structure: data_simulation/data_simu_<layoutname>/data_simu_<layoutname>_game_<num√©ro>.json
        """
        if not self.results:
            print("‚ùå Aucun r√©sultat √† sauvegarder")
            return
        
        # Cr√©er le dossier data_simulation parent
        data_simulation_dir = "data_simulation"
        os.makedirs(data_simulation_dir, exist_ok=True)
        
        files_created = []
        total_games = 0
        
        for layout_name, layout_data in self.results.items():
            if not layout_data.get('viable', False):
                print(f"‚ö†Ô∏è Layout {layout_name} non viable, ignor√©")
                continue
                
            # Extraire les donn√©es n√©cessaires
            individual_games = layout_data.get('individual_games', [])
            
            if not individual_games:
                print(f"‚ö†Ô∏è Pas de donn√©es de jeu individuelles pour {layout_name}")
                continue
            
            # Cr√©er le dossier sp√©cifique pour ce layout dans data_simulation
            layout_dir = os.path.join(data_simulation_dir, f"data_simu_{layout_name}")
            os.makedirs(layout_dir, exist_ok=True)
            
            # Traiter chaque game individuellement
            for game_idx, game_data in enumerate(individual_games):
                # Cr√©er info_sum pour ce game sp√©cifique
                info_sum = self._calculate_single_game_info_sum(game_data, layout_name)
                
                # Cr√©er l'historique des positions pour ce game uniquement
                history_info = self._create_single_game_history_info(game_data, game_idx)
                
                # Valider la coh√©rence des mouvements pour ce game
                validation_results = self._validate_movement_coherence(history_info)
                
                if not validation_results['is_coherent']:
                    print(f"‚ö†Ô∏è Layout {layout_name}, Game {game_idx}: {validation_results['invalid_movements']} mouvements incoh√©rents d√©tect√©s")
                    print(f"   üìä {validation_results['total_movements_checked']} mouvements v√©rifi√©s au total")
                
                # Structure finale compatible avec data_collecter_simualtion
                simulation_data = {
                    "simulation_data": {
                        "info_sum": info_sum,
                        "history_info": history_info,
                        "grid": self._extract_grid_from_game_data(game_data, layout_name)
                    }
                }
                
                # Nom du fichier pour ce game
                filename = f"data_simu_{layout_name}_game_{game_idx}.json"
                filepath = os.path.join(layout_dir, filename)
                
                # Sauvegarder
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(simulation_data, f, indent=4, ensure_ascii=False)
                
                files_created.append(filepath)
                total_games += 1
            
            print(f"‚úÖ Layout {layout_name}: {len(individual_games)} fichiers cr√©√©s dans {layout_dir}/")
        
        print(f"\nüìÅ {len(files_created)} fichiers de donn√©es de simulation cr√©√©s ({total_games} games total)")
        return files_created
    
    def _extract_grid_from_game_data(self, game_data: Dict, layout_name: str) -> List[List[str]]:
        """
        Extrait la grille utilis√©e pendant la simulation √† partir des donn√©es du jeu.
        """
        try:
            # R√©cup√©rer la grille depuis les donn√©es du jeu (maintenant stock√©e)
            if 'mdp_terrain' in game_data:
                return game_data['mdp_terrain']
            
            # Si la grille n'est pas dans les donn√©es, la recr√©er depuis le layout
            # mais signaler que c'est une reconstruction
            print(f"‚ö†Ô∏è Grille non trouv√©e dans les donn√©es de simulation pour {layout_name}, reconstruction depuis le layout")
            
            full_layout_path = f"generation_cesar/{layout_name}"
            mdp = OvercookedGridworld.from_layout_name(full_layout_path)
            
            # Convertir terrain_mtx en format de liste pour JSON
            grid_as_list = []
            for row in mdp.terrain_mtx:
                grid_row = []
                for cell in row:
                    grid_row.append(str(cell))
                grid_as_list.append(grid_row)
            
            return grid_as_list
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction de la grille pour {layout_name}: {e}")
            # Retourner une grille vide en cas d'erreur
            return []
    
    def _verify_layout_grid_consistency(self, layout_name: str, mdp: OvercookedGridworld) -> Dict:
        """
        V√©rifie la coh√©rence entre la grille du fichier .layout et celle charg√©e par OvercookedGridworld.
        """
        verification_result = {
            'has_discrepancy': False,
            'issues': [],
            'layout_file_found': False,
            'grid_comparison': None
        }
        
        try:
            # Charger la grille depuis le fichier .layout
            layout_path = f"overcooked_ai_py/data/layouts/generation_cesar/{layout_name}.layout"
            
            if not os.path.exists(layout_path):
                verification_result['issues'].append(f"Fichier .layout non trouv√©: {layout_path}")
                return verification_result
            
            verification_result['layout_file_found'] = True
            
            with open(layout_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # Extraire la grille avec regex
            import re
            pattern = r'"grid":\s*"""(.*?)"""'
            match = re.search(pattern, content, re.DOTALL)
            
            if not match:
                verification_result['issues'].append("Impossible d'extraire la grille du fichier .layout")
                return verification_result
            
            grid_content = match.group(1)
            grid_content = grid_content.replace('\t', '').strip()
            layout_grid_lines = [line.strip() for line in grid_content.split('\n') if line.strip()]
            
            # Comparer avec la grille MDP (terrain_mtx)
            mdp_grid_lines = []
            for row in mdp.terrain_mtx:
                mdp_grid_lines.append(''.join(row))
            
            # Normaliser les largeurs (padding avec des espaces)
            if layout_grid_lines and mdp_grid_lines:
                max_width_layout = max(len(line) for line in layout_grid_lines)
                max_width_mdp = max(len(line) for line in mdp_grid_lines)
                
                layout_grid_normalized = [line + ' ' * (max_width_layout - len(line)) for line in layout_grid_lines]
                mdp_grid_normalized = [line + ' ' * (max_width_mdp - len(line)) for line in mdp_grid_lines]
                
                # Comparer ligne par ligne
                discrepancies = []
                max_lines = max(len(layout_grid_normalized), len(mdp_grid_normalized))
                
                for i in range(max_lines):
                    layout_line = layout_grid_normalized[i] if i < len(layout_grid_normalized) else ''
                    mdp_line = mdp_grid_normalized[i] if i < len(mdp_grid_normalized) else ''
                    
                    if layout_line != mdp_line:
                        discrepancies.append({
                            'line': i,
                            'layout': layout_line,
                            'mdp': mdp_line
                        })
                
                if discrepancies:
                    verification_result['has_discrepancy'] = True
                    verification_result['grid_comparison'] = discrepancies
                    verification_result['issues'].append(f"{len(discrepancies)} lignes diff√®rent entre .layout et MDP")
                    
                    # D√©tailler les premi√®res diff√©rences
                    for i, diff in enumerate(discrepancies[:3]):  # Montrer seulement les 3 premi√®res
                        verification_result['issues'].append(
                            f"Ligne {diff['line']}: layout='{diff['layout']}' vs mdp='{diff['mdp']}'"
                        )
                    
                    if len(discrepancies) > 3:
                        verification_result['issues'].append(f"... et {len(discrepancies) - 3} autres diff√©rences")
                
        except Exception as e:
            verification_result['issues'].append(f"Erreur lors de la v√©rification: {e}")
        
        return verification_result
    
    def _calculate_single_game_info_sum(self, game_data: Dict, layout_name: str) -> Dict:
        """
        Calcule les statistiques pour un seul game (pas d'agr√©gation).
        """
        if not game_data:
            return {}
        
        # Extraire les m√©triques du game individuel
        info_sum = game_data.get('info_sum', {})
        
        # Structure de base
        single_game_info = {
            "number_games": 1,  # Un seul game
            "layout": layout_name,
            "time_elapsed": game_data.get('timing', {}).get('total_time_seconds', 0),
            "ai_action_per_step": 1,  # Valeur constante
            "step": int(game_data.get('steps', 0)),
            "fps": game_data.get('timing', {}).get('actual_fps', 0),
            "time": game_data.get('timing', {}).get('simulated_time_seconds', 0),
            "recipe_completed": int(game_data.get('orders_completed', 0)),
            "score": int(game_data.get('total_reward', 0)),
            "min_recipe_complete": int(game_data.get('orders_completed', 0)),
            "max_recipe_complete": int(game_data.get('orders_completed', 0)),
        }
        
        # Ajouter les m√©triques d'actions pour chaque agent
        for agent_key in ['agent_0', 'agent_1']:
            agent_stats = game_data.get('agent_statistics', {}).get(agent_key, {})
            action_count = int(agent_stats.get('total_actions', 0))
            
            action_key = f'{agent_key}_action_count'
            single_game_info[action_key] = action_count
            single_game_info[f'min_{action_key}'] = action_count
            single_game_info[f'max_{action_key}'] = action_count
            single_game_info[f'std_{action_key}'] = 0.0  # Pas de variance pour un seul game
            
            # Ajouter mouvements et interactions
            single_game_info[f'{agent_key}_mouvements'] = int(agent_stats.get('distance_traveled', 0))
            single_game_info[f'{agent_key}_stuck_loop'] = 0  # √Ä calculer si n√©cessaire
            single_game_info[f'{agent_key}_interaction'] = int(agent_stats.get('interact_count', 0))
        
        # Ajouter toutes les m√©triques comportementales du info_sum
        for key, value in info_sum.items():
            if key not in single_game_info:  # √âviter d'√©craser les cl√©s d√©j√† d√©finies
                single_game_info[key] = value
        
        # Ajouter all_orders (structure standard)
        single_game_info["all_orders"] = [
            {"ingredients": ["onion"]},
            {"ingredients": ["onion", "onion", "onion"]},
            {"ingredients": ["tomato"]},
            {"ingredients": ["tomato", "tomato", "tomato"]},
            {"ingredients": ["onion", "tomato"]}
        ]
        
        return single_game_info
    
    def _create_single_game_history_info(self, game_data: Dict, game_idx: int) -> Dict:
        """
        Cr√©e l'historique des positions des agents pour un seul game.
        """
        history_info = {}
        game_key = f"history_game_0"  # Un seul game, donc toujours game_0
        
        # Extraire les trajectoires du jeu (si disponibles)
        trajectory = game_data.get('trajectory', [])
        
        # Cr√©er l'historique des positions pour ce jeu
        history_entry = {
            "agent_0_history": {},
            "agent_1_history": {}
        }
        
        if trajectory and len(trajectory) > 0:
            # Collecter TOUS les steps de la simulation
            total_steps = len(trajectory)
            
            # Enregistrer tous les steps de la vraie simulation
            for step_idx in range(total_steps):
                if step_idx >= len(trajectory):
                    break
                    
                step_key = f"step_{step_idx}_position"
                step_data = trajectory[step_idx]
                
                # Extraire les vraies positions des agents √† ce step pr√©cis
                if 'state' in step_data and 'players' in step_data['state']:
                    players = step_data['state']['players']
                    agent_0_pos = list(players[0].get('position', [0, 0])) if len(players) > 0 else [0, 0]
                    agent_1_pos = list(players[1].get('position', [0, 1])) if len(players) > 1 else [0, 1]
                    
                    # S'assurer que les positions sont des int Python standards
                    agent_0_pos = [int(x) for x in agent_0_pos]
                    agent_1_pos = [int(x) for x in agent_1_pos]
                else:
                    # Si pas de donn√©es pour ce step, utiliser la position pr√©c√©dente
                    if step_idx > 0:
                        prev_key = f"step_{step_idx-1}_position"
                        agent_0_pos = history_entry["agent_0_history"].get(prev_key, [1, 1])
                        agent_1_pos = history_entry["agent_1_history"].get(prev_key, [1, 2])
                    else:
                        agent_0_pos = [1, 1]
                        agent_1_pos = [1, 2]
                
                history_entry["agent_0_history"][step_key] = agent_0_pos
                history_entry["agent_1_history"][step_key] = agent_1_pos
                
            # Ajouter m√©tadonn√©es r√©elles
            history_entry["_metadata"] = {
                "total_trajectory_steps": int(len(trajectory)),
                "recorded_steps": int(total_steps),
                "sampling_method": "complete_simulation_capture",
                "data_source": f"actual_simulation_game_{game_idx}"
            }
        else:
            # Aucune trajectoire disponible
            print(f"‚ö†Ô∏è Aucune trajectoire disponible pour game {game_idx} - aucune donn√©e de position collect√©e")
            history_entry["_metadata"] = {
                "total_trajectory_steps": 0,
                "recorded_steps": 0,
                "sampling_method": "no_data_available",
                "data_source": f"none_game_{game_idx}"
            }
        
        history_info[game_key] = [history_entry]
        return history_info
    
    def _calculate_aggregated_info_sum(self, individual_games: List[Dict], layout_name: str) -> Dict:
        """
        Calcule les statistiques agr√©g√©es (moyenne, √©cart-type, min, max) √† partir des jeux individuels.
        """
        if not individual_games:
            return {}
        
        # Extraire toutes les m√©triques de tous les jeux
        all_metrics = {}
        for game in individual_games:
            info_sum = game.get('info_sum', {})
            for key, value in info_sum.items():
                if key not in all_metrics:
                    all_metrics[key] = []
                
                # Traiter diff√©rents types de valeurs
                if isinstance(value, list) and len(value) == 2:
                    # Format [agent_0, agent_1]
                    all_metrics[key].append(value)
                elif isinstance(value, (int, float)):
                    all_metrics[key].append([value, 0])  # Format standardis√© [agent_0, agent_1]
                else:
                    all_metrics[key].append([0, 0])
        
        # Calculer les statistiques agr√©g√©es
        aggregated = {
            "number_games": len(individual_games),
            "layout": layout_name,
            "time_elapsed": np.mean([g.get('timing', {}).get('total_time_seconds', 0) for g in individual_games]),
            "ai_action_per_step": 1,  # Valeur constante
            "step": int(np.mean([g.get('steps', 0) for g in individual_games])),
            "fps": np.mean([g.get('timing', {}).get('actual_fps', 0) for g in individual_games]),
            "time": np.mean([g.get('timing', {}).get('simulated_time_seconds', 0) for g in individual_games]),
            "recipe_completed": int(np.mean([g.get('orders_completed', 0) for g in individual_games])),
            "score": int(np.mean([g.get('total_reward', 0) for g in individual_games])),
            "min_recipe_complete": int(np.min([g.get('orders_completed', 0) for g in individual_games])),
            "max_recipe_complete": int(np.max([g.get('orders_completed', 0) for g in individual_games])),
        }
        
        # Ajouter les m√©triques d'actions avec statistiques
        for agent_key in ['agent_0', 'agent_1']:
            values = [g.get('agent_statistics', {}).get(agent_key, {}).get('total_actions', 0) for g in individual_games]
            action_key = f'{agent_key}_action_count'
            aggregated[action_key] = int(np.mean(values))
            aggregated[f'min_{action_key}'] = int(np.min(values))
            aggregated[f'max_{action_key}'] = int(np.max(values))
            aggregated[f'std_{action_key}'] = float(np.std(values))
            
            # Ajouter mouvements et interactions
            movement_values = [g.get('agent_statistics', {}).get(agent_key, {}).get('distance_traveled', 0) for g in individual_games]
            interaction_values = [g.get('agent_statistics', {}).get(agent_key, {}).get('interact_count', 0) for g in individual_games]
            
            aggregated[f'{agent_key}_mouvements'] = int(np.mean(movement_values))
            aggregated[f'{agent_key}_stuck_loop'] = 0  # √Ä calculer si n√©cessaire
            aggregated[f'{agent_key}_interaction'] = int(np.mean(interaction_values))
        
        # Ajouter les m√©triques comportementales agr√©g√©es
        for metric_name, values_list in all_metrics.items():
            if values_list and isinstance(values_list[0], list):
                # Calculer moyennes pour [agent_0, agent_1]
                agent_0_values = [v[0] for v in values_list if len(v) > 0]
                agent_1_values = [v[1] for v in values_list if len(v) > 1]
                
                aggregated[metric_name] = [
                    int(np.mean(agent_0_values)) if agent_0_values else 0,
                    int(np.mean(agent_1_values)) if agent_1_values else 0
                ]
        
        # Ajouter all_orders (structure standard)
        aggregated["all_orders"] = [
            {"ingredients": ["onion"]},
            {"ingredients": ["onion", "onion", "onion"]},
            {"ingredients": ["tomato"]},
            {"ingredients": ["tomato", "tomato", "tomato"]},
            {"ingredients": ["onion", "tomato"]}
        ]
        
        return aggregated
    
    def _create_history_info(self, individual_games: List[Dict]) -> Dict:
        """
        Cr√©e l'historique des positions des agents pour chaque jeu.
        Collecte les vraies donn√©es de simulation step-by-step, pas d'√©chantillonnage.
        """
        history_info = {}
        
        for game_idx, game in enumerate(individual_games):
            game_key = f"history_game_{game_idx}"
            
            # Extraire les trajectoires du jeu (si disponibles)
            trajectory = game.get('trajectory', [])
            
            # Cr√©er l'historique des positions pour ce jeu
            history_entry = {
                "agent_0_history": {},
                "agent_1_history": {}
            }
            
            if trajectory and len(trajectory) > 0:
                # Collecter TOUS les steps de la simulation (pas de limitation artificielle)
                total_steps = len(trajectory)
                
                # Enregistrer tous les steps de la vraie simulation
                for step_idx in range(total_steps):
                    if step_idx >= len(trajectory):
                        break
                        
                    step_key = f"step_{step_idx}_position"
                    step_data = trajectory[step_idx]
                    
                    # Extraire les vraies positions des agents √† ce step pr√©cis
                    if 'state' in step_data and 'players' in step_data['state']:
                        players = step_data['state']['players']
                        agent_0_pos = list(players[0].get('position', [0, 0])) if len(players) > 0 else [0, 0]
                        agent_1_pos = list(players[1].get('position', [0, 1])) if len(players) > 1 else [0, 1]
                        
                        # S'assurer que les positions sont des int Python standards
                        agent_0_pos = [int(x) for x in agent_0_pos]
                        agent_1_pos = [int(x) for x in agent_1_pos]
                    else:
                        # Si pas de donn√©es pour ce step, utiliser la position pr√©c√©dente
                        if step_idx > 0:
                            prev_key = f"step_{step_idx-1}_position"
                            agent_0_pos = history_entry["agent_0_history"].get(prev_key, [1, 1])
                            agent_1_pos = history_entry["agent_1_history"].get(prev_key, [1, 2])
                        else:
                            agent_0_pos = [1, 1]
                            agent_1_pos = [1, 2]
                    
                    history_entry["agent_0_history"][step_key] = agent_0_pos
                    history_entry["agent_1_history"][step_key] = agent_1_pos
                    
                # Ajouter m√©tadonn√©es r√©elles
                history_entry["_metadata"] = {
                    "total_trajectory_steps": int(len(trajectory)),
                    "recorded_steps": int(total_steps),
                    "sampling_method": "complete_simulation_capture",
                    "data_source": "actual_simulation"
                }
            else:
                # Aucune trajectoire disponible - signaler mais ne pas g√©n√©rer de fausses donn√©es
                print(f"‚ö†Ô∏è Aucune trajectoire disponible pour {game_key} - aucune donn√©e de position collect√©e")
                history_entry["_metadata"] = {
                    "total_trajectory_steps": 0,
                    "recorded_steps": 0,
                    "sampling_method": "no_data_available",
                    "data_source": "none"
                }
            
            history_info[game_key] = [history_entry]
        
        return history_info
    
    def _validate_movement_coherence(self, history_info: Dict) -> Dict:
        """
        Valide que tous les mouvements dans l'historique sont coh√©rents 
        (distance max de 1 case entre steps cons√©cutifs).
        """
        validation_results = {
            'total_movements_checked': 0,
            'invalid_movements': 0,
            'invalid_movement_details': [],
            'is_coherent': True
        }
        
        for game_key, game_data in history_info.items():
            if not game_data or len(game_data) == 0:
                continue
                
            history_entry = game_data[0]
            
            for agent_key in ['agent_0_history', 'agent_1_history']:
                agent_history = history_entry.get(agent_key, {})
                positions = []
                
                # Extraire toutes les positions dans l'ordre
                step_numbers = []
                for step_key in agent_history.keys():
                    if step_key.startswith('step_') and step_key.endswith('_position'):
                        step_num = int(step_key.split('_')[1])
                        step_numbers.append(step_num)
                
                step_numbers.sort()
                
                for step_num in step_numbers:
                    step_key = f"step_{step_num}_position"
                    if step_key in agent_history:
                        positions.append(agent_history[step_key])
                
                # V√©rifier les distances entre steps cons√©cutifs
                for i in range(1, len(positions)):
                    pos_prev = positions[i-1]
                    pos_curr = positions[i]
                    
                    # Calculer la distance de Manhattan
                    distance = abs(pos_curr[0] - pos_prev[0]) + abs(pos_curr[1] - pos_prev[1])
                    
                    validation_results['total_movements_checked'] += 1
                    
                    if distance > 1:
                        validation_results['invalid_movements'] += 1
                        validation_results['is_coherent'] = False
                        validation_results['invalid_movement_details'].append({
                            'game': game_key,
                            'agent': agent_key,
                            'step_from': step_numbers[i-1],
                            'step_to': step_numbers[i],
                            'position_from': pos_prev,
                            'position_to': pos_curr,
                            'distance': distance
                        })
        
        return validation_results


def simulate_game_parallel(layout_name: str, game_id: int, evaluator_config: Dict) -> Dict:
    """
    Fonction pour simuler une partie en parall√®le.
    Cette fonction est ex√©cut√©e dans un processus s√©par√©.
    """
    # Recr√©er l'√©valuateur dans le processus worker
    evaluator = LayoutEvaluator(**evaluator_config)
    
    # Charger le MDP
    full_layout_path = f"generation_cesar/{layout_name}"
    mdp = OvercookedGridworld.from_layout_name(full_layout_path)
    
    # Cr√©er les agents
    success, agent_or_group = evaluator.create_agent_group(mdp)
    if not success:
        return {
            'game_id': game_id,
            'error': 'Failed to create agents',
            'completed': False
        }
    
    # Simuler la partie
    return evaluator.simulate_single_game(mdp, agent_or_group, game_id)


def main():
    """Fonction principale."""
    import sys
    
    # V√©rifier les arguments pour les diff√©rents modes
    single_agent_mode = '--solo' in sys.argv or '--single' in sys.argv
    greedy_with_stay_mode = '--stay' in sys.argv or '--greedy-stay' in sys.argv
    
    # Options d'optimisation
    parallel_mode = '--parallel' in sys.argv or '--fast' in sys.argv
    high_fps = '--speed' in sys.argv or '--turbo' in sys.argv
    
    # Param√®tres d'optimisation
    target_fps = 100.0 if high_fps else 10.0
    max_workers = None
    
    # D√©terminer le nombre de workers pour le parall√©lisme
    if parallel_mode:
        for i, arg in enumerate(sys.argv):
            if arg in ['--workers', '-w'] and i + 1 < len(sys.argv):
                try:
                    max_workers = int(sys.argv[i + 1])
                except ValueError:
                    pass
    
    # D√©terminer le mode et la description
    if single_agent_mode:
        mode_description = "1x GreedyAgent (SOLO PUR)"
        filename_suffix = "solo"
    elif greedy_with_stay_mode:
        mode_description = "GreedyAgent + StayAgent"
        filename_suffix = "greedy_stay"
    else:
        mode_description = "2x GreedyAgent (COOP)"
        filename_suffix = "coop"
    
    if parallel_mode:
        mode_description += " [PARALL√àLE]"
        filename_suffix += "_parallel"
    elif high_fps:
        mode_description += " [HIGH SPEED]"
        filename_suffix += "_speed"
    
    print("üéÆ √âVALUATEUR DE LAYOUTS OVERCOOKED")
    print(f"ü§ñ Mode: {mode_description}")
    print("=" * 60)
    
    # Configuration
    layouts_dir = "./overcooked_ai_py/data/layouts/generation_cesar/"
    
    if not os.path.exists(layouts_dir):
        print(f"‚ùå R√©pertoire {layouts_dir} non trouv√©")
        return
    
    # Cr√©er l'√©valuateur
    evaluator = LayoutEvaluator(
        layouts_directory=layouts_dir,
        horizon=600,  # Horizon raisonnable
        num_games_per_layout=10,  # Plusieurs parties pour moyenner
        target_fps=target_fps,
        parallel_games=parallel_mode,
        max_workers=max_workers,
        max_stuck_frames=50,  # √âviter les blocages infinis
        single_agent=single_agent_mode,  # Mode solo pur
        greedy_with_stay=greedy_with_stay_mode  # Mode GreedyAgent + StayAgent
    )
    
    # Lancer l'√©valuation
    results = evaluator.evaluate_all_layouts()
    
    # Sauvegarder avec un nom diff√©rent selon le mode
    filename = f"layout_evaluation_{filename_suffix}.json"
    # Sauvegarder seulement les m√©triques agr√©g√©es par layout (pas les parties individuelles)
    evaluator.save_results(filename, include_individual_games=False)
    
    # G√©n√©rer les fichiers de donn√©es de simulation individuels
    print(f"\nüîÑ G√âN√âRATION DES FICHIERS DE DONN√âES DE SIMULATION...")
    simulation_files = evaluator.save_simulation_data_files()
    
    print(f"\nüéØ √âVALUATION TERMIN√âE!")
    print(f"   üìä Mode: {mode_description}")
    print(f"   üìä M√©triques comportementales compl√®tes par layout")
    print(f"   üíæ R√©sultats agr√©g√©s sauvegard√©s dans {filename}")
    print(f"   üìÅ {len(simulation_files)} fichiers de simulation cr√©√©s dans dossiers individuels par layout")
    
    # Optionnel: sauvegarder aussi le fichier d√©taill√© pour debug
    if '--debug' in sys.argv or '--detailed' in sys.argv:
        detailed_filename = f"layout_evaluation_{filename_suffix}_detailed.json"
        evaluator.save_results(detailed_filename, include_individual_games=True)
        print(f"   ÔøΩ R√©sultats d√©taill√©s sauvegard√©s dans {detailed_filename}")
    
    print(f"\nüí° MODES DISPONIBLES:")
    print(f"   ‚Ä¢ Mode coop√©ratif (d√©faut): python {sys.argv[0]}")
    print(f"   ‚Ä¢ Mode solo pur: python {sys.argv[0]} --solo")
    print(f"   ‚Ä¢ Mode GreedyAgent + StayAgent: python {sys.argv[0]} --stay")
    print(f"   ‚Ä¢ Mode parall√®le: python {sys.argv[0]} --parallel [--workers N]")
    print(f"   ‚Ä¢ Mode haute vitesse: python {sys.argv[0]} --speed")
    print(f"   ‚Ä¢ Ajouter --debug pour sauvegarder aussi les d√©tails complets")
    print(f"\nüöÄ OPTIONS D'OPTIMISATION:")
    print(f"   ‚Ä¢ --parallel : Ex√©cute les parties en parall√®le")
    print(f"   ‚Ä¢ --workers N : Nombre de processus parall√®les (d√©faut: auto)")
    print(f"   ‚Ä¢ --speed : FPS √©lev√© (100 FPS au lieu de 10)")
    print(f"   ‚Ä¢ --fast : √âquivalent √† --parallel --speed")
    print(f"\nüìÇ FICHIERS G√âN√âR√âS:")
    print(f"   ‚Ä¢ {filename}: M√©triques agr√©g√©es par layout")
    print(f"   ‚Ä¢ data_simulation/data_simu_<layoutname>/: Dossiers individuels par layout")
    print(f"   ‚Ä¢ data_simu_<layoutname>_game_<N>.json: Un fichier par game jou√©")


if __name__ == "__main__":
    main()
