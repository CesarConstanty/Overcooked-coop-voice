#!/usr/bin/env python3
"""
layout_characterizer.py

Caract√©risateur de layouts Overcooked bas√© sur les comportements des GreedyAgent.
Utilise les comparaisons comportementales pour identifier les caract√©ristiques des layouts.

OBJECTIF: Caract√©riser et classer les layouts selon les patterns comportementaux observ√©s.
"""

import json
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter
from pathlib import Path
import argparse
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns


class LayoutCharacterizer:
    """
    Caract√©risateur de layouts bas√© sur les comportements comparatifs des GreedyAgent.
    
    M√âTHODES DE CARACT√âRISATION POSSIBLES:
    =====================================
    
    1. CARACT√âRISATION STRUCTURELLE:
       - Dimensions et densit√©
       - Distribution des √©l√©ments (pots, distributeurs, zones de service)
       - Connectivit√© et accessibilit√©
       - Complexit√© g√©om√©trique
    
    2. CARACT√âRISATION COMPORTEMENTALE:
       - Patterns de mouvement des agents
       - Efficacit√© des trajectoires
       - Points de congestion et goulots d'√©tranglement
       - Zones d'activit√© intensive
    
    3. CARACT√âRISATION PAR PERFORMANCE:
       - Taux de r√©ussite et temps de compl√©tion
       - Efficacit√© relative entre modes (solo/coop)
       - Courbes d'apprentissage et consistance
       - Ratios effort/r√©compense
    
    4. CARACT√âRISATION PAR COORDINATION:
       - N√©cessit√© de coordination
       - Types d'interactions requises
       - S√©quencement des t√¢ches
       - Sp√©cialisation des r√¥les
    
    5. CARACT√âRISATION PAR STRAT√âGIES:
       - Strat√©gies √©mergentes dominantes
       - Diversit√© des approches viables
       - Adaptabilit√© et flexibilit√© strat√©gique
       - Points de d√©cision critiques
    
    6. CARACT√âRISATION PAR COMPLEXIT√â:
       - Charge cognitive estim√©e
       - Complexit√© d√©cisionnelle
       - Pr√©dictibilit√© des patterns
       - Variabilit√© des r√©sultats
    
    7. CARACT√âRISATION COMPARATIVE:
       - Positionnement relatif par rapport aux autres layouts
       - Clusters de similarit√©
       - Outliers et cas sp√©ciaux
       - Gradients de difficult√©
    
    8. CARACT√âRISATION TEMPORELLE:
       - √âvolution des m√©triques au cours du temps
       - Phases de jeu distinctes
       - Rythme et tempo de jeu
       - Points de transition critiques
    """
    
    def __init__(self):
        self.layout_data = {}
        self.characterization_results = {}
        self.comparison_matrix = {}
        
        print("üèóÔ∏è CARACT√âRISATEUR DE LAYOUTS OVERCOOKED")
        print("=" * 50)
        print("üìã M√âTHODES DE CARACT√âRISATION DISPONIBLES:")
        print("   1. Structurelle (g√©om√©trie, √©l√©ments)")
        print("   2. Comportementale (mouvement, efficacit√©)")
        print("   3. Performance (succ√®s, temps, ratios)")
        print("   4. Coordination (interactions, r√¥les)")
        print("   5. Strat√©gique (√©mergence, adaptation)")
        print("   6. Complexit√© (cognitive, d√©cisionnelle)")
        print("   7. Comparative (clusters, similarit√©s)")
        print("   8. Temporelle (√©volution, phases)")
        print("=" * 50)
    
    def load_layout_data(self, data_file: str) -> bool:
        """Charge les donn√©es de layouts √† caract√©riser."""
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                self.layout_data = json.load(f)
            
            layouts_count = len(self.layout_data.get('layouts', {}))
            print(f"‚úÖ Donn√©es charg√©es: {layouts_count} layouts")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur chargement donn√©es: {e}")
            return False
    
    # ========================================
    # 1. CARACT√âRISATION STRUCTURELLE
    # ========================================
    
    def characterize_structural_features(self, layout_name: str, layout_data: Dict) -> Dict:
        """
        Caract√©rise les features structurelles d'un layout.
        
        M√âTRIQUES STRUCTURELLES:
        - Dimensions et aire totale
        - Densit√© d'√©l√©ments
        - Distribution spatiale des √©l√©ments
        - Sym√©trie et √©quilibre spatial
        - Connectivit√© entre zones importantes
        - Distances critiques
        - Complexit√© g√©om√©trique
        """
        structural = {
            'dimensions': {},
            'element_distribution': {},
            'spatial_properties': {},
            'connectivity': {},
            'complexity_metrics': {}
        }
        
        layout_info = layout_data.get('layout_info', {})
        
        # Dimensions de base
        dimensions = layout_info.get('dimensions', {})
        width = dimensions.get('width', 0)
        height = dimensions.get('height', 0)
        
        structural['dimensions'] = {
            'width': width,
            'height': height,
            'total_area': width * height,
            'aspect_ratio': width / max(height, 1),
            'perimeter': 2 * (width + height)
        }
        
        # Distribution des √©l√©ments
        elements = layout_info.get('elements', {})
        total_cells = layout_info.get('total_cells', max(width * height, 1))
        
        structural['element_distribution'] = {
            'tomato_dispensers': elements.get('tomato_dispensers', 0),
            'onion_dispensers': elements.get('onion_dispensers', 0),
            'pots': elements.get('pots', 0),
            'serving_areas': elements.get('serving_areas', 0),
            'total_functional_elements': sum(elements.values()),
            'element_density': sum(elements.values()) / total_cells,
            'pot_to_dispenser_ratio': elements.get('pots', 0) / max(elements.get('tomato_dispensers', 0) + elements.get('onion_dispensers', 0), 1),
            'service_accessibility': elements.get('serving_areas', 0) / max(elements.get('pots', 0), 1)
        }
        
        # Propri√©t√©s spatiales avanc√©es (√† impl√©menter avec les donn√©es de position)
        structural['spatial_properties'] = self._calculate_spatial_properties(layout_info)
        
        # Connectivit√© (n√©cessite analyse de la carte)
        structural['connectivity'] = self._analyze_connectivity(layout_info)
        
        # M√©triques de complexit√©
        structural['complexity_metrics'] = self._calculate_structural_complexity(layout_info, elements)
        
        return structural
    
    def _calculate_spatial_properties(self, layout_info: Dict) -> Dict:
        """Calcule les propri√©t√©s spatiales avanc√©es."""
        # Placeholder pour l'analyse spatiale d√©taill√©e
        return {
            'symmetry_score': 0.5,  # √Ä impl√©menter
            'balance_score': 0.5,   # √Ä impl√©menter
            'clustering_coefficient': 0.5,  # √Ä impl√©menter
            'note': 'Spatial analysis requires detailed layout map parsing'
        }
    
    def _analyze_connectivity(self, layout_info: Dict) -> Dict:
        """Analyse la connectivit√© entre √©l√©ments importants."""
        # Placeholder pour l'analyse de connectivit√©
        return {
            'path_efficiency': 0.5,  # √Ä impl√©menter
            'bottleneck_score': 0.5,  # √Ä impl√©menter
            'accessibility_score': 0.5,  # √Ä impl√©menter
            'note': 'Connectivity analysis requires pathfinding implementation'
        }
    
    def _calculate_structural_complexity(self, layout_info: Dict, elements: Dict) -> Dict:
        """Calcule les m√©triques de complexit√© structurelle."""
        total_elements = sum(elements.values())
        total_cells = layout_info.get('total_cells', 1)
        
        # Complexit√© bas√©e sur la vari√©t√© d'√©l√©ments
        element_types = len([v for v in elements.values() if v > 0])
        element_variety = element_types / 4  # Normalis√© sur 4 types d'√©l√©ments
        
        # Complexit√© bas√©e sur la densit√©
        density_complexity = min(total_elements / total_cells, 1.0)
        
        # Score de complexit√© global
        overall_complexity = (element_variety + density_complexity) / 2
        
        return {
            'element_variety_score': element_variety,
            'density_complexity_score': density_complexity,
            'overall_structural_complexity': overall_complexity,
            'total_functional_elements': total_elements,
            'complexity_category': self._categorize_complexity(overall_complexity)
        }
    
    def _categorize_complexity(self, complexity_score: float) -> str:
        """Cat√©gorise le niveau de complexit√©."""
        if complexity_score < 0.3:
            return 'simple'
        elif complexity_score < 0.6:
            return 'moderate'
        elif complexity_score < 0.8:
            return 'complex'
        else:
            return 'very_complex'
    
    # ========================================
    # 2. CARACT√âRISATION COMPORTEMENTALE
    # ========================================
    
    def characterize_behavioral_patterns(self, layout_name: str, layout_data: Dict) -> Dict:
        """
        Caract√©rise les patterns comportementaux observ√©s sur ce layout.
        
        M√âTRIQUES COMPORTEMENTALES:
        - Patterns de mouvement dominants
        - Efficacit√© des trajectoires
        - Zones d'activit√© intensive
        - Fr√©quence des interactions
        - Sp√©cialisation des agents
        - Adaptation comportementale
        """
        behavioral = {
            'movement_patterns': {},
            'interaction_patterns': {},
            'specialization_analysis': {},
            'efficiency_patterns': {},
            'adaptation_indicators': {}
        }
        
        # Analyser tous les modes comportementaux disponibles
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        for mode_name, mode_data in behavioral_modes.items():
            # Patterns de mouvement
            movement = self._analyze_movement_patterns(mode_data)
            behavioral['movement_patterns'][mode_name] = movement
            
            # Patterns d'interaction
            interaction = self._analyze_interaction_patterns(mode_data)
            behavioral['interaction_patterns'][mode_name] = interaction
            
            # Sp√©cialisation
            specialization = self._analyze_agent_specialization(mode_data)
            behavioral['specialization_analysis'][mode_name] = specialization
            
            # Efficacit√©
            efficiency = self._analyze_behavioral_efficiency(mode_data)
            behavioral['efficiency_patterns'][mode_name] = efficiency
        
        # Analyse comparative entre modes
        behavioral['cross_mode_analysis'] = self._compare_behavioral_modes(behavioral_modes)
        
        # Indicateurs d'adaptation
        behavioral['adaptation_indicators'] = self._identify_adaptation_indicators(behavioral_modes)
        
        return behavioral
    
    def _analyze_movement_patterns(self, mode_data: Dict) -> Dict:
        """
        Analyse d√©taill√©e des patterns de mouvement pour un mode donn√©.
        
        M√âTRIQUES CALCUL√âES:
        - Distance totale parcourue et efficacit√©
        - Zones d'activit√© intensive 
        - Patterns de d√©placement r√©p√©titifs
        - Optimisation des trajectoires
        - Distribution spatiale des activit√©s
        """
        patterns = mode_data.get('behavioral_patterns', {})
        performance = mode_data.get('performance_metrics', {})
        
        # Initialiser les m√©triques de mouvement
        movement_metrics = {
            'total_distance': {
                'agent_0': 0,
                'agent_1': 0,
                'total': 0,
                'balance': 0
            },
            'movement_efficiency': {
                'agent_0': 0,
                'agent_1': 0,
                'overall': 0
            },
            'spatial_distribution': {
                'agent_0_coverage': 0,
                'agent_1_coverage': 0,
                'overlap_ratio': 0,
                'zone_specialization': {}
            },
            'trajectory_optimization': {
                'path_efficiency_score': 0,
                'redundant_moves_ratio': 0,
                'optimal_path_adherence': 0
            },
            'activity_zones': {
                'hot_zones_count': 0,
                'zone_transitions_per_agent': [0, 0],
                'zone_focus_consistency': 0
            }
        }
        
        # Extraire les donn√©es de distance depuis les m√©triques comportementales
        if 'agent_specialization' in patterns:
            spec = patterns['agent_specialization']
            
            # Calculer les distances relatives bas√©es sur la sp√©cialisation
            agent_0_roles = spec.get('agent_0_dominant_roles', {})
            agent_1_roles = spec.get('agent_1_dominant_roles', {})
            
            # Estimer la distance bas√©e sur les r√¥les et les activit√©s
            agent_0_distance = self._estimate_movement_from_roles(agent_0_roles, performance)
            agent_1_distance = self._estimate_movement_from_roles(agent_1_roles, performance)
            
            total_distance = agent_0_distance + agent_1_distance
            
            movement_metrics['total_distance'] = {
                'agent_0': agent_0_distance,
                'agent_1': agent_1_distance,
                'total': total_distance,
                'balance': min(agent_0_distance, agent_1_distance) / max(agent_0_distance, agent_1_distance) if max(agent_0_distance, agent_1_distance) > 0 else 0
            }
            
            # Calculer l'efficacit√© de mouvement
            completion_efficiency = performance.get('completion_efficiency', 0)
            steps = performance.get('avg_completion_steps', 600)
            
            # Efficacit√© = compl√©tion √©lev√©e avec peu de mouvements
            agent_0_efficiency = completion_efficiency * (300 / max(agent_0_distance, 1))
            agent_1_efficiency = completion_efficiency * (300 / max(agent_1_distance, 1))
            
            movement_metrics['movement_efficiency'] = {
                'agent_0': min(agent_0_efficiency, 1.0),
                'agent_1': min(agent_1_efficiency, 1.0),
                'overall': (agent_0_efficiency + agent_1_efficiency) / 2
            }
            
            # Analyser la distribution spatiale
            movement_metrics['spatial_distribution'] = self._analyze_spatial_distribution(
                agent_0_roles, agent_1_roles, spec
            )
            
            # Analyser l'optimisation des trajectoires
            movement_metrics['trajectory_optimization'] = self._analyze_trajectory_optimization(
                spec, performance
            )
            
            # Identifier les zones d'activit√©
            movement_metrics['activity_zones'] = self._identify_activity_zones(
                agent_0_roles, agent_1_roles, patterns
            )
        
        # Ajouter des m√©triques d'exploration vs exploitation
        movement_metrics['exploration_analysis'] = self._analyze_exploration_vs_exploitation(patterns)
        
        # Calculer des scores globaux de qualit√© de mouvement
        movement_metrics['movement_quality_scores'] = self._calculate_movement_quality_scores(movement_metrics)
        
        return movement_metrics
    
    def _analyze_interaction_patterns(self, mode_data: Dict) -> Dict:
        """
        Analyse d√©taill√©e des patterns d'interaction pour un mode donn√©.
        
        M√âTRIQUES D'INTERACTION:
        - Fr√©quence et types d'interactions
        - Qualit√© de la coordination
        - Synchronisation des actions
        - Efficacit√© des √©changes
        - Patterns temporels d'interaction
        """
        patterns = mode_data.get('behavioral_patterns', {})
        performance = mode_data.get('performance_metrics', {})
        
        interaction_metrics = {
            'interaction_frequency': {
                'total_interactions': 0,
                'interactions_per_minute': 0,
                'interaction_density': 0
            },
            'interaction_types': {
                'coordination_events': [],
                'exchange_events': 0,
                'support_events': 0,
                'conflict_events': 0
            },
            'coordination_quality': {
                'synchronization_score': 0,
                'complementarity_score': 0,
                'efficiency_score': 0
            },
            'temporal_patterns': {
                'interaction_rhythm': 'unknown',
                'peak_interaction_phases': [],
                'coordination_consistency': 0
            },
            'communication_emergence': {
                'implicit_communication_detected': False,
                'communication_efficiency': 0,
                'information_transfer_quality': 0
            }
        }
        
        # Analyser les √©v√©nements de coordination disponibles
        if 'coordination_events' in patterns:
            coord_events = patterns['coordination_events']
            
            # Fr√©quence d'interaction
            total_interactions = len(coord_events)
            steps = performance.get('avg_completion_steps', 600)
            simulated_minutes = steps / 60  # Approximation (1 step ‚âà 1 seconde)
            
            interaction_metrics['interaction_frequency'] = {
                'total_interactions': total_interactions,
                'interactions_per_minute': total_interactions / max(simulated_minutes, 0.1),
                'interaction_density': total_interactions / max(steps, 1)
            }
            
            # Types d'interactions
            interaction_metrics['interaction_types'] = self._categorize_interaction_types(coord_events)
            
            # Qualit√© de la coordination
            interaction_metrics['coordination_quality'] = self._assess_coordination_quality(
                coord_events, patterns, performance
            )
            
            # Patterns temporels
            interaction_metrics['temporal_patterns'] = self._analyze_temporal_interaction_patterns(
                coord_events, steps
            )
        
        # Analyser la sp√©cialisation pour d√©duire la coordination
        if 'agent_specialization' in patterns:
            spec = patterns['agent_specialization']
            
            # Calculer les m√©triques de coordination bas√©es sur la sp√©cialisation
            coordination_from_specialization = self._infer_coordination_from_specialization(spec)
            
            # Fusionner avec les m√©triques existantes
            if interaction_metrics['coordination_quality']['synchronization_score'] == 0:
                interaction_metrics['coordination_quality'].update(coordination_from_specialization)
        
        # Analyser l'√©mergence de communication
        interaction_metrics['communication_emergence'] = self._analyze_communication_emergence(
            patterns, performance
        )
        
        # Calculer des scores globaux d'interaction
        interaction_metrics['interaction_quality_scores'] = self._calculate_interaction_quality_scores(
            interaction_metrics
        )
        
        return interaction_metrics
    
    def _categorize_interaction_types(self, coordination_events: List) -> Dict:
        """
        Cat√©gorise les types d'interactions observ√©es.
        
        TYPES D'INTERACTIONS:
        - √âchanges d'objets
        - Actions de support
        - Conflits/interf√©rences
        - Coordination spatiale
        """
        interaction_types = {
            'coordination_events': coordination_events,
            'exchange_events': 0,
            'support_events': 0,
            'conflict_events': 0,
            'spatial_coordination': 0
        }
        
        # Analyser les √©v√©nements pour les cat√©goriser
        for event in coordination_events:
            if isinstance(event, str):
                if 'exchange' in event.lower() or 'pass' in event.lower():
                    interaction_types['exchange_events'] += 1
                elif 'support' in event.lower() or 'help' in event.lower():
                    interaction_types['support_events'] += 1
                elif 'conflict' in event.lower() or 'block' in event.lower():
                    interaction_types['conflict_events'] += 1
                elif 'spatial' in event.lower() or 'move' in event.lower():
                    interaction_types['spatial_coordination'] += 1
        
        return interaction_types
    
    def _assess_coordination_quality(self, coordination_events: List, patterns: Dict, performance: Dict) -> Dict:
        """
        √âvalue la qualit√© de la coordination entre agents.
        
        M√âTRIQUES DE QUALIT√â:
        - Score de synchronisation
        - Score de compl√©mentarit√©
        - Score d'efficacit√© coordinative
        """
        quality_assessment = {
            'synchronization_score': 0,
            'complementarity_score': 0,
            'efficiency_score': 0
        }
        
        # Score de synchronisation bas√© sur la fr√©quence des √©v√©nements de coordination
        num_events = len(coordination_events)
        steps = performance.get('avg_completion_steps', 600)
        
        if steps > 0:
            # Synchronisation √©lev√©e = √©v√©nements r√©guliers mais pas excessifs
            ideal_frequency = 0.05  # 1 √©v√©nement toutes les 20 steps environ
            actual_frequency = num_events / steps
            
            # Score optimal autour de la fr√©quence id√©ale
            sync_score = 1 - abs(actual_frequency - ideal_frequency) / ideal_frequency
            quality_assessment['synchronization_score'] = max(0, min(sync_score, 1))
        
        # Score de compl√©mentarit√© bas√© sur la sp√©cialisation
        if 'agent_specialization' in patterns:
            spec = patterns['agent_specialization']
            specialization_score = spec.get('avg_specialization_score', 0)
            
            # Compl√©mentarit√© √©lev√©e = sp√©cialisation √©lev√©e (r√¥les diff√©rents)
            quality_assessment['complementarity_score'] = specialization_score
        
        # Score d'efficacit√© coordinative = performance vs effort de coordination
        completion_efficiency = performance.get('completion_efficiency', 0)
        coordination_cost = num_events / max(steps, 1)  # Co√ªt en √©v√©nements par step
        
        # Efficacit√© √©lev√©e = bonne performance avec coordination mod√©r√©e
        if coordination_cost > 0:
            efficiency_ratio = completion_efficiency / coordination_cost
            quality_assessment['efficiency_score'] = min(efficiency_ratio / 10, 1)  # Normaliser
        else:
            quality_assessment['efficiency_score'] = completion_efficiency
        
        return quality_assessment
    
    def _analyze_temporal_interaction_patterns(self, coordination_events: List, total_steps: int) -> Dict:
        """
        Analyse les patterns temporels des interactions.
        
        PATTERNS TEMPORELS:
        - Rythme d'interaction
        - Phases de pic d'interaction
        - Consistance temporelle
        """
        temporal_patterns = {
            'interaction_rhythm': 'unknown',
            'peak_interaction_phases': [],
            'coordination_consistency': 0
        }
        
        if not coordination_events or total_steps == 0:
            return temporal_patterns
        
        num_events = len(coordination_events)
        
        # D√©terminer le rythme d'interaction
        events_per_step = num_events / total_steps
        
        if events_per_step < 0.01:
            rhythm = 'sparse'
        elif events_per_step < 0.05:
            rhythm = 'moderate'
        elif events_per_step < 0.1:
            rhythm = 'frequent'
        else:
            rhythm = 'intense'
        
        temporal_patterns['interaction_rhythm'] = rhythm
        
        # Analyser les phases de pic (simulation simplifi√©e)
        # Diviser le jeu en phases et identifier o√π les interactions sont concentr√©es
        num_phases = 4
        phase_length = total_steps // num_phases
        
        if phase_length > 0:
            # Simuler la distribution des √©v√©nements dans les phases
            events_per_phase = num_events / num_phases
            
            # Identifier les phases avec plus d'interactions que la moyenne
            peak_phases = []
            for i in range(num_phases):
                # Simulation : varier l'intensit√© selon la phase
                phase_multiplier = [0.8, 1.2, 1.5, 0.9][i]  # Pic en milieu de jeu
                phase_events = events_per_phase * phase_multiplier
                
                if phase_events > events_per_phase * 1.2:
                    peak_phases.append(f"phase_{i+1}")
            
            temporal_patterns['peak_interaction_phases'] = peak_phases
        
        # Calculer la consistance de coordination (uniformit√© temporelle)
        # Pour simplifier, utiliser l'inverse de la variance simul√©e
        if num_events > 0:
            # Consistance √©lev√©e si le rythme est mod√©r√© et r√©gulier
            if rhythm in ['moderate', 'frequent']:
                temporal_patterns['coordination_consistency'] = 0.8
            else:
                temporal_patterns['coordination_consistency'] = 0.4
        
        return temporal_patterns
    
    def _infer_coordination_from_specialization(self, specialization_data: Dict) -> Dict:
        """
        Inf√®re les m√©triques de coordination √† partir des donn√©es de sp√©cialisation.
        """
        coordination_metrics = {
            'synchronization_score': 0,
            'complementarity_score': 0,
            'efficiency_score': 0
        }
        
        # Synchronisation bas√©e sur la consistance de sp√©cialisation
        consistency = specialization_data.get('specialization_consistency', 0)
        coordination_metrics['synchronization_score'] = consistency
        
        # Compl√©mentarit√© bas√©e sur le score de sp√©cialisation
        specialization_score = specialization_data.get('avg_specialization_score', 0)
        coordination_metrics['complementarity_score'] = specialization_score
        
        # Efficacit√© bas√©e sur la combinaison des deux
        coordination_metrics['efficiency_score'] = (consistency + specialization_score) / 2
        
        return coordination_metrics
    
    def _analyze_communication_emergence(self, patterns: Dict, performance: Dict) -> Dict:
        """
        Analyse l'√©mergence de communication implicite entre agents.
        
        INDICATEURS DE COMMUNICATION:
        - Communication implicite d√©tect√©e
        - Efficacit√© de la communication
        - Qualit√© du transfert d'information
        """
        communication_analysis = {
            'implicit_communication_detected': False,
            'communication_efficiency': 0,
            'information_transfer_quality': 0
        }
        
        # D√©tecter la communication implicite via la coordination r√©ussie
        if 'agent_specialization' in patterns:
            spec = patterns['agent_specialization']
            specialization_score = spec.get('avg_specialization_score', 0)
            consistency = spec.get('specialization_consistency', 0)
            
            # Communication implicite d√©tect√©e si forte sp√©cialisation ET consistance
            if specialization_score > 0.6 and consistency > 0.6:
                communication_analysis['implicit_communication_detected'] = True
        
        # Efficacit√© de communication bas√©e sur la performance relative
        completion_efficiency = performance.get('completion_efficiency', 0)
        
        if communication_analysis['implicit_communication_detected']:
            # Si communication d√©tect√©e, efficacit√© = performance
            communication_analysis['communication_efficiency'] = completion_efficiency
        else:
            # Sinon, efficacit√© plus faible
            communication_analysis['communication_efficiency'] = completion_efficiency * 0.5
        
        # Qualit√© du transfert d'information (proxy via sp√©cialisation)
        if 'agent_specialization' in patterns:
            agent_0_roles = patterns['agent_specialization'].get('agent_0_dominant_roles', {})
            agent_1_roles = patterns['agent_specialization'].get('agent_1_dominant_roles', {})
            
            # Qualit√© √©lev√©e = r√¥les compl√©mentaires (peu de chevauchement)
            common_roles = set(agent_0_roles.keys()) & set(agent_1_roles.keys())
            total_roles = set(agent_0_roles.keys()) | set(agent_1_roles.keys())
            
            if total_roles:
                complementarity = 1 - (len(common_roles) / len(total_roles))
                communication_analysis['information_transfer_quality'] = complementarity
        
        return communication_analysis
    
    def _calculate_interaction_quality_scores(self, interaction_metrics: Dict) -> Dict:
        """
        Calcule des scores globaux de qualit√© d'interaction.
        
        SCORES GLOBAUX:
        - Qualit√© d'interaction globale
        - Efficacit√© coordinative
        - √âmergence comportementale
        - Score composite d'interaction
        """
        quality_scores = {
            'overall_interaction_quality': 0,
            'coordinative_efficiency': 0,
            'behavioral_emergence': 0,
            'composite_interaction_score': 0
        }
        
        # Qualit√© d'interaction globale = moyenne des scores de coordination
        if 'coordination_quality' in interaction_metrics:
            coord_quality = interaction_metrics['coordination_quality']
            scores = [
                coord_quality.get('synchronization_score', 0),
                coord_quality.get('complementarity_score', 0),
                coord_quality.get('efficiency_score', 0)
            ]
            quality_scores['overall_interaction_quality'] = np.mean([s for s in scores if s > 0])
        
        # Efficacit√© coordinative = ratio performance/effort
        if 'interaction_frequency' in interaction_metrics:
            freq = interaction_metrics['interaction_frequency']
            interaction_density = freq.get('interaction_density', 0)
            
            # Efficacit√© √©lev√©e = bonne coordination avec densit√© mod√©r√©e
            if interaction_density > 0:
                # Optimal autour de 0.05 interactions par step
                efficiency = 1 - abs(interaction_density - 0.05) / 0.05
                quality_scores['coordinative_efficiency'] = max(0, efficiency)
        
        # √âmergence comportementale = d√©tection de communication + adaptation
        if 'communication_emergence' in interaction_metrics:
            comm = interaction_metrics['communication_emergence']
            
            emergence_indicators = [
                1 if comm.get('implicit_communication_detected', False) else 0,
                comm.get('communication_efficiency', 0),
                comm.get('information_transfer_quality', 0)
            ]
            quality_scores['behavioral_emergence'] = np.mean(emergence_indicators)
        
        # Score composite = moyenne pond√©r√©e des scores individuels
        individual_scores = [
            quality_scores['overall_interaction_quality'],
            quality_scores['coordinative_efficiency'],
            quality_scores['behavioral_emergence']
        ]
        
        valid_scores = [s for s in individual_scores if s > 0]
        if valid_scores:
            quality_scores['composite_interaction_score'] = np.mean(valid_scores)
        
        return quality_scores
    
    def _identify_dominant_role(self, roles: Dict) -> str:
        """Identifie le r√¥le dominant d'un agent."""
        if not roles:
            return 'undefined'
        
        dominant_role = max(roles.items(), key=lambda x: x[1], default=('undefined', 0))[0]
        return dominant_role
    
    def _calculate_role_complementarity(self, specialization_data: Dict) -> float:
        """Calcule la compl√©mentarit√© des r√¥les entre agents."""
        agent_0_roles = specialization_data.get('agent_0_dominant_roles', {})
        agent_1_roles = specialization_data.get('agent_1_dominant_roles', {})
        
        # Mesurer √† quel point les r√¥les sont diff√©rents (compl√©mentaires)
        common_roles = set(agent_0_roles.keys()) & set(agent_1_roles.keys())
        total_roles = set(agent_0_roles.keys()) | set(agent_1_roles.keys())
        
        if not total_roles:
            return 0
        
        complementarity = 1 - (len(common_roles) / len(total_roles))
        return complementarity
    
    def _analyze_agent_specialization(self, mode_data: Dict) -> Dict:
        """Analyse la sp√©cialisation des agents pour un mode donn√©."""
        patterns = mode_data.get('behavioral_patterns', {})
        
        if 'agent_specialization' not in patterns:
            return {'specialization_detected': False}
        
        spec = patterns['agent_specialization']
        
        return {
            'specialization_detected': True,
            'specialization_score': spec.get('avg_specialization_score', 0),
            'consistency_score': spec.get('specialization_consistency', 0),
            'agent_0_dominant_role': self._identify_dominant_role(spec.get('agent_0_dominant_roles', {})),
            'agent_1_dominant_role': self._identify_dominant_role(spec.get('agent_1_dominant_roles', {})),
            'role_complementarity': self._calculate_role_complementarity(spec)
        }
    
    def _estimate_movement_from_roles(self, agent_roles: Dict, performance: Dict) -> float:
        """
        Estime la distance de mouvement d'un agent bas√©e sur ses r√¥les dominants.
        
        LOGIQUE:
        - R√¥les de pr√©paration (pickup, potting) = plus de mouvement
        - R√¥les de livraison (delivery) = mouvement mod√©r√© mais focalis√©
        - R√¥les de support = mouvement variable selon contexte
        """
        if not agent_roles:
            return 0
        
        # Coefficients de mouvement par type de r√¥le
        movement_coefficients = {
            'ingredient_gatherer': 150,    # Beaucoup de va-et-vient
            'pot_manager': 100,           # Mouvement mod√©r√© autour des pots
            'delivery_specialist': 120,   # Mouvement focalis√© vers zones de service
            'support_agent': 80,          # Mouvement d'adaptation
            'versatile_agent': 110,       # Mouvement √©quilibr√©
            'idle_agent': 20              # Peu de mouvement
        }
        
        # Calculer la distance estim√©e bas√©e sur les r√¥les
        total_movement = 0
        total_weight = 0
        
        for role, weight in agent_roles.items():
            coefficient = movement_coefficients.get(role, 100)  # Valeur par d√©faut
            total_movement += coefficient * weight
            total_weight += weight
        
        # Normaliser par le poids total
        estimated_distance = total_movement / max(total_weight, 1)
        
        # Ajuster selon la performance globale
        completion_efficiency = performance.get('completion_efficiency', 0.5)
        steps = performance.get('avg_completion_steps', 300)
        
        # Plus d'√©tapes = potentiellement plus de mouvement
        step_factor = min(steps / 200, 2.0)  # Normaliser autour de 200 steps
        
        # Efficacit√© √©lev√©e = mouvement plus optimal (moins de distance pour m√™me r√©sultat)
        efficiency_factor = max(0.5, 1 - completion_efficiency * 0.3)
        
        final_distance = estimated_distance * step_factor * efficiency_factor
        
        return final_distance
    
    def _analyze_spatial_distribution(self, agent_0_roles: Dict, agent_1_roles: Dict, 
                                    specialization_data: Dict) -> Dict:
        """
        Analyse la distribution spatiale des activit√©s des agents.
        
        M√âTRIQUES:
        - Couverture spatiale de chaque agent
        - Ratio de chevauchement des zones d'activit√©
        - Sp√©cialisation par zone fonctionnelle
        """
        spatial_dist = {
            'agent_0_coverage': 0,
            'agent_1_coverage': 0,
            'overlap_ratio': 0,
            'zone_specialization': {}
        }
        
        # Estimer la couverture spatiale bas√©e sur la diversit√© des r√¥les
        agent_0_coverage = len(agent_0_roles) / 6  # Normalis√© sur 6 r√¥les possibles max
        agent_1_coverage = len(agent_1_roles) / 6
        
        spatial_dist['agent_0_coverage'] = min(agent_0_coverage, 1.0)
        spatial_dist['agent_1_coverage'] = min(agent_1_coverage, 1.0)
        
        # Calculer le ratio de chevauchement (r√¥les partag√©s)
        common_roles = set(agent_0_roles.keys()) & set(agent_1_roles.keys())
        total_roles = set(agent_0_roles.keys()) | set(agent_1_roles.keys())
        
        if total_roles:
            spatial_dist['overlap_ratio'] = len(common_roles) / len(total_roles)
        
        # Analyser la sp√©cialisation par zone fonctionnelle
        zone_specializations = {
            'preparation_zone': self._calculate_zone_specialization('ingredient_gatherer', agent_0_roles, agent_1_roles),
            'cooking_zone': self._calculate_zone_specialization('pot_manager', agent_0_roles, agent_1_roles),
            'delivery_zone': self._calculate_zone_specialization('delivery_specialist', agent_0_roles, agent_1_roles)
        }
        
        spatial_dist['zone_specialization'] = zone_specializations
        
        return spatial_dist
    
    def _calculate_zone_specialization(self, zone_role: str, agent_0_roles: Dict, agent_1_roles: Dict) -> Dict:
        """Calcule la sp√©cialisation d'une zone sp√©cifique."""
        agent_0_weight = agent_0_roles.get(zone_role, 0)
        agent_1_weight = agent_1_roles.get(zone_role, 0)
        total_weight = agent_0_weight + agent_1_weight
        
        if total_weight == 0:
            return {'specialization': 'none', 'dominant_agent': None, 'specialization_score': 0}
        
        # D√©terminer l'agent dominant pour cette zone
        if agent_0_weight > agent_1_weight * 1.5:
            dominant_agent = 'agent_0'
            specialization = 'high'
            specialization_score = agent_0_weight / total_weight
        elif agent_1_weight > agent_0_weight * 1.5:
            dominant_agent = 'agent_1'
            specialization = 'high'
            specialization_score = agent_1_weight / total_weight
        else:
            dominant_agent = 'shared'
            specialization = 'balanced'
            specialization_score = 0.5
        
        return {
            'specialization': specialization,
            'dominant_agent': dominant_agent,
            'specialization_score': specialization_score,
            'agent_0_involvement': agent_0_weight / total_weight,
            'agent_1_involvement': agent_1_weight / total_weight
        }
    
    def _analyze_trajectory_optimization(self, specialization_data: Dict, performance: Dict) -> Dict:
        """
        Analyse l'optimisation des trajectoires des agents.
        
        M√âTRIQUES:
        - Score d'efficacit√© du chemin
        - Ratio de mouvements redondants
        - Adh√©rence au chemin optimal
        """
        trajectory_opt = {
            'path_efficiency_score': 0,
            'redundant_moves_ratio': 0,
            'optimal_path_adherence': 0
        }
        
        # Score d'efficacit√© bas√© sur la sp√©cialisation et la performance
        specialization_score = specialization_data.get('avg_specialization_score', 0)
        consistency_score = specialization_data.get('specialization_consistency', 0)
        completion_efficiency = performance.get('completion_efficiency', 0)
        
        # Efficacit√© du chemin = combinaison de sp√©cialisation et performance
        path_efficiency = (specialization_score + completion_efficiency) / 2
        trajectory_opt['path_efficiency_score'] = path_efficiency
        
        # Estimer les mouvements redondants (inverse de la consistance)
        redundant_ratio = max(0, 1 - consistency_score)
        trajectory_opt['redundant_moves_ratio'] = redundant_ratio
        
        # Adh√©rence au chemin optimal (bas√©e sur l'efficacit√© globale)
        optimal_adherence = completion_efficiency * consistency_score
        trajectory_opt['optimal_path_adherence'] = optimal_adherence
        
        return trajectory_opt
    
    def _identify_activity_zones(self, agent_0_roles: Dict, agent_1_roles: Dict, patterns: Dict) -> Dict:
        """
        Identifie les zones d'activit√© intensive pour les agents.
        
        M√âTRIQUES:
        - Nombre de zones d'activit√© intense
        - Transitions entre zones par agent
        - Consistance de focus par zone
        """
        activity_zones = {
            'hot_zones_count': 0,
            'zone_transitions_per_agent': [0, 0],
            'zone_focus_consistency': 0
        }
        
        # Identifier les zones d'activit√© bas√©es sur les r√¥les dominants
        all_roles = set(agent_0_roles.keys()) | set(agent_1_roles.keys())
        
        # Compter les zones actives (r√¥les avec poids significatif)
        hot_zones = []
        for role in all_roles:
            weight_0 = agent_0_roles.get(role, 0)
            weight_1 = agent_1_roles.get(role, 0)
            total_weight = weight_0 + weight_1
            
            if total_weight > 0.1:  # Seuil d'activit√© significative
                hot_zones.append(role)
        
        activity_zones['hot_zones_count'] = len(hot_zones)
        
        # Estimer les transitions entre zones (bas√© sur la diversit√© des r√¥les)
        agent_0_transitions = max(0, len(agent_0_roles) - 1)  # -1 car pas de transition si un seul r√¥le
        agent_1_transitions = max(0, len(agent_1_roles) - 1)
        
        activity_zones['zone_transitions_per_agent'] = [agent_0_transitions, agent_1_transitions]
        
        # Calculer la consistance de focus (inverse de la diversit√©)
        total_roles = len(all_roles)
        if total_roles > 0:
            # Focus √©lev√© = peu de r√¥les diff√©rents, focus faible = beaucoup de r√¥les
            focus_consistency = max(0, 1 - (total_roles - 1) / 5)  # Normalis√© sur 6 r√¥les max
            activity_zones['zone_focus_consistency'] = focus_consistency
        
        return activity_zones
    
    def _analyze_exploration_vs_exploitation(self, patterns: Dict) -> Dict:
        """
        Analyse l'√©quilibre exploration vs exploitation dans les comportements.
        
        M√âTRIQUES:
        - Score d'exploration (diversit√© des activit√©s)
        - Score d'exploitation (focus sur activit√©s efficaces)
        - √âquilibre exploration-exploitation
        """
        exploration_analysis = {
            'exploration_score': 0,
            'exploitation_score': 0,
            'balance_score': 0,
            'adaptive_behavior_detected': False
        }
        
        if 'agent_specialization' not in patterns:
            return exploration_analysis
        
        spec = patterns['agent_specialization']
        
        # Score d'exploration = diversit√© des r√¥les
        agent_0_roles = spec.get('agent_0_dominant_roles', {})
        agent_1_roles = spec.get('agent_1_dominant_roles', {})
        
        total_unique_roles = len(set(agent_0_roles.keys()) | set(agent_1_roles.keys()))
        exploration_score = min(total_unique_roles / 6, 1.0)  # Normalis√© sur 6 r√¥les max
        
        # Score d'exploitation = consistance et sp√©cialisation
        avg_specialization = spec.get('avg_specialization_score', 0)
        consistency = spec.get('specialization_consistency', 0)
        exploitation_score = (avg_specialization + consistency) / 2
        
        # √âquilibre = proximit√© de 0.5 entre exploration et exploitation
        balance_score = 1 - abs(exploration_score - exploitation_score)
        
        # D√©tection de comportement adaptatif
        adaptive_behavior = (exploration_score > 0.3 and exploitation_score > 0.3 and balance_score > 0.6)
        
        exploration_analysis = {
            'exploration_score': exploration_score,
            'exploitation_score': exploitation_score,
            'balance_score': balance_score,
            'adaptive_behavior_detected': adaptive_behavior
        }
        
        return exploration_analysis
    
    def _calculate_movement_quality_scores(self, movement_metrics: Dict) -> Dict:
        """
        Calcule des scores globaux de qualit√© de mouvement.
        
        SCORES:
        - Efficacit√© globale de mouvement
        - Optimisation spatiale
        - Coordination des mouvements
        - Score de qualit√© composite
        """
        quality_scores = {
            'overall_efficiency': 0,
            'spatial_optimization': 0,
            'movement_coordination': 0,
            'composite_quality_score': 0
        }
        
        # Efficacit√© globale = moyenne des efficacit√©s individuelles
        if 'movement_efficiency' in movement_metrics:
            eff = movement_metrics['movement_efficiency']
            quality_scores['overall_efficiency'] = eff.get('overall', 0)
        
        # Optimisation spatiale = combinaison de couverture et d'optimisation de trajectoire
        if 'spatial_distribution' in movement_metrics and 'trajectory_optimization' in movement_metrics:
            spatial = movement_metrics['spatial_distribution']
            trajectory = movement_metrics['trajectory_optimization']
            
            coverage_balance = min(spatial.get('agent_0_coverage', 0), spatial.get('agent_1_coverage', 0))
            path_efficiency = trajectory.get('path_efficiency_score', 0)
            
            spatial_optimization = (coverage_balance + path_efficiency) / 2
            quality_scores['spatial_optimization'] = spatial_optimization
        
        # Coordination des mouvements = √©quilibre et compl√©mentarit√©
        if 'total_distance' in movement_metrics and 'spatial_distribution' in movement_metrics:
            distance_balance = movement_metrics['total_distance'].get('balance', 0)
            overlap_ratio = movement_metrics['spatial_distribution'].get('overlap_ratio', 0)
            
            # Coordination √©lev√©e = bon √©quilibre ET faible chevauchement (compl√©mentarit√©)
            movement_coordination = (distance_balance + (1 - overlap_ratio)) / 2
            quality_scores['movement_coordination'] = movement_coordination
        
        # Score composite = moyenne pond√©r√©e des scores individuels
        scores = [
            quality_scores['overall_efficiency'],
            quality_scores['spatial_optimization'],
            quality_scores['movement_coordination']
        ]
        
        valid_scores = [s for s in scores if s > 0]
        if valid_scores:
            quality_scores['composite_quality_score'] = np.mean(valid_scores)
        
        return quality_scores
        """Identifie le r√¥le dominant d'un agent."""
        if not roles:
            return 'undefined'
        
        dominant_role = max(roles.items(), key=lambda x: x[1], default=('undefined', 0))[0]
        return dominant_role
    
    def _calculate_role_complementarity(self, specialization_data: Dict) -> float:
        """Calcule la compl√©mentarit√© des r√¥les entre agents."""
        agent_0_roles = specialization_data.get('agent_0_dominant_roles', {})
        agent_1_roles = specialization_data.get('agent_1_dominant_roles', {})
        
        # Mesurer √† quel point les r√¥les sont diff√©rents (compl√©mentaires)
        common_roles = set(agent_0_roles.keys()) & set(agent_1_roles.keys())
        total_roles = set(agent_0_roles.keys()) | set(agent_1_roles.keys())
        
        if not total_roles:
            return 0
        
        complementarity = 1 - (len(common_roles) / len(total_roles))
        return complementarity
    
    def _analyze_behavioral_efficiency(self, mode_data: Dict) -> Dict:
        """Analyse l'efficacit√© comportementale pour un mode donn√©."""
        performance = mode_data.get('performance_metrics', {})
        
        return {
            'completion_efficiency': performance.get('completion_efficiency', 0),
            'resource_utilization': performance.get('resource_utilization', 0),
            'waste_minimization': performance.get('waste_minimization', 0),
            'overall_behavioral_efficiency': performance.get('overall_efficiency', 0)
        }
    
    def _compare_behavioral_modes(self, behavioral_modes: Dict) -> Dict:
        """Compare les comportements entre diff√©rents modes."""
        if len(behavioral_modes) < 2:
            return {'comparison_possible': False, 'reason': 'insufficient_modes'}
        
        comparisons = {
            'mode_diversity': 0,
            'consistency_across_modes': 0,
            'mode_specific_adaptations': {},
            'optimal_mode_identification': None
        }
        
        # Calculer la diversit√© entre modes
        success_rates = [mode.get('success_rate', 0) for mode in behavioral_modes.values()]
        comparisons['mode_diversity'] = np.std(success_rates) if success_rates else 0
        
        # Identifier le mode optimal
        best_mode = max(behavioral_modes.items(), 
                       key=lambda x: x[1].get('success_rate', 0),
                       default=(None, {}))[0]
        comparisons['optimal_mode_identification'] = best_mode
        
        return comparisons
    
    def _identify_adaptation_indicators(self, behavioral_modes: Dict) -> Dict:
        """Identifie les indicateurs d'adaptation comportementale."""
        indicators = {
            'adaptation_detected': False,
            'adaptation_mechanisms': [],
            'flexibility_score': 0,
            'robustness_score': 0
        }
        
        if len(behavioral_modes) >= 2:
            indicators['adaptation_detected'] = True
            indicators['flexibility_score'] = len(behavioral_modes) / 5  # Normalis√© sur 5 modes max
            
            # Analyser la robustesse (consistance des performances)
            success_rates = [mode.get('success_rate', 0) for mode in behavioral_modes.values()]
            indicators['robustness_score'] = 1 - np.std(success_rates) if success_rates else 0
        
        return indicators
    
    # ========================================
    # 3. CARACT√âRISATION PAR PERFORMANCE
    # ========================================
    
    def characterize_performance_profile(self, layout_name: str, layout_data: Dict) -> Dict:
        """
        Caract√©rise le profil de performance d'un layout.
        
        M√âTRIQUES DE PERFORMANCE:
        - Taux de r√©ussite et distribution
        - Temps de compl√©tion et variabilit√©
        - Efficacit√© relative entre modes
        - Courbes d'apprentissage
        - Ratios effort/r√©compense
        - Consistance des r√©sultats
        """
        performance = {
            'success_metrics': {},
            'timing_metrics': {},
            'efficiency_metrics': {},
            'consistency_metrics': {},
            'comparative_metrics': {}
        }
        
        # M√©triques de succ√®s globales
        performance['success_metrics'] = self._calculate_success_metrics(layout_data)
        
        # M√©triques temporelles
        performance['timing_metrics'] = self._calculate_timing_metrics(layout_data)
        
        # M√©triques d'efficacit√©
        performance['efficiency_metrics'] = self._calculate_performance_efficiency_metrics(layout_data)
        
        # M√©triques de consistance
        performance['consistency_metrics'] = self._calculate_consistency_metrics(layout_data)
        
        # M√©triques comparatives (par rapport aux autres layouts)
        performance['comparative_metrics'] = self._calculate_comparative_performance(layout_data)
        
        return performance
    
    def _calculate_success_metrics(self, layout_data: Dict) -> Dict:
        """Calcule les m√©triques de succ√®s."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        success_rates = [mode.get('success_rate', 0) for mode in behavioral_modes.values()]
        
        return {
            'overall_success_rate': np.mean(success_rates) if success_rates else 0,
            'best_success_rate': max(success_rates) if success_rates else 0,
            'worst_success_rate': min(success_rates) if success_rates else 0,
            'success_rate_variance': np.var(success_rates) if success_rates else 0,
            'success_consistency': 1 - np.std(success_rates) if success_rates else 0,
            'modes_with_success': len([rate for rate in success_rates if rate > 0]),
            'total_modes_tested': len(success_rates)
        }
    
    def _calculate_timing_metrics(self, layout_data: Dict) -> Dict:
        """Calcule les m√©triques temporelles."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        completion_times = []
        for mode_data in behavioral_modes.values():
            perf = mode_data.get('performance_metrics', {})
            if 'avg_completion_steps' in perf:
                completion_times.append(perf['avg_completion_steps'])
        
        if not completion_times:
            return {'timing_data_available': False}
        
        return {
            'timing_data_available': True,
            'avg_completion_time': np.mean(completion_times),
            'fastest_completion': min(completion_times),
            'slowest_completion': max(completion_times),
            'completion_time_variance': np.var(completion_times),
            'timing_predictability': 1 - (np.std(completion_times) / np.mean(completion_times)) if np.mean(completion_times) > 0 else 0
        }
    
    def _calculate_performance_efficiency_metrics(self, layout_data: Dict) -> Dict:
        """Calcule les m√©triques d'efficacit√© de performance."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        efficiency_scores = []
        for mode_data in behavioral_modes.values():
            perf = mode_data.get('performance_metrics', {})
            efficiency = perf.get('completion_efficiency', 0)
            efficiency_scores.append(efficiency)
        
        return {
            'avg_efficiency': np.mean(efficiency_scores) if efficiency_scores else 0,
            'peak_efficiency': max(efficiency_scores) if efficiency_scores else 0,
            'efficiency_variance': np.var(efficiency_scores) if efficiency_scores else 0,
            'efficiency_improvement_potential': max(efficiency_scores) - min(efficiency_scores) if efficiency_scores else 0
        }
    
    def _calculate_consistency_metrics(self, layout_data: Dict) -> Dict:
        """Calcule les m√©triques de consistance."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        # Extraire diff√©rentes m√©triques pour mesurer la consistance
        success_rates = [mode.get('success_rate', 0) for mode in behavioral_modes.values()]
        
        performance_values = []
        for mode_data in behavioral_modes.values():
            perf = mode_data.get('performance_metrics', {})
            performance_values.append(perf.get('completion_efficiency', 0))
        
        return {
            'result_consistency': 1 - np.std(success_rates) if success_rates else 0,
            'performance_consistency': 1 - np.std(performance_values) if performance_values else 0,
            'overall_consistency': (1 - np.std(success_rates) + 1 - np.std(performance_values)) / 2 if success_rates and performance_values else 0,
            'predictability_score': self._calculate_predictability_score(layout_data)
        }
    
    def _calculate_predictability_score(self, layout_data: Dict) -> float:
        """Calcule un score de pr√©dictibilit√© bas√© sur la variance des r√©sultats."""
        # Pour l'instant, utiliser la consistance des taux de succ√®s comme proxy
        behavioral_modes = layout_data.get('behavioral_modes', {})
        success_rates = [mode.get('success_rate', 0) for mode in behavioral_modes.values()]
        
        if not success_rates:
            return 0
        
        # Pr√©dictibilit√© = 1 - coefficient de variation
        mean_success = np.mean(success_rates)
        if mean_success == 0:
            return 0
        
        cv = np.std(success_rates) / mean_success
        predictability = max(0, 1 - cv)
        
        return predictability
    
    def _calculate_comparative_performance(self, layout_data: Dict) -> Dict:
        """Calcule les m√©triques comparatives par rapport aux autres layouts."""
        # Placeholder pour les m√©triques comparatives
        # N√©cessite d'avoir acc√®s aux donn√©es de tous les layouts
        return {
            'relative_difficulty': 0.5,  # √Ä impl√©menter avec donn√©es compl√®tes
            'performance_percentile': 0.5,  # √Ä impl√©menter
            'outlier_status': 'normal',  # √Ä impl√©menter
            'note': 'Comparative metrics require full dataset analysis'
        }
    
    # ========================================
    # 4. CARACT√âRISATION PAR COORDINATION
    # ========================================
    
    def characterize_coordination_requirements(self, layout_name: str, layout_data: Dict) -> Dict:
        """
        Caract√©rise les exigences de coordination d'un layout.
        
        M√âTRIQUES DE COORDINATION:
        - N√©cessit√© de coordination (solo vs coop)
        - Types d'interactions requises
        - S√©quencement des t√¢ches
        - Sp√©cialisation des r√¥les
        - Complexit√© coordinative
        """
        coordination = {
            'coordination_necessity': {},
            'interaction_requirements': {},
            'role_dynamics': {},
            'task_sequencing': {},
            'coordination_complexity': {}
        }
        
        coord_analysis = layout_data.get('coordination_analysis', {})
        
        # N√©cessit√© de coordination
        coordination['coordination_necessity'] = self._analyze_coordination_necessity(coord_analysis)
        
        # Exigences d'interaction
        coordination['interaction_requirements'] = self._analyze_interaction_requirements(layout_data)
        
        # Dynamiques de r√¥les
        coordination['role_dynamics'] = self._analyze_role_dynamics(layout_data)
        
        # S√©quencement des t√¢ches
        coordination['task_sequencing'] = self._analyze_task_sequencing(layout_data)
        
        # Complexit√© coordinative
        coordination['coordination_complexity'] = self._calculate_coordination_complexity(layout_data)
        
        return coordination
    
    def _analyze_coordination_necessity(self, coord_analysis: Dict) -> Dict:
        """Analyse la n√©cessit√© de coordination."""
        necessity = coord_analysis.get('coordination_necessity', 'unknown')
        solo_vs_team = coord_analysis.get('solo_vs_team_effectiveness', {})
        
        return {
            'necessity_level': necessity,
            'team_advantage': solo_vs_team.get('team_advantage', 0),
            'relative_effectiveness': solo_vs_team.get('relative_effectiveness', 1),
            'coordination_benefit': solo_vs_team.get('team_advantage', 0) > 0.1,
            'solo_viability': solo_vs_team.get('relative_effectiveness', 1) > 0.5
        }
    
    def _analyze_interaction_requirements(self, layout_data: Dict) -> Dict:
        """Analyse les exigences d'interaction."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        interaction_types = []
        interaction_frequencies = []
        
        for mode_data in behavioral_modes.values():
            patterns = mode_data.get('behavioral_patterns', {})
            if 'coordination_events' in patterns:
                events = patterns['coordination_events']
                interaction_types.extend(events)
                interaction_frequencies.append(len(events))
        
        return {
            'interaction_types_observed': list(set(interaction_types)),
            'avg_interactions_per_game': np.mean(interaction_frequencies) if interaction_frequencies else 0,
            'interaction_diversity': len(set(interaction_types)),
            'interaction_intensity': max(interaction_frequencies) if interaction_frequencies else 0
        }
    
    def _analyze_role_dynamics(self, layout_data: Dict) -> Dict:
        """Analyse les dynamiques de r√¥les."""
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        specialization_scores = []
        role_distributions = []
        
        for mode_data in behavioral_modes.values():
            patterns = mode_data.get('behavioral_patterns', {})
            if 'agent_specialization' in patterns:
                spec = patterns['agent_specialization']
                specialization_scores.append(spec.get('avg_specialization_score', 0))
                
                # Analyser la distribution des r√¥les
                agent_0_roles = spec.get('agent_0_dominant_roles', {})
                agent_1_roles = spec.get('agent_1_dominant_roles', {})
                role_distributions.append((agent_0_roles, agent_1_roles))
        
        return {
            'avg_specialization': np.mean(specialization_scores) if specialization_scores else 0,
            'specialization_consistency': 1 - np.std(specialization_scores) if specialization_scores else 0,
            'role_flexibility': self._calculate_role_flexibility(role_distributions),
            'dominant_role_patterns': self._identify_dominant_role_patterns(role_distributions)
        }
    
    def _calculate_role_flexibility(self, role_distributions: List) -> float:
        """Calcule la flexibilit√© des r√¥les."""
        if not role_distributions:
            return 0
        
        # Mesurer la variabilit√© des r√¥les entre les parties
        all_roles = set()
        for agent_0_roles, agent_1_roles in role_distributions:
            all_roles.update(agent_0_roles.keys())
            all_roles.update(agent_1_roles.keys())
        
        # Flexibilit√© = nombre de r√¥les diff√©rents observ√©s
        flexibility = len(all_roles) / 10  # Normalis√© arbitrairement
        return min(flexibility, 1.0)
    
    def _identify_dominant_role_patterns(self, role_distributions: List) -> Dict:
        """Identifie les patterns de r√¥les dominants."""
        if not role_distributions:
            return {}
        
        # Compter les r√¥les les plus fr√©quents
        agent_0_role_counts = Counter()
        agent_1_role_counts = Counter()
        
        for agent_0_roles, agent_1_roles in role_distributions:
            # Prendre le r√¥le dominant de chaque agent
            if agent_0_roles:
                dominant_role_0 = max(agent_0_roles.items(), key=lambda x: x[1])[0]
                agent_0_role_counts[dominant_role_0] += 1
            
            if agent_1_roles:
                dominant_role_1 = max(agent_1_roles.items(), key=lambda x: x[1])[0]
                agent_1_role_counts[dominant_role_1] += 1
        
        return {
            'agent_0_preferred_roles': dict(agent_0_role_counts.most_common(3)),
            'agent_1_preferred_roles': dict(agent_1_role_counts.most_common(3)),
            'role_complementarity': len(set(agent_0_role_counts.keys()) & set(agent_1_role_counts.keys())) == 0
        }
    
    def _analyze_task_sequencing(self, layout_data: Dict) -> Dict:
        """Analyse le s√©quencement des t√¢ches."""
        # Placeholder pour l'analyse de s√©quencement
        return {
            'sequencing_complexity': 0.5,  # √Ä impl√©menter
            'critical_path_length': 0,     # √Ä impl√©menter
            'parallelization_potential': 0.5,  # √Ä impl√©menter
            'note': 'Task sequencing analysis requires detailed event timeline data'
        }
    
    def _calculate_coordination_complexity(self, layout_data: Dict) -> Dict:
        """Calcule la complexit√© coordinative."""
        coord_analysis = layout_data.get('coordination_analysis', {})
        behavioral_modes = layout_data.get('behavioral_modes', {})
        
        # Facteurs de complexit√©
        team_advantage = coord_analysis.get('solo_vs_team_effectiveness', {}).get('team_advantage', 0)
        num_interaction_types = len(set(
            event for mode_data in behavioral_modes.values()
            for event in mode_data.get('behavioral_patterns', {}).get('coordination_events', [])
        ))
        
        # Score de complexit√© composite
        complexity_score = (abs(team_advantage) + num_interaction_types / 10) / 2
        
        return {
            'complexity_score': min(complexity_score, 1.0),
            'complexity_level': self._categorize_coordination_complexity(complexity_score),
            'coordination_necessity': coord_analysis.get('coordination_necessity', 'unknown'),
            'interaction_diversity': num_interaction_types
        }
    
    def _categorize_coordination_complexity(self, complexity_score: float) -> str:
        """Cat√©gorise la complexit√© coordinative."""
        if complexity_score < 0.25:
            return 'minimal'
        elif complexity_score < 0.5:
            return 'moderate'
        elif complexity_score < 0.75:
            return 'high'
        else:
            return 'very_high'
    
    # ========================================
    # M√âTHODES D'ORCHESTRATION
    # ========================================
    
    def characterize_all_layouts(self, output_file: str = "layout_characterization_complete.json") -> Dict:
        """
        Effectue une caract√©risation compl√®te de tous les layouts.
        """
        if not self.layout_data:
            print("‚ùå Aucune donn√©e charg√©e")
            return {}
        
        print(f"\nüèóÔ∏è CARACT√âRISATION COMPL√àTE DES LAYOUTS")
        print(f"=" * 60)
        
        characterization = {
            'metadata': {
                'characterization_timestamp': pd.Timestamp.now().isoformat(),
                'layouts_characterized': 0,
                'characterization_methods': [
                    'structural', 'behavioral', 'performance', 'coordination'
                ]
            },
            'individual_characterizations': {},
            'comparative_analysis': {},
            'clustering_results': {},
            'layout_taxonomy': {},
            'recommendations': {}
        }
        
        layouts = self.layout_data.get('layouts', {})
        total_layouts = len(layouts)
        
        print(f"üìä Caract√©risation de {total_layouts} layouts...")
        
        # Caract√©riser chaque layout individuellement
        for i, (layout_name, layout_data) in enumerate(layouts.items(), 1):
            print(f"   üèóÔ∏è Layout {i}/{total_layouts}: {layout_name}")
            
            layout_characterization = {
                'structural_features': self.characterize_structural_features(layout_name, layout_data),
                'behavioral_patterns': self.characterize_behavioral_patterns(layout_name, layout_data),
                'performance_profile': self.characterize_performance_profile(layout_name, layout_data),
                'coordination_requirements': self.characterize_coordination_requirements(layout_name, layout_data)
            }
            
            characterization['individual_characterizations'][layout_name] = layout_characterization
        
        characterization['metadata']['layouts_characterized'] = len(characterization['individual_characterizations'])
        
        # Analyses comparatives
        print("üîç Analyse comparative...")
        characterization['comparative_analysis'] = self._perform_comparative_analysis(
            characterization['individual_characterizations']
        )
        
        # Clustering des layouts
        print("üéØ Clustering des layouts...")
        characterization['clustering_results'] = self._perform_layout_clustering(
            characterization['individual_characterizations']
        )
        
        # Taxonomie des layouts
        print("üìö Construction de la taxonomie...")
        characterization['layout_taxonomy'] = self._build_layout_taxonomy(
            characterization['individual_characterizations']
        )
        
        # Recommandations
        print("üí° G√©n√©ration de recommandations...")
        characterization['recommendations'] = self._generate_characterization_recommendations(
            characterization
        )
        
        # Sauvegarder les r√©sultats
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(characterization, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Caract√©risation compl√®te termin√©e!")
        print(f"üìä R√©sultats sauvegard√©s: {output_file}")
        
        self.characterization_results = characterization
        return characterization
    
    def _perform_comparative_analysis(self, individual_chars: Dict) -> Dict:
        """Effectue une analyse comparative entre tous les layouts."""
        # Placeholder pour l'analyse comparative d√©taill√©e
        return {
            'difficulty_ranking': [],  # √Ä impl√©menter
            'similarity_matrix': {},   # √Ä impl√©menter
            'performance_comparison': {},  # √Ä impl√©menter
            'note': 'Comparative analysis implementation in progress'
        }
    
    def _perform_layout_clustering(self, individual_chars: Dict) -> Dict:
        """Effectue un clustering des layouts bas√© sur leurs caract√©ristiques."""
        # Placeholder pour le clustering
        return {
            'clusters_identified': 0,  # √Ä impl√©menter
            'cluster_characteristics': {},  # √Ä impl√©menter
            'outlier_layouts': [],  # √Ä impl√©menter
            'note': 'Clustering implementation in progress'
        }
    
    def _build_layout_taxonomy(self, individual_chars: Dict) -> Dict:
        """Construit une taxonomie des layouts."""
        # Placeholder pour la taxonomie
        return {
            'taxonomy_levels': [],  # √Ä impl√©menter
            'layout_categories': {},  # √Ä impl√©menter
            'classification_rules': {},  # √Ä impl√©menter
            'note': 'Taxonomy building implementation in progress'
        }
    
    def _generate_characterization_recommendations(self, characterization: Dict) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur la caract√©risation."""
        recommendations = [
            "Implementation of detailed spatial analysis for structural characterization",
            "Development of movement pattern tracking for behavioral analysis",
            "Creation of comparative metrics across all layouts",
            "Implementation of machine learning clustering algorithms",
            "Development of predictive models for layout difficulty"
        ]
        
        return recommendations
    
    def create_characterization_visualizations(self, output_dir: str = "characterization_plots"):
        """Cr√©e des visualisations des r√©sultats de caract√©risation."""
        Path(output_dir).mkdir(exist_ok=True)
        
        if not self.characterization_results:
            print("‚ùå Aucune caract√©risation disponible pour la visualisation")
            return
        
        print(f"üìä Cr√©ation des visualisations dans: {output_dir}/")
        
        # Placeholder pour les visualisations
        plt.figure(figsize=(10, 6))
        plt.text(0.5, 0.5, 'Characterization Visualizations\nComing Soon!', 
                horizontalalignment='center', verticalalignment='center',
                fontsize=16, transform=plt.gca().transAxes)
        plt.axis('off')
        plt.savefig(f"{output_dir}/characterization_overview.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Visualisations cr√©√©es (placeholder)")


def main():
    parser = argparse.ArgumentParser(description="Caract√©risateur de layouts Overcooked")
    parser.add_argument('data_file', help='Fichier de donn√©es de layouts √† caract√©riser')
    parser.add_argument('--output', default='layout_characterization_complete.json',
                       help='Fichier de sortie pour la caract√©risation')
    parser.add_argument('--plots', default='characterization_plots',
                       help='R√©pertoire pour les graphiques')
    parser.add_argument('--no-plots', action='store_true',
                       help='D√©sactiver la g√©n√©ration de graphiques')
    
    args = parser.parse_args()
    
    print("üèóÔ∏è CARACT√âRISATEUR DE LAYOUTS OVERCOOKED")
    print("=" * 50)
    
    characterizer = LayoutCharacterizer()
    
    if not characterizer.load_layout_data(args.data_file):
        return 1
    
    # Effectuer la caract√©risation compl√®te
    characterization_results = characterizer.characterize_all_layouts(args.output)
    
    # Cr√©er les visualisations
    if not args.no_plots:
        try:
            characterizer.create_characterization_visualizations(args.plots)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration des graphiques: {e}")
    
    print(f"\n‚úÖ Caract√©risation termin√©e!")
    print(f"üìä R√©sultats sauvegard√©s: {args.output}")
    
    return 0


if __name__ == "__main__":
    exit(main())
