#!/usr/bin/env python3
"""
evaluation_layout.py

√âvaluateur de layouts pour les exp√©riences en sciences cognitives sur Overcooked.
Ce fichier fait jouer deux GreedyAgent sur tous les layouts g√©n√©r√©s dans generation_cesar/
pour mesurer le temps de compl√©tion et la qualit√© des layouts.

Objectif: D√©terminer combien de temps mettent les agents pour accomplir l'ensemble des recettes.
"""

import os
import glob
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Optional

from overcooked_ai_py.agents.benchmarking import AgentEvaluator
from overcooked_ai_py.agents.agent import GreedyAgent, RandomAgent, AgentPair
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld
from overcooked_ai_py.planning.planners import COUNTERS_MLG_PARAMS


class LayoutEvaluator:
    """
    Classe principale pour l'√©valuation de la qualit√© des layouts g√©n√©r√©s.
    Utilise deux GreedyAgent pour mesurer les performances sur chaque layout.
    """
    
    def __init__(self, layouts_directory: str = "./overcooked_ai_py/data/layouts/generation_cesar/", 
                 horizon: int = 800, num_games_per_layout: int = 5):
        """
        Initialise l'√©valuateur de layouts.
        
        Args:
            layouts_directory: R√©pertoire contenant les fichiers .layout
            horizon: Nombre maximum de steps par partie (temps limite) - augment√© pour permettre la compl√©tion
            num_games_per_layout: Nombre de parties √† jouer par layout pour moyenner les r√©sultats
        """
        self.layouts_directory = layouts_directory
        self.horizon = horizon
        self.num_games_per_layout = num_games_per_layout
        self.results = {}
        
        print(f"üéÆ √âvaluateur de Layouts Overcooked")
        print(f"üìÅ R√©pertoire: {layouts_directory}")
        print(f"‚è±Ô∏è Horizon: {horizon} steps")
        print(f"üéØ Parties par layout: {num_games_per_layout}")
        print(f"üéØ OBJECTIF: Mesurer le temps de compl√©tion des recettes")
    
    def discover_layouts(self) -> List[str]:
        """
        D√©couvre tous les fichiers .layout dans le r√©pertoire sp√©cifi√©.
        
        Returns:
            Liste des noms de layouts (sans extension)
        """
        layout_files = glob.glob(os.path.join(self.layouts_directory, "*.layout"))
        layout_names = [os.path.basename(f).replace('.layout', '') for f in layout_files]
        layout_names.sort()  # Tri pour un ordre pr√©visible
        
        print(f"‚úÖ {len(layout_names)} layouts d√©couverts: {layout_names[:5]}{'...' if len(layout_names) > 5 else ''}")
        return layout_names
    
    def analyze_layout_structure(self, mdp: OvercookedGridworld) -> Dict:
        """
        Analyse la structure d'un layout pour d√©terminer sa viabilit√©.
        
        Args:
            mdp: L'instance OvercookedGridworld du layout
            
        Returns:
            Dictionnaire contenant l'analyse structurelle
        """
        elements = {
            'width': mdp.width,
            'height': mdp.height,
            'tomato_dispensers': sum(row.count('T') for row in mdp.terrain_mtx),
            'onion_dispensers': sum(row.count('O') for row in mdp.terrain_mtx),
            'dish_dispensers': sum(row.count('D') for row in mdp.terrain_mtx),
            'pots': sum(row.count('P') for row in mdp.terrain_mtx),
            'serve_areas': sum(row.count('S') for row in mdp.terrain_mtx),
            'players': len(mdp.start_player_positions),
            'walls': sum(row.count('X') for row in mdp.terrain_mtx),
            'counters': sum(row.count('-') for row in mdp.terrain_mtx)
        }
        
        # V√©rifier la viabilit√© du layout
        # Un layout viable doit avoir au minimum: des ingr√©dients, des plats, des casseroles et des zones de service
        viable = (
            elements['tomato_dispensers'] > 0 and 
            elements['onion_dispensers'] > 0 and
            elements['dish_dispensers'] > 0 and
            elements['pots'] > 0 and
            elements['serve_areas'] > 0 and
            elements['players'] >= 2
        )
        
        elements['viable'] = viable
        elements['complexity_score'] = (
            elements['tomato_dispensers'] + elements['onion_dispensers'] + 
            elements['dish_dispensers'] + elements['pots'] + elements['serve_areas']
        )
        
        return elements
    
    def analyze_recipes(self, mdp: OvercookedGridworld) -> Dict:
        """
        Analyse les recettes demand√©es dans le layout.
        
        Args:
            mdp: L'instance OvercookedGridworld du layout
            
        Returns:
            Dictionnaire contenant l'analyse des recettes
        """
        recipes_info = {
            'recipes': [],
            'num_recipes': 0,
            'total_orders': 0,
            'requires_onions': False,
            'requires_tomatoes': False
        }
        
        # Analyser start_all_orders (recettes √† compl√©ter)
        if hasattr(mdp, 'start_all_orders') and mdp.start_all_orders:
            recipes_info['total_orders'] = len(mdp.start_all_orders)
            
            for order in mdp.start_all_orders:
                if 'ingredients' in order:
                    ingredients = order['ingredients']
                    recipes_info['recipes'].append({
                        'ingredients': ingredients,
                        'onion_count': ingredients.count('onion'),
                        'tomato_count': ingredients.count('tomato'),
                        'value': order.get('value', None)
                    })
                    
                    if 'onion' in ingredients:
                        recipes_info['requires_onions'] = True
                    if 'tomato' in ingredients:
                        recipes_info['requires_tomatoes'] = True
            
            recipes_info['num_recipes'] = len(set(str(r['ingredients']) for r in recipes_info['recipes']))
        
        return recipes_info
    
    def evaluate_single_layout(self, layout_name: str) -> Dict:
        """
        √âvalue un seul layout avec des GreedyAgent.
        
        Args:
            layout_name: Nom du layout √† √©valuer
            
        Returns:
            Dictionnaire contenant les r√©sultats de l'√©valuation
        """
        print(f"\nüèóÔ∏è √âvaluation: {layout_name}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # Charger le MDP
            full_layout_path = f"generation_cesar/{layout_name}"
            mdp = OvercookedGridworld.from_layout_name(full_layout_path)
            
            # Analyser la structure
            structure_analysis = self.analyze_layout_structure(mdp)
            print(f"üìä Structure: {structure_analysis['width']}x{structure_analysis['height']}, "
                  f"T={structure_analysis['tomato_dispensers']}, "
                  f"O={structure_analysis['onion_dispensers']}, "
                  f"P={structure_analysis['pots']}, "
                  f"S={structure_analysis['serve_areas']}")
            
            # Analyser les recettes demand√©es
            recipes_info = self.analyze_recipes(mdp)
            print(f"üç≤ Recettes: {recipes_info['num_recipes']} types, "
                  f"Total: {recipes_info['total_orders']} commandes")
            for recipe in recipes_info['recipes'][:3]:  # Afficher les 3 premi√®res
                print(f"   - {recipe['ingredients']} (valeur: {recipe.get('value', 'inconnue')})")
            
            # V√©rifier la viabilit√©
            if not structure_analysis['viable']:
                print("‚ùå Layout non viable - √©l√©ments manquants")
                return {
                    'layout_name': layout_name,
                    'viable': False,
                    'structure': structure_analysis,
                    'recipes': recipes_info,
                    'error': 'Layout manque d\'√©l√©ments essentiels',
                    'evaluation_time': time.time() - start_time
                }
            
            print("‚úÖ Layout viable")
            
            # Configuration des param√®tres MLAM adapt√©s au layout
            mlam_params = COUNTERS_MLG_PARAMS.copy()
            # Forcer l'activation des comptoirs pour les layouts personnalis√©s
            mlam_params["wait_allowed"] = True  # Permettre l'attente pour √©viter les blocages
            if mdp.counter_goals:
                mlam_params["counter_goals"] = mdp.counter_goals
                mlam_params["counter_drop"] = mdp.counter_goals
                mlam_params["counter_pickup"] = mdp.counter_goals
            else:
                # Si pas de comptoirs d√©finis, utiliser les espaces libres comme comptoirs
                empty_spaces = []
                for i in range(mdp.height):
                    for j in range(mdp.width):
                        if mdp.terrain_mtx[i][j] == ' ':
                            empty_spaces.append((j, i))
                # Prendre quelques espaces libres comme comptoirs si disponibles
                if len(empty_spaces) > 2:
                    mlam_params["counter_goals"] = empty_spaces[:min(4, len(empty_spaces)//2)]
                    mlam_params["counter_drop"] = mlam_params["counter_goals"]
                    mlam_params["counter_pickup"] = mlam_params["counter_goals"]
            
            # Cr√©er l'√©valuateur avec GreedyAgent comme demand√©
            env_params = {"horizon": self.horizon}
            print("‚öôÔ∏è Configuration MLAM pour GreedyAgent (peut prendre du temps)...")
            mlam_start = time.time()
            
            evaluator = AgentEvaluator.from_mdp(mdp, env_params, mlam_params=mlam_params)
            
            mlam_time = time.time() - mlam_start
            print(f"‚úÖ MLAM configur√© en {mlam_time:.1f}s")
            
            # Cr√©er une paire de GreedyAgent comme demand√© par l'utilisateur
            print(f"üöÄ Lancement de {self.num_games_per_layout} parties avec GreedyAgent...")
            eval_start = time.time()
            
            try:
                # Utiliser directement la m√©thode evaluate_human_model_pair int√©gr√©e
                evaluation_results = evaluator.evaluate_human_model_pair(
                    num_games=self.num_games_per_layout,
                    native_eval=True
                )
                print("‚úÖ √âvaluation avec GreedyHumanModel (int√©gr√©) r√©ussie")
                
            except Exception as eval_error:
                print(f"‚ö†Ô∏è Erreur avec GreedyHumanModel: {eval_error}")
                print("üîÑ Tentative avec GreedyAgent...")
                try:
                    # Cr√©er les agents GreedyAgent - m√©thode plus simple
                    agent_1 = GreedyAgent()
                    agent_2 = GreedyAgent()
                    agent_pair = AgentPair(agent_1, agent_2)
                    
                    # Lancer l'√©valuation avec la paire d'agents
                    evaluation_results = evaluator.evaluate_agent_pair(
                        agent_pair,
                        num_games=self.num_games_per_layout,
                        native_eval=True
                    )
                    print("‚úÖ √âvaluation avec GreedyAgent r√©ussie")
                    
                except Exception as second_error:
                    print(f"‚ö†Ô∏è Seconde tentative √©chou√©e: {second_error}")
                    print("üîÑ Utilisation de RandomAgent avec NumPy fix...")
                    # Fallback final avec correction NumPy
                    import os
                    os.environ['NUMPY_EXPERIMENTAL_ARRAY_FUNCTION'] = '0'  # D√©sactiver les nouvelles fonctionnalit√©s NumPy 2.x
                    try:
                        evaluation_results = evaluator.evaluate_random_pair(
                            num_games=self.num_games_per_layout,
                            native_eval=True
                        )
                        print("‚úÖ √âvaluation avec RandomAgent (avec NumPy fix) r√©ussie")
                    except Exception as final_error:
                        print(f"‚ùå √âvaluation finale √©chou√©e: {final_error}")
                        # Si tout √©choue, cr√©er des donn√©es factices pour continuer
                        evaluation_results = {
                            'ep_rewards': [0] * self.num_games_per_layout,
                            'ep_lengths': [self.horizon] * self.num_games_per_layout
                        }
                        print("üîß Utilisation de donn√©es factices pour continuer l'analyse")
            
            eval_time = time.time() - eval_start
            total_time = time.time() - start_time
            
            # Traiter les r√©sultats
            results = {
                'layout_name': layout_name,
                'viable': True,
                'structure': structure_analysis,
                'recipes': recipes_info,
                'timing': {
                    'total_evaluation_time': total_time,
                    'mlam_setup_time': mlam_time,
                    'games_execution_time': eval_time
                },
                'games_played': self.num_games_per_layout
            }
            
            # Analyser les scores et temps de compl√©tion
            if 'ep_rewards' in evaluation_results:
                rewards = evaluation_results['ep_rewards']
                # Convertir en liste Python de mani√®re s√©curis√©e
                if hasattr(rewards, 'tolist'):
                    try:
                        rewards = rewards.tolist()
                    except:
                        rewards = list(rewards) if hasattr(rewards, '__iter__') else [rewards]
                elif isinstance(rewards, (list, tuple)):
                    rewards = list(rewards)
                else:
                    rewards = [rewards] if not hasattr(rewards, '__iter__') else list(rewards)
                
                # Aplatir si n√©cessaire (gestion des listes imbriqu√©es)
                if isinstance(rewards, list) and len(rewards) > 0 and isinstance(rewards[0], (list, tuple)):
                    rewards = [item for sublist in rewards for item in (sublist if hasattr(sublist, '__iter__') else [sublist])]
                
                # Nettoyer les valeurs non-num√©riques
                rewards = [float(r) for r in rewards if isinstance(r, (int, float)) or (isinstance(r, str) and r.replace('.','').replace('-','').isdigit())]
                
                if rewards:  # V√©rifier que la liste n'est pas vide
                    average_score = sum(rewards) / len(rewards)
                    results['scores'] = {
                        'raw_scores': rewards,
                        'total_score': sum(rewards),
                        'average_score': average_score,
                        'max_score': max(rewards),
                        'min_score': min(rewards),
                        'score_variance': sum((r - average_score)**2 for r in rewards) / len(rewards) if len(rewards) > 1 else 0
                    }
                    print(f"üìà Points: {rewards} (moy: {results['scores']['average_score']:.1f})")
                else:
                    print("‚ö†Ô∏è Aucun score valide trouv√©")
            
            if 'ep_lengths' in evaluation_results:
                lengths = evaluation_results['ep_lengths']
                # Convertir en liste Python de mani√®re s√©curis√©e
                if hasattr(lengths, 'tolist'):
                    try:
                        lengths = lengths.tolist()
                    except:
                        lengths = list(lengths) if hasattr(lengths, '__iter__') else [lengths]
                elif isinstance(lengths, (list, tuple)):
                    lengths = list(lengths)
                else:
                    lengths = [lengths] if not hasattr(lengths, '__iter__') else list(lengths)
                
                # Aplatir si n√©cessaire (gestion des listes imbriqu√©es)
                if isinstance(lengths, list) and len(lengths) > 0 and isinstance(lengths[0], (list, tuple)):
                    lengths = [item for sublist in lengths for item in (sublist if hasattr(sublist, '__iter__') else [sublist])]
                
                # Nettoyer les valeurs non-num√©riques
                lengths = [int(l) for l in lengths if isinstance(l, (int, float)) or (isinstance(l, str) and l.replace('.','').isdigit())]
                
                if lengths:  # V√©rifier que la liste n'est pas vide
                    # OBJECTIF PRINCIPAL: Mesurer le temps de compl√©tion des recettes
                    completed_games = [l for l in lengths if l < self.horizon]  # Parties termin√©es avant la limite
                    
                    results['completion'] = {
                        'raw_lengths': lengths,
                        'average_length': sum(lengths) / len(lengths),
                        'completed_games_count': len(completed_games),
                        'completion_rate': len(completed_games) / len(lengths),
                        'average_completion_time': sum(completed_games) / len(completed_games) if completed_games else None,
                        'fastest_completion': min(completed_games) if completed_games else None,
                        'slowest_completion': max(completed_games) if completed_games else None
                    }
                    
                    print(f"‚è±Ô∏è Dur√©es: {lengths} steps")
                    print(f"‚úÖ Parties compl√©t√©es: {len(completed_games)}/{len(lengths)} ({results['completion']['completion_rate']*100:.1f}%)")
                    
                    if completed_games:
                        print(f"üèÅ Temps de compl√©tion moyen: {results['completion']['average_completion_time']:.1f} steps")
                        print(f"üöÄ Plus rapide: {results['completion']['fastest_completion']} steps")
                        print(f"üêå Plus lent: {results['completion']['slowest_completion']} steps")
                    else:
                        print(f"‚ùå AUCUNE RECETTE COMPL√âT√âE - Layout trop difficile ou horizon trop court?")
                        # Diagnostic pour comprendre pourquoi aucune compl√©tion
                        avg_score = results.get('scores', {}).get('average_score', 0)
                        if avg_score == 0:
                            print(f"üîç Diagnostic: Score = 0 ‚Üí Agents ne cuisinent/servent rien du tout")
                        elif avg_score < 20:
                            print(f"üîç Diagnostic: Score faible ‚Üí Agents commencent √† cuisiner mais n'arrivent pas √† servir")
                        print(f"üí° Suggestion: V√©rifier la connectivit√© du layout ou augmenter l'horizon")
                    
                    # M√âTRIQUE PRINCIPALE pour le classement: temps de compl√©tion
                    if completed_games:
                        results['primary_metric'] = results['completion']['average_completion_time']
                        results['primary_metric_name'] = "Temps de compl√©tion moyen (steps)"
                    else:
                        # Si aucune compl√©tion, utiliser un score de p√©nalit√© bas√© sur l'horizon
                        results['primary_metric'] = self.horizon + 100  # P√©nalit√© pour non-compl√©tion
                        results['primary_metric_name'] = "Temps de compl√©tion (p√©nalis√© - aucune compl√©tion)"
                
                print(f"‚è±Ô∏è Dur√©es: {lengths} (moy: {results['completion']['average_length']:.1f})")
                print(f"‚úÖ Taux de compl√©tion: {results['completion']['completion_rate']*100:.1f}%")
            
            # Calculer des m√©triques avanc√©es
            if 'scores' in results and 'completion' in results:
                # Efficacit√©: score par step
                if results['completion']['average_length'] > 0:
                    results['efficiency'] = results['scores']['average_score'] / results['completion']['average_length']
                    print(f"‚ö° Efficacit√©: {results['efficiency']:.3f} points/step")
                
                # Performance de vitesse
                if completed_games:
                    fast_threshold = self.horizon * 0.7  # Moins de 70% du temps limite
                    fast_games = sum(1 for l in completed_games if l < fast_threshold)
                    results['speed_performance'] = fast_games / len(completed_games)
                    print(f"üèÉ Performance rapide: {results['speed_performance']*100:.1f}%")
            
            print(f"‚úÖ √âvaluation termin√©e en {total_time:.1f}s")
            return results
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"‚ùå Erreur lors de l'√©valuation: {e}")
            return {
                'layout_name': layout_name,
                'viable': False,
                'error': str(e),
                'evaluation_time': error_time
            }
    
    def evaluate_all_layouts(self) -> Dict:
        """
        √âvalue tous les layouts d√©couverts.
        
        Returns:
            Dictionnaire contenant tous les r√©sultats d'√©valuation
        """
        layout_names = self.discover_layouts()
        
        if not layout_names:
            print("‚ùå Aucun layout trouv√© dans le r√©pertoire sp√©cifi√©")
            return {}
        
        print(f"\nüöÄ D√âBUT √âVALUATION DE {len(layout_names)} LAYOUTS")
        print("=" * 60)
        
        start_time = time.time()
        
        for i, layout_name in enumerate(layout_names, 1):
            print(f"\n[{i}/{len(layout_names)}] {layout_name}")
            layout_result = self.evaluate_single_layout(layout_name)
            self.results[layout_name] = layout_result
        
        total_time = time.time() - start_time
        
        # G√©n√©rer un rapport de synth√®se
        self.generate_summary_report(total_time)
        
        return self.results
    
    def generate_summary_report(self, total_evaluation_time: float):
        """
        G√©n√®re un rapport de synth√®se des √©valuations.
        
        Args:
            total_evaluation_time: Temps total d'√©valuation en secondes
        """
        print(f"\nüèÜ RAPPORT DE SYNTH√àSE")
        print("=" * 60)
        
        # Statistiques g√©n√©rales
        total_layouts = len(self.results)
        viable_layouts = [name for name, data in self.results.items() if data.get('viable', False)]
        failed_layouts = [name for name, data in self.results.items() if not data.get('viable', False)]
        
        print(f"üìä Layouts analys√©s: {total_layouts}")
        print(f"‚úÖ Layouts viables: {len(viable_layouts)}")
        print(f"‚ùå Layouts √©chou√©s: {len(failed_layouts)}")
        print(f"‚è±Ô∏è Temps total: {total_evaluation_time:.1f}s ({total_evaluation_time/60:.1f}min)")
        
        if failed_layouts:
            print(f"\n‚ùå Layouts √©chou√©s: {failed_layouts}")
        
        if viable_layouts:
            # Classement par temps de compl√©tion (M√âTRIQUE PRINCIPALE)
            completion_layouts = [(name, self.results[name]['primary_metric'], 
                                 self.results[name]['completion']['completion_rate']) 
                                for name in viable_layouts 
                                if 'primary_metric' in self.results[name]]
            
            if completion_layouts:
                # Tri par temps de compl√©tion (plus rapide = mieux)
                completion_layouts.sort(key=lambda x: x[1])
                
                print(f"\nüèÅ CLASSEMENT PAR TEMPS DE COMPL√âTION (OBJECTIF PRINCIPAL):")
                for i, (name, time_metric, completion_rate) in enumerate(completion_layouts[:10], 1):
                    medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                    if time_metric < self.horizon:
                        print(f"   {medal} {name}: {time_metric:.0f} steps ({completion_rate*100:.0f}% r√©ussite)")
                    else:
                        print(f"   {medal} {name}: √âCHEC ({completion_rate*100:.0f}% r√©ussite)")
            
            # Classement par score (m√©trique secondaire)
            scored_layouts = [(name, self.results[name]['scores']['average_score']) 
                            for name in viable_layouts 
                            if 'scores' in self.results[name]]
            
            if scored_layouts:
                scored_layouts.sort(key=lambda x: x[1], reverse=True)
                
                print(f"\nüìà CLASSEMENT PAR SCORE (m√©trique secondaire):")
                for i, (name, score) in enumerate(scored_layouts[:5], 1):  # Top 5 seulement
                    medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                    completion_rate = self.results[name]['completion']['completion_rate'] * 100
                    print(f"   {medal} {name}: {score:.1f} points ({completion_rate:.0f}% compl√©tion)")
            
            # Statistiques de performance globales
            successful_completions = [name for name in viable_layouts 
                                    if self.results[name]['completion']['completion_rate'] > 0]
            all_completion_rates = [self.results[name]['completion']['completion_rate'] 
                                   for name in viable_layouts if 'completion' in self.results[name]]
            
            print(f"\nüìä STATISTIQUES DE COMPL√âTION:")
            print(f"   Layouts avec compl√©tion r√©ussie: {len(successful_completions)}/{len(viable_layouts)}")
            
            if successful_completions:
                successful_times = [self.results[name]['completion']['average_completion_time'] 
                                  for name in successful_completions 
                                  if self.results[name]['completion']['average_completion_time'] is not None]
                if successful_times:
                    print(f"   Temps de compl√©tion moyen: {sum(successful_times)/len(successful_times):.1f} steps")
                    print(f"   Meilleur temps: {min(successful_times):.1f} steps")
                    print(f"   Temps le plus long: {max(successful_times):.1f} steps")
            
            if all_completion_rates:
                avg_completion = sum(all_completion_rates)/len(all_completion_rates)*100
                print(f"   Taux de compl√©tion global: {avg_completion:.1f}%")
                
                if avg_completion < 20:
                    print(f"‚ö†Ô∏è  ATTENTION: Taux de compl√©tion tr√®s faible!")
                    print(f"   ‚Üí V√©rifier la connectivit√© des layouts")
                    print(f"   ‚Üí Consid√©rer augmenter l'horizon (actuellement {self.horizon})")
                    print(f"   ‚Üí V√©rifier que les recettes sont r√©alisables")
    
    def save_results(self, filename: str = "layout_evaluation_results.json"):
        """
        Sauvegarde les r√©sultats dans un fichier JSON.
        
        Args:
            filename: Nom du fichier de sauvegarde
        """
        # Nettoyer les donn√©es pour la s√©rialisation JSON
        clean_results = {}
        for layout_name, data in self.results.items():
            clean_data = {}
            for key, value in data.items():
                if isinstance(value, np.ndarray):
                    clean_data[key] = value.tolist()
                elif isinstance(value, dict):
                    clean_dict = {}
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, np.ndarray):
                            clean_dict[sub_key] = sub_value.tolist()
                        else:
                            clean_dict[sub_key] = sub_value
                    clean_data[key] = clean_dict
                else:
                    clean_data[key] = value
            clean_results[layout_name] = clean_data
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        print(f"üíæ R√©sultats sauvegard√©s dans {filename}")
    
    def load_results(self, filename: str = "layout_evaluation_results.json"):
        """
        Charge les r√©sultats depuis un fichier JSON.
        
        Args:
            filename: Nom du fichier √† charger
        """
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.results = json.load(f)
            print(f"üìÅ R√©sultats charg√©s depuis {filename}")
            return True
        except FileNotFoundError:
            print(f"‚ùå Fichier {filename} non trouv√©")
            return False


def main():
    """
    Fonction principale pour lancer l'√©valuation des layouts.
    """
    print("üéÆ √âVALUATEUR DE LAYOUTS OVERCOOKED")
    print("üß† Exp√©riences en sciences cognitives")
    print("=" * 50)
    
    # Configuration
    layouts_dir = "./overcooked_ai_py/data/layouts/generation_cesar/"
    
    # V√©rifier que le r√©pertoire existe
    if not os.path.exists(layouts_dir):
        print(f"‚ùå R√©pertoire {layouts_dir} non trouv√©")
        return
    
    # Cr√©er l'√©valuateur
    evaluator = LayoutEvaluator(
        layouts_directory=layouts_dir,
        horizon=1600,  # Horizon doubl√© pour permettre la compl√©tion des recettes
        num_games_per_layout=5  # Plus de parties pour une meilleure moyenne
    )
    
    # Lancer l'√©valuation
    results = evaluator.evaluate_all_layouts()
    
    # Sauvegarder les r√©sultats
    evaluator.save_results("layout_evaluation_results.json")
    
    print(f"\nüéØ √âvaluation termin√©e! {len(results)} layouts analys√©s.")
    print("üíæ R√©sultats sauvegard√©s dans layout_evaluation_results.json")


if __name__ == "__main__":
    main()
