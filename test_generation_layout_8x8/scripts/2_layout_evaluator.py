#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
√âvaluateur exhaustif qui teste tous les layouts avec tous les groupes de recettes
Chaque layout est √©valu√© avec chaque groupe de 6 recettes
L'ID du groupe de recettes est inclus dans le nom du layout √©valu√©
"""

import json
import time
import argparse
from pathlib import Path
import random
from collections import defaultdict
import multiprocessing as mp

class ExhaustiveEvaluator:
    """√âvaluateur exhaustif layout x groupes de recettes."""
    
    def __init__(self):
        """Initialise l'√©valuateur exhaustif."""
        self.base_dir = Path(__file__).parent.parent
        self.layouts_dir = self.base_dir / "outputs" / "validated_layouts"
        self.evaluation_dir = self.base_dir / "outputs" / "exhaustive_evaluation"
        self.evaluation_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"üéØ √âvaluateur exhaustif initialis√©")
        print(f"üìÅ Layouts: {self.layouts_dir}")
        print(f"üìÅ R√©sultats: {self.evaluation_dir}")
    
    def load_recipe_groups(self):
        """Charge tous les groupes de recettes."""
        # Chercher le fichier de groupes le plus r√©cent
        group_files = list(self.base_dir.glob("outputs/all_recipe_groups_*.json"))
        
        if not group_files:
            print("‚ùå Aucun fichier de groupes de recettes trouv√©")
            print("   Ex√©cutez d'abord: python3 scripts/0_recipe_groups_generator.py")
            return None
        
        # Prendre le plus r√©cent
        latest_file = max(group_files, key=lambda f: f.stat().st_mtime)
        
        print(f"üìä Chargement: {latest_file.name}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        recipe_groups = data['recipe_groups']
        base_recipes = data['base_recipes']
        
        print(f"‚úÖ {len(recipe_groups):,} groupes de recettes charg√©s")
        print(f"‚úÖ {len(base_recipes)} recettes de base")
        
        return recipe_groups, base_recipes
    
    def load_layouts(self, sample_size=None):
        """Charge les layouts √† √©valuer."""
        layout_files = list(self.layouts_dir.glob("*.json"))
        
        if not layout_files:
            print(f"‚ùå Aucun fichier de layout trouv√© dans {self.layouts_dir}")
            return []
        
        all_layouts = []
        for layout_file in layout_files:
            try:
                with open(layout_file, 'r') as f:
                    data = json.load(f)
                    if 'layouts' in data:
                        all_layouts.extend(data['layouts'])
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lecture {layout_file}: {e}")
        
        if len(all_layouts) == 0:
            print("‚ùå Aucun layout valide trouv√©")
            return []
        
        # √âchantillonner si demand√©
        if sample_size and len(all_layouts) > sample_size:
            all_layouts = random.sample(all_layouts, sample_size)
        
        return all_layouts
    
    def analyze_layout_structure(self, layout_str):
        """Analyse la structure d'un layout (repris de l'√©valuateur structural)."""
        lines = layout_str.strip().split('\n')
        grid = [list(line) for line in lines]
        
        # Positions des √©l√©ments
        positions = self.extract_positions(grid)
        
        # Calculs de distances et connectivit√©
        connectivity_score = self.calculate_connectivity(positions)
        distance_efficiency = self.calculate_distance_efficiency(positions)
        interaction_potential = self.calculate_interaction_potential(positions)
        
        return {
            'connectivity_score': connectivity_score,
            'distance_efficiency': distance_efficiency,
            'interaction_potential': interaction_potential,
            'positions': positions
        }
    
    def extract_positions(self, grid):
        """Extrait les positions de tous les objets."""
        positions = {
            'player1': None,
            'player2': None,
            'onion_dispenser': [],
            'tomato_dispenser': [],
            'pot': [],
            'dish_dispenser': [],
            'serving_station': [],
            'counter': [],
            'walls': [],
            'empty': []
        }
        
        for i, row in enumerate(grid):
            for j, cell in enumerate(row):
                pos = (i, j)
                if cell == '1':
                    positions['player1'] = pos
                elif cell == '2':
                    positions['player2'] = pos
                elif cell == 'O':
                    positions['onion_dispenser'].append(pos)
                elif cell == 'T':
                    positions['tomato_dispenser'].append(pos)
                elif cell == 'P':
                    positions['pot'].append(pos)
                elif cell == 'D':
                    positions['dish_dispenser'].append(pos)
                elif cell == 'S':
                    positions['serving_station'].append(pos)
                elif cell == 'Y':
                    positions['counter'].append(pos)
                elif cell == 'X':
                    positions['walls'].append(pos)
                elif cell == '.':
                    positions['empty'].append(pos)
        
        return positions
    
    def manhattan_distance(self, pos1, pos2):
        """Calcule la distance de Manhattan entre deux positions."""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
    
    def calculate_connectivity(self, positions):
        """Calcule un score de connectivit√©."""
        if not positions['player1'] or not positions['player2']:
            return 0
        
        player_distance = self.manhattan_distance(positions['player1'], positions['player2'])
        
        essential_objects = (
            positions['onion_dispenser'] + positions['tomato_dispenser'] +
            positions['pot'] + positions['dish_dispenser'] + positions['serving_station']
        )
        
        if not essential_objects:
            return 0
        
        p1_distances = [self.manhattan_distance(positions['player1'], obj) for obj in essential_objects]
        p2_distances = [self.manhattan_distance(positions['player2'], obj) for obj in essential_objects]
        
        avg_p1_distance = sum(p1_distances) / len(p1_distances)
        avg_p2_distance = sum(p2_distances) / len(p2_distances)
        
        optimal_player_distance = 3
        distance_score = 1 / (1 + abs(player_distance - optimal_player_distance))
        
        max_distance = 10
        access_score = (2 * max_distance - avg_p1_distance - avg_p2_distance) / (2 * max_distance)
        access_score = max(0, min(1, access_score))
        
        return (distance_score + access_score) / 2
    
    def calculate_distance_efficiency(self, positions):
        """Calcule l'efficacit√© des distances."""
        if not positions['player1'] or not positions['player2']:
            return 0
        
        workflow_distances = []
        
        for player_pos in [positions['player1'], positions['player2']]:
            ingredient_distances = []
            for ingredient_pos in positions['onion_dispenser'] + positions['tomato_dispenser']:
                ingredient_distances.append(self.manhattan_distance(player_pos, ingredient_pos))
            
            pot_distances = []
            for pot_pos in positions['pot']:
                pot_distances.append(self.manhattan_distance(player_pos, pot_pos))
            
            dish_distances = []
            for dish_pos in positions['dish_dispenser']:
                dish_distances.append(self.manhattan_distance(player_pos, dish_pos))
            
            serving_distances = []
            for serving_pos in positions['serving_station']:
                serving_distances.append(self.manhattan_distance(player_pos, serving_pos))
            
            if (ingredient_distances and pot_distances and 
                dish_distances and serving_distances):
                
                min_workflow = (
                    min(ingredient_distances) + min(pot_distances) +
                    min(dish_distances) + min(serving_distances)
                )
                workflow_distances.append(min_workflow)
        
        if not workflow_distances:
            return 0
        
        avg_workflow = sum(workflow_distances) / len(workflow_distances)
        max_workflow = 20
        
        efficiency_score = (max_workflow - avg_workflow) / max_workflow
        return max(0, min(1, efficiency_score))
    
    def calculate_interaction_potential(self, positions):
        """Calcule le potentiel d'interaction."""
        if not positions['player1'] or not positions['player2']:
            return 0
        
        interaction_zones = positions['counter'] + positions['empty']
        
        shared_zones = 0
        for zone in interaction_zones:
            p1_dist = self.manhattan_distance(positions['player1'], zone)
            p2_dist = self.manhattan_distance(positions['player2'], zone)
            
            if p1_dist <= 3 and p2_dist <= 3:
                shared_zones += 1
        
        max_zones = len(interaction_zones)
        if max_zones == 0:
            return 0
        
        base_score = shared_zones / max_zones
        counter_ratio = len(positions['counter']) / (len(positions['counter']) + len(positions['empty']) + 1)
        
        return min(1, base_score * (1 + counter_ratio * 0.5))
    
    def estimate_metrics_for_recipe_group(self, structural_analysis, recipe_group):
        """
        Estime les m√©triques pour un groupe de recettes sp√©cifique.
        
        Args:
            structural_analysis: Analyse structurelle du layout
            recipe_group: Groupe de 6 recettes
            
        Returns:
            dict: M√©triques estim√©es pour ce groupe
        """
        recipes = recipe_group['recipes']
        
        # Calculer la complexit√© du groupe
        total_ingredients = sum(len(recipe['ingredients']) for recipe in recipes)
        avg_complexity = recipe_group['avg_complexity']
        max_complexity = max(recipe['complexity'] for recipe in recipes)
        
        # Facteurs de base
        efficiency_factor = structural_analysis['distance_efficiency']
        connectivity_factor = structural_analysis['connectivity_score']
        interaction_factor = structural_analysis['interaction_potential']
        
        # Estimation duo steps (influenced by recipe complexity)
        base_steps = 120
        complexity_multiplier = 1 + (avg_complexity - 1) * 0.3  # Plus complexe = plus de steps
        ingredient_multiplier = 1 + (total_ingredients - 12) * 0.02  # Plus d'ingr√©dients = plus de steps
        
        estimated_duo_steps = base_steps * complexity_multiplier * ingredient_multiplier
        estimated_duo_steps *= (2 - efficiency_factor - connectivity_factor * 0.5)
        estimated_duo_steps = max(60, min(400, estimated_duo_steps))
        
        # Estimation solo steps (toujours plus √©lev√©, plus affect√© par la complexit√©)
        solo_multiplier = 1.8 + (avg_complexity - 1) * 0.2
        estimated_solo_steps = estimated_duo_steps * solo_multiplier
        
        # Estimation cooperation gain
        if estimated_solo_steps > 0:
            cooperation_gain = (estimated_solo_steps - estimated_duo_steps) / estimated_solo_steps * 100
        else:
            cooperation_gain = 0
        
        # Estimation exchanges (influenced by recipe complexity and interaction potential)
        base_exchanges = interaction_factor * 6
        complexity_exchange_bonus = (avg_complexity - 1) * 0.5  # Plus complexe = plus d'√©changes
        estimated_exchanges = base_exchanges + complexity_exchange_bonus
        estimated_exchanges = max(0, min(12, estimated_exchanges))
        
        return {
            'estimated_duo_steps': round(estimated_duo_steps),
            'estimated_solo_steps': round(estimated_solo_steps),
            'estimated_cooperation_gain': round(cooperation_gain, 1),
            'estimated_exchanges': round(estimated_exchanges, 1),
            'recipe_complexity_score': round(avg_complexity * 20, 1),
            'recipe_group_id': recipe_group['group_id'],
            'recipe_ids': recipe_group['recipe_ids']
        }
    
    def evaluate_layout_with_all_groups(self, layout_data, recipe_groups):
        """
        √âvalue un layout avec tous les groupes de recettes.
        
        Args:
            layout_data: Donn√©es du layout
            recipe_groups: Liste de tous les groupes de recettes
            
        Returns:
            list: Liste des √©valuations pour chaque groupe
        """
        layout_id = layout_data['hash']
        layout_str = layout_data['grid']
        
        # Analyse structurelle une seule fois par layout
        structural_analysis = self.analyze_layout_structure(layout_str)
        
        evaluations = []
        
        for recipe_group in recipe_groups:
            # Cr√©er un nom unique combinant layout et groupe de recettes
            combined_id = f"{layout_id}_{recipe_group['group_id']}"
            
            # Estimer les m√©triques pour ce groupe de recettes
            metrics = self.estimate_metrics_for_recipe_group(structural_analysis, recipe_group)
            
            evaluation = {
                'layout_id': layout_id,
                'recipe_group_id': recipe_group['group_id'],
                'combined_id': combined_id,
                'layout_hash': layout_data['hash'],
                'n_empty': layout_data['n_empty'],
                'density_type': layout_data.get('density_type', 'unknown'),
                'recipe_group_info': {
                    'group_id': recipe_group['group_id'],
                    'recipe_ids': recipe_group['recipe_ids'],
                    'avg_complexity': recipe_group['avg_complexity'],
                    'total_ingredients': recipe_group['total_ingredients'],
                    'complexity_distribution': recipe_group['complexity_distribution']
                },
                'structural_analysis': structural_analysis,
                'estimated_metrics': metrics
            }
            
            evaluations.append(evaluation)
        
        return evaluations
    
    def evaluate_batch_worker(self, args):
        """Worker pour √©valuation parall√®le."""
        layouts_batch, recipe_groups, batch_id = args
        
        print(f"üìä Worker {batch_id}: traitement de {len(layouts_batch)} layouts")
        
        all_evaluations = []
        
        for i, layout in enumerate(layouts_batch):
            if i % 5 == 0:
                print(f"   Worker {batch_id}: layout {i}/{len(layouts_batch)}")
            
            layout_evaluations = self.evaluate_layout_with_all_groups(layout, recipe_groups)
            all_evaluations.extend(layout_evaluations)
        
        print(f"‚úÖ Worker {batch_id}: {len(all_evaluations)} √©valuations compl√©t√©es")
        
        return all_evaluations
    
    def run_exhaustive_evaluation(self, layout_sample_size=None, recipe_group_sample_size=None, processes=None):
        """
        Lance l'√©valuation exhaustive.
        
        Args:
            layout_sample_size: Nombre de layouts √† √©valuer (None = tous)
            recipe_group_sample_size: Nombre de groupes de recettes (None = tous)
            processes: Nombre de processus parall√®les
        """
        print(f"üöÄ √âVALUATION EXHAUSTIVE")
        print("="*70)
        
        # Charger les groupes de recettes
        recipe_data = self.load_recipe_groups()
        if recipe_data is None:
            return False
        
        recipe_groups, base_recipes = recipe_data
        
        # √âchantillonner les groupes de recettes si demand√©
        if recipe_group_sample_size and len(recipe_groups) > recipe_group_sample_size:
            recipe_groups = random.sample(recipe_groups, recipe_group_sample_size)
            print(f"üìã √âchantillon de {len(recipe_groups)} groupes de recettes")
        
        # Charger les layouts
        layouts = self.load_layouts(layout_sample_size)
        if not layouts:
            return False
        
        print(f"üìä Configuration d'√©valuation:")
        print(f"   - Layouts: {len(layouts)}")
        print(f"   - Groupes de recettes: {len(recipe_groups):,}")
        print(f"   - √âvaluations totales: {len(layouts) * len(recipe_groups):,}")
        
        # V√©rifier si c'est raisonnable
        total_evaluations = len(layouts) * len(recipe_groups)
        if total_evaluations > 1_000_000:
            print(f"‚ö†Ô∏è  ATTENTION: {total_evaluations:,} √©valuations au total!")
            print("   Cela peut prendre beaucoup de temps...")
        
        # Configuration du parall√©lisme
        if processes is None:
            processes = min(mp.cpu_count() - 1, 4)  # Limiter pour √©viter surcharge
        
        print(f"üöÄ √âvaluation parall√®le avec {processes} processus")
        
        start_time = time.time()
        
        # Diviser les layouts en batches pour parall√©lisation
        batch_size = max(1, len(layouts) // processes)
        layout_batches = [layouts[i:i+batch_size] for i in range(0, len(layouts), batch_size)]
        
        # Pr√©parer les arguments pour les workers
        worker_args = [(batch, recipe_groups, i) for i, batch in enumerate(layout_batches)]
        
        # Ex√©cution parall√®le
        if processes == 1:
            # Mode s√©quentiel pour debug
            batch_results = [self.evaluate_batch_worker(args) for args in worker_args]
        else:
            # Mode parall√®le
            with mp.Pool(processes) as pool:
                batch_results = pool.map(self.evaluate_batch_worker, worker_args)
        
        # Combiner tous les r√©sultats
        all_evaluations = []
        for batch_result in batch_results:
            all_evaluations.extend(batch_result)
        
        duration = time.time() - start_time
        
        print(f"\nüéâ √âVALUATION EXHAUSTIVE TERMIN√âE!")
        print(f"üìä R√©sultats:")
        print(f"   - √âvaluations r√©alis√©es: {len(all_evaluations):,}")
        print(f"   - Dur√©e: {duration:.1f}s ({duration/60:.1f} min)")
        if duration > 0:
            print(f"   - Vitesse: {len(all_evaluations)/duration:.1f} √©val/seconde")
        
        # Sauvegarder les r√©sultats
        output_file = self.save_exhaustive_results(all_evaluations, layouts, recipe_groups)
        
        print(f"üíæ R√©sultats sauv√©s: {output_file.name}")
        
        # Afficher quelques statistiques
        self.print_evaluation_statistics(all_evaluations)
        
        return True
    
    def save_exhaustive_results(self, all_evaluations, layouts, recipe_groups):
        """Sauvegarde les r√©sultats de l'√©valuation exhaustive."""
        timestamp = int(time.time())
        output_file = self.evaluation_dir / f"exhaustive_evaluation_{timestamp}.json"
        
        # Statistiques de l'√©valuation
        stats = {
            'total_evaluations': len(all_evaluations),
            'unique_layouts': len(layouts),
            'recipe_groups_used': len(recipe_groups),
            'evaluations_per_layout': len(recipe_groups),
            'avg_estimated_duo_steps': sum(eval_data['estimated_metrics']['estimated_duo_steps'] 
                                         for eval_data in all_evaluations) / len(all_evaluations),
            'avg_estimated_exchanges': sum(eval_data['estimated_metrics']['estimated_exchanges'] 
                                         for eval_data in all_evaluations) / len(all_evaluations)
        }
        
        output_data = {
            'timestamp': timestamp,
            'evaluation_type': 'exhaustive_layout_recipe_groups',
            'statistics': stats,
            'layouts_count': len(layouts),
            'recipe_groups_count': len(recipe_groups),
            'total_evaluations': len(all_evaluations),
            'evaluations': all_evaluations
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        return output_file
    
    def print_evaluation_statistics(self, all_evaluations):
        """Affiche les statistiques d'√©valuation."""
        if not all_evaluations:
            return
        
        print(f"\nüìà STATISTIQUES D'√âVALUATION:")
        print("="*60)
        
        # M√©triques moyennes
        duo_steps = [eval_data['estimated_metrics']['estimated_duo_steps'] for eval_data in all_evaluations]
        exchanges = [eval_data['estimated_metrics']['estimated_exchanges'] for eval_data in all_evaluations]
        cooperation_gains = [eval_data['estimated_metrics']['estimated_cooperation_gain'] for eval_data in all_evaluations]
        
        print(f"üéØ M√âTRIQUES MOYENNES:")
        print(f"   - Steps duo: {sum(duo_steps) / len(duo_steps):.1f}")
        print(f"   - √âchanges: {sum(exchanges) / len(exchanges):.1f}")
        print(f"   - Gain coop√©ration: {sum(cooperation_gains) / len(cooperation_gains):.1f}%")
        
        # Top √©valuations
        top_cooperation = sorted(all_evaluations, 
                               key=lambda x: x['estimated_metrics']['estimated_cooperation_gain'], 
                               reverse=True)[:5]
        
        print(f"\nüèÜ TOP 5 MEILLEURES √âVALUATIONS (gain coop√©ration):")
        for i, eval_data in enumerate(top_cooperation, 1):
            combined_id = eval_data['combined_id']
            gain = eval_data['estimated_metrics']['estimated_cooperation_gain']
            steps = eval_data['estimated_metrics']['estimated_duo_steps']
            exchanges = eval_data['estimated_metrics']['estimated_exchanges']
            recipe_group = eval_data['recipe_group_id']
            
            print(f"   {i}. {combined_id[:20]}...")
            print(f"      Gain: {gain:.1f}%, Steps: {steps}, √âchanges: {exchanges:.1f}")
            print(f"      Groupe recettes: {recipe_group}")

def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description='√âvaluateur exhaustif layout x groupes de recettes'
    )
    
    parser.add_argument(
        '--layout-sample',
        type=int,
        default=None,
        help='Nombre de layouts √† √©valuer (d√©faut: tous)'
    )
    
    parser.add_argument(
        '--recipe-group-sample',
        type=int,
        default=None,
        help='Nombre de groupes de recettes (d√©faut: tous)'
    )
    
    parser.add_argument(
        '--processes',
        type=int,
        default=None,
        help='Nombre de processus parall√®les (d√©faut: auto)'
    )
    
    args = parser.parse_args()
    
    try:
        evaluator = ExhaustiveEvaluator()
        success = evaluator.run_exhaustive_evaluation(
            layout_sample_size=args.layout_sample,
            recipe_group_sample_size=args.recipe_group_sample,
            processes=args.processes
        )
        
        if success:
            print("‚úÖ √âvaluation exhaustive r√©ussie!")
        else:
            print("‚ùå √âchec de l'√©valuation")
            exit(1)
            
    except Exception as e:
        print(f"üí• Erreur: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

if __name__ == "__main__":
    main()
