#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pipeline orchestrateur professionnel pour la g√©n√©ration de layouts Overcooked
- Gestion des 5 √©tapes du pipeline avec reprise possible
- Logging centralis√© et gestion d'erreurs robuste
- Surveillance des ressources et optimisation automatique
- Rapports de progression et m√©triques de performance
"""

import argparse
import json
import logging
import sys
import time
import traceback
import psutil
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import multiprocessing as mp

# Configuration du logging principal
def setup_logging(log_dir: Path, log_level: str = "INFO") -> logging.Logger:
    """Configure le syst√®me de logging centralis√©."""
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Logger principal
    logger = logging.getLogger("pipeline")
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # Supprimer les handlers existants
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Formatter commun
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Handler fichier principal
    file_handler = logging.FileHandler(
        log_dir / f"pipeline_execution_{int(time.time())}.log"
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # Handler console
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Handler erreurs s√©par√©
    error_handler = logging.FileHandler(
        log_dir / f"pipeline_errors_{int(time.time())}.log"
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(formatter)
    logger.addHandler(error_handler)
    
    return logger

def setup_minimal_logging() -> logging.Logger:
    """Configuration logging minimal pour mode production."""
    logger = logging.getLogger("pipeline")
    logger.setLevel(logging.ERROR)
    
    # Handler console uniquement pour erreurs critiques
    console_handler = logging.StreamHandler(sys.stderr)
    console_handler.setLevel(logging.ERROR)
    formatter = logging.Formatter('%(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    return logger

class ResourceMonitor:
    """Moniteur des ressources syst√®me pour optimisation automatique."""
    
    def __init__(self):
        self.start_time = time.time()
        self.initial_memory = psutil.virtual_memory().percent
        self.initial_cpu = psutil.cpu_percent(interval=1)
    
    def get_current_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques actuelles du syst√®me."""
        return {
            'memory_percent': psutil.virtual_memory().percent,
            'memory_available_gb': psutil.virtual_memory().available / (1024**3),
            'cpu_percent': psutil.cpu_percent(interval=1),
            'cpu_count': psutil.cpu_count(),
            'disk_usage_percent': psutil.disk_usage('/').percent,
            'elapsed_time': time.time() - self.start_time
        }
    
    def get_recommended_processes(self) -> int:
        """Recommande le nombre optimal de processus selon les ressources."""
        stats = self.get_current_stats()
        
        # Facteurs limitants
        cpu_limit = max(1, stats['cpu_count'] - 1)  # Laisser 1 CPU libre
        memory_limit = max(1, int(stats['memory_available_gb'] / 2))  # 2GB par processus
        
        # Adapter selon la charge actuelle
        if stats['memory_percent'] > 80:
            memory_limit = max(1, memory_limit // 2)
        
        if stats['cpu_percent'] > 80:
            cpu_limit = max(1, cpu_limit // 2)
        
        return min(cpu_limit, memory_limit, 8)  # Max 8 processus

class PipelineStep:
    """Repr√©sentation d'une √©tape du pipeline."""
    
    def __init__(self, step_id: int, name: str, script_path: str, description: str):
        self.step_id = step_id
        self.name = name
        self.script_path = script_path
        self.description = description
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None
        self.status = "pending"  # pending, running, completed, failed, skipped
        self.error_message = ""
        self.output_files: List[str] = []
        self.metrics: Dict[str, Any] = {}
    
    @property
    def duration(self) -> Optional[float]:
        """Dur√©e d'ex√©cution de l'√©tape."""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None
    
    @property
    def is_completed(self) -> bool:
        """V√©rifie si l'√©tape est termin√©e avec succ√®s."""
        return self.status == "completed"
    
    @property
    def is_failed(self) -> bool:
        """V√©rifie si l'√©tape a √©chou√©."""
        return self.status == "failed"

class ProfessionalPipelineOrchestrator:
    """Orchestrateur principal du pipeline professionnel."""
    
    def __init__(self, config_file: str = "config/pipeline_config.json"):
        """Initialise l'orchestrateur."""
        self.base_dir = Path(__file__).parent.parent
        self.config_file = self.base_dir / config_file
        self.config = self.load_config()
        
        # Dossiers
        self.outputs_dir = self.base_dir / "outputs"
        
        # Mode production
        self.production_mode = self.config.get("production_mode", {})
        self.is_production = self.production_mode.get("enabled", False)
        
        if self.is_production:
            print(f"üè≠ Mode production activ√©")
        
        # Logging adapt√© au mode
        if self.is_production and self.production_mode.get("minimal_logging", True):
            log_level = "ERROR"
            self.logs_dir = None  # Pas de fichiers de log en production
        else:
            log_level = self.config.get("pipeline_config", {}).get("logging", {}).get("level", "INFO")
            self.logs_dir = self.outputs_dir / "logs" / "step_logs"
        
        self.logger = setup_logging(self.logs_dir, log_level) if self.logs_dir else setup_minimal_logging()
        
        # Scripts directory
        self.scripts_dir = self.base_dir / "scripts"
        
        # Monitoring
        self.resource_monitor = ResourceMonitor()
        
        # D√©finition des √©tapes
        self.steps = [
            PipelineStep(0, "Recipe Generation", "0_recipe_generator.py", 
                        "G√©n√©ration des combinaisons de recettes"),
            PipelineStep(1, "Layout Generation", "1_layout_generator.py", 
                        "G√©n√©ration massive des layouts avec formes canoniques"),
            PipelineStep(2, "Layout Evaluation", "2_layout_evaluator.py", 
                        "√âvaluation massive - chaque layout avec chaque groupe de recettes"),
            PipelineStep(3, "Results Analysis", "3_results_analyzer.py", 
                        "Analyse statistique et ranking des r√©sultats d'√©valuation"),
            PipelineStep(4, "Final Selection", "4_final_selector.py", 
                        "S√©lection finale selon crit√®res pond√©r√©s et formatage .layout")
        ]
        
        # √âtat du pipeline
        self.pipeline_start_time: Optional[float] = None
        self.pipeline_end_time: Optional[float] = None
        self.total_errors = 0
        self.execution_metadata: Dict[str, Any] = {}
        
        self.logger.info("üöÄ Pipeline orchestrateur initialis√©")
        self.logger.info(f"üìÅ Base: {self.base_dir}")
        self.logger.info(f"üìã Configuration: {self.config_file}")
        self.logger.info(f"üìä {len(self.steps)} √©tapes configur√©es")
    
    def load_config(self) -> Dict:
        """Charge la configuration du pipeline."""
        if not self.config_file.exists():
            raise FileNotFoundError(f"Configuration non trouv√©e: {self.config_file}")
        
        with open(self.config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def validate_environment(self) -> bool:
        """Valide l'environnement avant ex√©cution."""
        self.logger.info("üîç Validation de l'environnement...")
        
        issues = []
        
        # V√©rifier les scripts
        for step in self.steps:
            script_path = self.scripts_dir / step.script_path
            if not script_path.exists():
                issues.append(f"Script manquant: {script_path}")
        
        # V√©rifier les dossiers de sortie
        if not self.outputs_dir.exists():
            issues.append(f"Dossier outputs manquant: {self.outputs_dir}")
        
        # V√©rifier les ressources syst√®me
        stats = self.resource_monitor.get_current_stats()
        if stats['memory_available_gb'] < 1:
            issues.append(f"M√©moire insuffisante: {stats['memory_available_gb']:.1f}GB disponible")
        
        if stats['disk_usage_percent'] > 90:
            issues.append(f"Espace disque insuffisant: {stats['disk_usage_percent']:.1f}% utilis√©")
        
        # V√©rifier les d√©pendances Python
        try:
            import numpy, matplotlib, seaborn, pandas
        except ImportError as e:
            issues.append(f"D√©pendance Python manquante: {e}")
        
        if issues:
            self.logger.error("‚ùå Probl√®mes d'environnement d√©tect√©s:")
            for issue in issues:
                self.logger.error(f"  ‚Ä¢ {issue}")
            return False
        
        self.logger.info("‚úÖ Environnement valid√©")
        return True
    
    def execute_step(self, step: PipelineStep, force: bool = False) -> bool:
        """Ex√©cute une √©tape du pipeline."""
        
        # V√©rifier si d√©j√† termin√©e (sauf si force)
        if step.is_completed and not force:
            self.logger.info(f"‚è≠Ô∏è √âtape {step.step_id} d√©j√† termin√©e, passage √† la suivante")
            step.status = "skipped"
            return True
        
        self.logger.info(f"üöÄ D√©marrage √©tape {step.step_id}: {step.name}")
        self.logger.info(f"üìù {step.description}")
        
        step.start_time = time.time()
        step.status = "running"
        
        script_path = self.scripts_dir / step.script_path
        
        try:
            # Pr√©parer les arguments
            cmd = [sys.executable, str(script_path)]
            
            # Ajouter arguments sp√©cifiques
            if step.step_id == 1:  # Layout generator
                recommended_processes = self.resource_monitor.get_recommended_processes()
                cmd.extend(["--processes", str(recommended_processes)])
                self.logger.info(f"üîß Processus recommand√©s: {recommended_processes}")
            
            # Configurer les variables d'environnement
            env = {
                **dict(os.environ),
                'PYTHONPATH': str(self.base_dir),
                'PIPELINE_STEP': str(step.step_id),
                'PIPELINE_BASE_DIR': str(self.base_dir)
            }
            
            # Ex√©cuter le script
            self.logger.info(f"üíª Commande: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                cwd=self.base_dir,
                env=env,
                capture_output=True,
                text=True,
                timeout=self.config.get("pipeline_config", {}).get("timeouts", {}).get(f"step_{step.step_id}", 3600)
            )
            
            step.end_time = time.time()
            
            # Analyser le r√©sultat
            if result.returncode == 0:
                step.status = "completed"
                self.logger.info(f"‚úÖ √âtape {step.step_id} termin√©e avec succ√®s ({step.duration:.1f}s)")
                
                # Logger la sortie si demand√©
                if result.stdout and self.logger.level <= logging.DEBUG:
                    self.logger.debug(f"Sortie √©tape {step.step_id}:\n{result.stdout}")
                
                return True
            else:
                step.status = "failed"
                step.error_message = result.stderr or "Erreur inconnue"
                self.logger.error(f"‚ùå √âtape {step.step_id} √©chou√©e (code: {result.returncode})")
                self.logger.error(f"Erreur: {step.error_message}")
                
                if result.stdout:
                    self.logger.error(f"Sortie: {result.stdout}")
                
                return False
                
        except subprocess.TimeoutExpired:
            step.end_time = time.time()
            step.status = "failed"
            step.error_message = "Timeout d√©pass√©"
            self.logger.error(f"‚è∞ √âtape {step.step_id} timeout apr√®s {step.duration:.1f}s")
            return False
            
        except Exception as e:
            step.end_time = time.time()
            step.status = "failed"
            step.error_message = str(e)
            self.logger.error(f"üí• Erreur √©tape {step.step_id}: {e}")
            self.logger.error(traceback.format_exc())
            return False
    
    def run_pipeline(self, start_step: int = 0, end_step: Optional[int] = None, 
                    force: bool = False) -> bool:
        """Ex√©cute le pipeline complet ou partiellement."""
        
        self.pipeline_start_time = time.time()
        
        if end_step is None:
            end_step = len(self.steps) - 1
        
        self.logger.info("üé¨ D√©marrage du pipeline de g√©n√©ration de layouts")
        self.logger.info(f"üìä √âtapes {start_step} √† {end_step}")
        self.logger.info(f"üîÑ Force rebuild: {force}")
        
        # Validation environnement
        if not self.validate_environment():
            self.logger.error("‚ùå √âchec validation environnement")
            return False
        
        # Statistiques initiales
        initial_stats = self.resource_monitor.get_current_stats()
        self.logger.info(f"üíæ M√©moire: {initial_stats['memory_percent']:.1f}% | CPU: {initial_stats['cpu_percent']:.1f}%")
        
        # Ex√©cution des √©tapes
        success = True
        
        for i in range(start_step, end_step + 1):
            if i >= len(self.steps):
                self.logger.warning(f"‚ö†Ô∏è √âtape {i} n'existe pas (max: {len(self.steps) - 1})")
                continue
            
            step = self.steps[i]
            
            # Surveiller les ressources avant chaque √©tape
            current_stats = self.resource_monitor.get_current_stats()
            if current_stats['memory_percent'] > 85:
                self.logger.warning(f"‚ö†Ô∏è M√©moire √©lev√©e: {current_stats['memory_percent']:.1f}%")
            
            # Ex√©cuter l'√©tape
            step_success = self.execute_step(step, force)
            
            if not step_success:
                self.total_errors += 1
                success = False
                
                # D√©cider si continuer ou arr√™ter
                if self.config.get("pipeline_config", {}).get("execution", {}).get("stop_on_error", True):
                    self.logger.error(f"üõë Arr√™t du pipeline suite √† l'√©chec de l'√©tape {i}")
                    break
                else:
                    self.logger.warning(f"‚ö†Ô∏è Continuation malgr√© l'√©chec de l'√©tape {i}")
        
        self.pipeline_end_time = time.time()
        
        # Nettoyage en mode production
        if self.is_production and self.production_mode.get("cleanup_intermediate_files", True):
            self.cleanup_intermediate_files()
        
        # Rapport final
        self.generate_execution_report()
        
        if success:
            self.logger.info("üéâ Pipeline termin√© avec succ√®s!")
        else:
            self.logger.error(f"üí• Pipeline termin√© avec {self.total_errors} erreurs")
        
        return success
    
    def generate_execution_report(self):
        """G√©n√®re un rapport d'ex√©cution d√©taill√©."""
        
        total_duration = self.pipeline_end_time - self.pipeline_start_time if self.pipeline_end_time else 0
        final_stats = self.resource_monitor.get_current_stats()
        
        report = {
            'execution_info': {
                'start_time': self.pipeline_start_time,
                'end_time': self.pipeline_end_time,
                'total_duration': total_duration,
                'total_errors': self.total_errors,
                'execution_date': datetime.now().isoformat()
            },
            'resource_usage': {
                'initial_stats': {
                    'memory_percent': self.resource_monitor.initial_memory,
                    'cpu_percent': self.resource_monitor.initial_cpu
                },
                'final_stats': final_stats
            },
            'steps_summary': []
        }
        
        # R√©sum√© des √©tapes
        for step in self.steps:
            step_info = {
                'step_id': step.step_id,
                'name': step.name,
                'status': step.status,
                'duration': step.duration,
                'error_message': step.error_message
            }
            report['steps_summary'].append(step_info)
        
        # Sauvegarder le rapport
        if self.logs_dir:  # Seulement si logs_dir existe (pas en mode production minimal)
            report_file = self.logs_dir / f"execution_report_{int(time.time())}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"üìÑ Rapport sauvegard√©: {report_file}")
        
        # Rapport texte pour le log
        self.logger.info("üìä Rapport d'ex√©cution:")
        self.logger.info(f"  ‚è±Ô∏è Dur√©e totale: {total_duration:.1f}s ({total_duration/60:.1f}min)")
        self.logger.info(f"  ‚ùå Erreurs: {self.total_errors}")
        self.logger.info(f"  üíæ M√©moire finale: {final_stats['memory_percent']:.1f}%")
        self.logger.info(f"  üíª CPU final: {final_stats['cpu_percent']:.1f}%")
        
        # D√©tail des √©tapes
        for step in self.steps:
            status_emoji = {"completed": "‚úÖ", "failed": "‚ùå", "skipped": "‚è≠Ô∏è", "pending": "‚è∏Ô∏è"}
            emoji = status_emoji.get(step.status, "‚ùì")
            duration_str = f"{step.duration:.1f}s" if step.duration else "N/A"
            self.logger.info(f"  {emoji} √âtape {step.step_id}: {step.name} ({duration_str})")
    
    def cleanup_intermediate_files(self):
        """Nettoie les fichiers interm√©diaires en mode production pour optimiser l'espace disque."""
        if not self.is_production:
            return
        
        essential_outputs = self.production_mode.get("essential_outputs", ["final_layouts", "summary_report"])
        cleanup_dirs = []
        
        self.logger.info("üßπ Nettoyage des fichiers interm√©diaires...")
        
        # D√©terminer les dossiers √† nettoyer
        if "final_layouts" not in essential_outputs:
            cleanup_dirs.append("layouts_selectionnes")
        
        if "summary_report" not in essential_outputs:
            cleanup_dirs.append("selection_analysis")
        
        # Toujours nettoyer les fichiers interm√©diaires lourds
        cleanup_dirs.extend([
            "layouts_generes",      # Layouts bruts g√©n√©r√©s (remplac√©s par s√©lectionn√©s)
            "detailed_evaluation",  # √âvaluations d√©taill√©es (gard√© que r√©sum√©)
            "trajectoires_layouts", # Trajectoires d√©taill√©es
            "recipe_combinations",  # Recettes temporaires
            "analysis_results",     # Analyses interm√©diaires
            "logs"                  # Logs de debug
        ])
        
        files_removed = 0
        space_freed = 0
        
        for cleanup_dir in cleanup_dirs:
            dir_path = self.outputs_dir / cleanup_dir
            if dir_path.exists():
                try:
                    for file_path in dir_path.rglob("*"):
                        if file_path.is_file():
                            file_size = file_path.stat().st_size
                            file_path.unlink()
                            files_removed += 1
                            space_freed += file_size
                    
                    # Supprimer les dossiers vides
                    for dir_path in sorted(dir_path.rglob("*"), key=lambda p: str(p), reverse=True):
                        if dir_path.is_dir() and not any(dir_path.iterdir()):
                            dir_path.rmdir()
                            
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erreur nettoyage {cleanup_dir}: {e}")
        
        space_freed_mb = space_freed / (1024 * 1024)
        self.logger.info(f"üßπ Nettoyage termin√©: {files_removed} fichiers supprim√©s, {space_freed_mb:.1f}MB lib√©r√©s")

def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description="Pipeline orchestrateur professionnel pour layouts Overcooked",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  python run_pipeline.py                    # Pipeline complet
  python run_pipeline.py --start 2          # √Ä partir de l'√©tape 2
  python run_pipeline.py --start 1 --end 3 # √âtapes 1 √† 3 seulement
  python run_pipeline.py --force            # Forcer la re-ex√©cution
  python run_pipeline.py --step 2           # √âtape 2 uniquement
        """
    )
    
    parser.add_argument("--config", default="config/pipeline_config.json",
                       help="Fichier de configuration")
    parser.add_argument("--start", type=int, default=0,
                       help="√âtape de d√©marrage (0-4)")
    parser.add_argument("--end", type=int, 
                       help="√âtape de fin (0-4)")
    parser.add_argument("--step", type=int,
                       help="Ex√©cuter une seule √©tape")
    parser.add_argument("--force", action="store_true",
                       help="Forcer la re-ex√©cution des √©tapes termin√©es")
    parser.add_argument("--dry-run", action="store_true",
                       help="Simulation sans ex√©cution r√©elle")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       default="INFO", help="Niveau de logging")
    
    args = parser.parse_args()
    
    try:
        # Mode √©tape unique
        if args.step is not None:
            args.start = args.step
            args.end = args.step
        
        # Validation des arguments
        if args.start < 0 or args.start > 4:
            print("‚ùå √âtape de d√©marrage invalide (0-4)")
            return 1
        
        if args.end is not None and (args.end < args.start or args.end > 4):
            print("‚ùå √âtape de fin invalide")
            return 1
        
        # Initialiser le pipeline
        orchestrator = ProfessionalPipelineOrchestrator(args.config)
        
        if args.dry_run:
            print("üîç Mode simulation - aucune ex√©cution r√©elle")
            print(f"üìä √âtapes planifi√©es: {args.start} √† {args.end or 4}")
            return 0
        
        # Ex√©cuter le pipeline
        success = orchestrator.run_pipeline(
            start_step=args.start,
            end_step=args.end,
            force=args.force
        )
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interruption utilisateur")
        return 130
    except Exception as e:
        print(f"üí• Erreur critique: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    import os
    exit(main())
