#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test complet du pipeline de g√©n√©ration et d'√©valuation
V√©rifie que tout le processus fonctionne de bout en bout
"""

import json
import gzip
import subprocess
import sys
from pathlib import Path
import time

def run_command(cmd: str, description: str) -> bool:
    """Ex√©cute une commande et retourne le succ√®s"""
    print(f"üîÑ {description}")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Erreur: {e}")
        print(f"   stdout: {e.stdout}")
        print(f"   stderr: {e.stderr}")
        return False

def check_file_exists(file_path: Path, description: str) -> bool:
    """V√©rifie qu'un fichier existe"""
    if file_path.exists():
        print(f"‚úÖ {description}: {file_path}")
        return True
    else:
        print(f"‚ùå Fichier manquant: {file_path}")
        return False

def analyze_results(results_dir: Path) -> dict:
    """Analyse les r√©sultats d'√©valuation"""
    metrics_files = list(results_dir.glob("evaluation_metrics_batch_*.json"))
    
    if not metrics_files:
        return {"error": "Aucun fichier de m√©triques trouv√©"}
    
    total_evaluations = 0
    successful_evaluations = 0
    
    for metrics_file in metrics_files:
        with open(metrics_file, 'r') as f:
            data = json.load(f)
            batch_total = data.get('total_evaluations', 0)
            total_evaluations += batch_total
            
            # Compter les √©valuations r√©ussies (celles avec des m√©triques valides)
            for metric in data.get('metrics', []):
                if metric.get('solo_steps', 0) > 0 and metric.get('duo_steps', 0) > 0:
                    successful_evaluations += 1
    
    return {
        'total_evaluations': total_evaluations,
        'successful_evaluations': successful_evaluations,
        'success_rate': (successful_evaluations / total_evaluations * 100) if total_evaluations > 0 else 0
    }

def main():
    """Test complet du pipeline"""
    print("üß™ TEST COMPLET DU PIPELINE OVERCOOKED")
    print("=" * 50)
    
    base_dir = Path(".")
    test_target = 10  # Nombre de layouts pour le test
    
    # √âtape 1: Nettoyer les anciens fichiers
    print("\n1Ô∏è‚É£ NETTOYAGE")
    for pattern in ["outputs/layouts_generes/*", "outputs/evaluation_results/*"]:
        run_command(f"rm -rf {pattern}", f"Nettoyage: {pattern}")
    
    # √âtape 2: G√©n√©ration des layouts
    print("\n2Ô∏è‚É£ G√âN√âRATION DES LAYOUTS")
    cmd = f"/home/cesar/environnements/overcooked/bin/python scripts/1_layout_generator.py --target {test_target}"
    success = run_command(cmd, f"G√©n√©ration de {test_target} layouts")
    
    if not success:
        print("‚ùå √âchec de la g√©n√©ration")
        return 1
    
    # V√©rification des fichiers g√©n√©r√©s
    layouts_dir = base_dir / "outputs" / "layouts_generes"
    layout_files = list(layouts_dir.glob("layout_batch_*.jsonl.gz"))
    
    if not layout_files:
        print("‚ùå Aucun fichier de layout g√©n√©r√©")
        return 1
    
    print(f"‚úÖ {len(layout_files)} batch(s) de layouts g√©n√©r√©s")
    
    # √âtape 3: Validation de la qualit√© des layouts
    print("\n3Ô∏è‚É£ VALIDATION DE LA QUALIT√â")
    cmd = "/home/cesar/environnements/overcooked/bin/python test/test_layout_quality.py"
    success = run_command(cmd, "Validation de la qualit√© des layouts")
    
    if not success:
        print("‚ùå Validation √©chou√© - layouts de mauvaise qualit√©")
        return 1
    
    print("‚úÖ Tous les layouts sont valides")
    
    # √âtape 4: √âvaluation des layouts
    print("\n4Ô∏è‚É£ √âVALUATION DES LAYOUTS")
    cmd = "/home/cesar/environnements/overcooked/bin/python scripts/2_layout_evaluator.py"
    success = run_command(cmd, "√âvaluation des combinaisons layout+recettes")
    
    if not success:
        print("‚ùå √âchec de l'√©valuation")
        return 1
    
    # V√©rification des r√©sultats d'√©valuation
    results_dir = base_dir / "outputs" / "evaluation_results"
    if not check_file_exists(results_dir, "Dossier de r√©sultats"):
        return 1
    
    # Analyse des r√©sultats
    print("\n5Ô∏è‚É£ ANALYSE DES R√âSULTATS")
    analysis = analyze_results(results_dir)
    
    if "error" in analysis:
        print(f"‚ùå {analysis['error']}")
        return 1
    
    print(f"üìä Statistiques d'√©valuation:")
    print(f"   ‚Ä¢ Total √©valuations: {analysis['total_evaluations']:,}")
    print(f"   ‚Ä¢ √âvaluations r√©ussies: {analysis['successful_evaluations']:,}")
    print(f"   ‚Ä¢ Taux de succ√®s: {analysis['success_rate']:.1f}%")
    
    # √âtape 6: V√©rification de l'int√©grit√© des donn√©es
    print("\n6Ô∏è‚É£ V√âRIFICATION DE L'INT√âGRIT√â")
    
    # Compter les layouts g√©n√©r√©s
    total_layouts = 0
    for layout_file in layout_files:
        with gzip.open(layout_file, 'rt') as f:
            total_layouts += sum(1 for _ in f)
    
    # V√©rifier que le nombre d'√©valuations correspond
    expected_evaluations = total_layouts * 84  # 84 groupes de recettes
    
    if analysis['total_evaluations'] == expected_evaluations:
        print(f"‚úÖ Coh√©rence des donn√©es: {total_layouts} layouts √ó 84 recettes = {expected_evaluations} √©valuations")
    else:
        print(f"‚ö†Ô∏è  Incoh√©rence: {analysis['total_evaluations']} √©valuations trouv√©es, {expected_evaluations} attendues")
    
    # Verdict final
    print("\n" + "=" * 50)
    if analysis['success_rate'] >= 95:  # Au moins 95% de succ√®s
        print("üéâ PIPELINE VALID√â AVEC SUCC√àS! üéâ")
        print(f"   ‚Ä¢ {total_layouts} layouts g√©n√©r√©s et valid√©s")
        print(f"   ‚Ä¢ {analysis['total_evaluations']:,} √©valuations r√©alis√©es")
        print(f"   ‚Ä¢ {analysis['success_rate']:.1f}% de taux de succ√®s")
        return 0
    else:
        print("‚ö†Ô∏è  PIPELINE PARTIELLEMENT FONCTIONNEL")
        print(f"   ‚Ä¢ Taux de succ√®s: {analysis['success_rate']:.1f}% (< 95%)")
        return 1

if __name__ == "__main__":
    exit(main())
