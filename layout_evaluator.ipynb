{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator, LayoutGenerator\n",
    "from overcooked_ai_py.agents.agent import Agent, AgentPair, StayAgent, GreedyAgent\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.planning.planners import MediumLevelActionManager, COUNTERS_MLG_PARAMS, MotionPlanner\n",
    "from game import OvercookedGame, PlanningGame\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et analyse du layout\n",
    "layout_name = \"generation_cesar/layout_cesar_1\"\n",
    "layouts_dir = \"./overcooked_ai_py/data/layouts/generation_cesar/\"\n",
    "\n",
    "print(f\"üèóÔ∏è √âvaluation du layout: {layout_name}\")\n",
    "\n",
    "# Charger le MDP\n",
    "mdp = OvercookedGridworld.from_layout_name(layout_name)\n",
    "print(f\"‚úÖ MDP charg√©: {mdp.width}x{mdp.height}\")\n",
    "\n",
    "# Analyser les √©l√©ments du layout\n",
    "elements = {\n",
    "    'tomato_dispensers': sum(row.count('T') for row in mdp.terrain_mtx),\n",
    "    'onion_dispensers': sum(row.count('O') for row in mdp.terrain_mtx),\n",
    "    'dish_dispensers': sum(row.count('D') for row in mdp.terrain_mtx),\n",
    "    'pots': sum(row.count('P') for row in mdp.terrain_mtx),\n",
    "    'serve_areas': sum(row.count('S') for row in mdp.terrain_mtx),\n",
    "    'players': len(mdp.start_player_positions)\n",
    "}\n",
    "\n",
    "print(f\"üìä Analyse du layout:\")\n",
    "for element, count in elements.items():\n",
    "    print(f\"   {element}: {count}\")\n",
    "\n",
    "# Configuration des param√®tres MLAM\n",
    "counter_params = COUNTERS_MLG_PARAMS\n",
    "if mdp.counter_goals:\n",
    "    counter_params[\"counter_goals\"] = mdp.counter_goals\n",
    "    counter_params[\"counter_drop\"] = mdp.counter_goals\n",
    "    counter_params[\"counter_pickup\"] = mdp.counter_goals\n",
    "\n",
    "print(f\"‚öôÔ∏è Param√®tres MLAM configur√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23321d8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:844\u001b[0m, in \u001b[0;36mMediumLevelActionManager.from_pickle_or_compute\u001b[0;34m(mdp, mlam_params, custom_filename, force_compute, info)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m     mlam \u001b[38;5;241m=\u001b[39m \u001b[43mMediumLevelActionManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mlam\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m!=\u001b[39m mlam_params \u001b[38;5;129;01mor\u001b[39;00m mlam\u001b[38;5;241m.\u001b[39mmdp\u001b[38;5;241m.\u001b[39mterrain_mtx \u001b[38;5;241m!=\u001b[39m mdp\u001b[38;5;241m.\u001b[39mterrain_mtx:\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:832\u001b[0m, in \u001b[0;36mMediumLevelActionManager.from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_file\u001b[39m(filename):\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_saved_action_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/data/planners/__init__.py:6\u001b[0m, in \u001b[0;36mload_saved_action_manager\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_saved_action_manager\u001b[39m(filename):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPLANNERS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         mlp_action_manager \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/data/planners/generation_cesar/layout_cesar_1_am.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m agent_eval \u001b[38;5;241m=\u001b[39m AgentEvaluator\u001b[38;5;241m.\u001b[39mfrom_mdp(mdp, env_params, mlam_params\u001b[38;5;241m=\u001b[39mcounter_params)\n\u001b[1;32m      3\u001b[0m greedyagent1 \u001b[38;5;241m=\u001b[39m GreedyAgent()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mgreedyagent1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_mdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m greedyagent2 \u001b[38;5;241m=\u001b[39m GreedyAgent()\n\u001b[1;32m      6\u001b[0m greedyagent2\u001b[38;5;241m.\u001b[39mset_mdp(mdp)\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/agents/agent.py:313\u001b[0m, in \u001b[0;36mPlanningAgent.set_mdp\u001b[0;34m(self, mdp)\u001b[0m\n\u001b[1;32m    311\u001b[0m     counter_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounter_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdp\u001b[38;5;241m.\u001b[39mcounter_goals\n\u001b[1;32m    312\u001b[0m     counter_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounter_pickup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdp\u001b[38;5;241m.\u001b[39mcounter_goals\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlam \u001b[38;5;241m=\u001b[39m \u001b[43mMediumLevelActionManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pickle_or_compute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounter_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_compute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:854\u001b[0m, in \u001b[0;36mMediumLevelActionManager.from_pickle_or_compute\u001b[0;34m(mdp, mlam_params, custom_filename, force_compute, info)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecomputing planner due to:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMediumLevelActionManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_mlam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlam_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded MediumLevelActionManager from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PLANNERS_DIR, filename)))\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:866\u001b[0m, in \u001b[0;36mMediumLevelActionManager.compute_mlam\u001b[0;34m(filename, mdp, mlam_params, info)\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing MediumLevelActionManager to be saved in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(final_filepath))\n\u001b[1;32m    865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 866\u001b[0m mlam \u001b[38;5;241m=\u001b[39m \u001b[43mMediumLevelActionManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlam_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlam_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds to create mlam\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:823\u001b[0m, in \u001b[0;36mMediumLevelActionManager.__init__\u001b[0;34m(self, mdp, mlam_params)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter_drop \u001b[38;5;241m=\u001b[39m mlam_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounter_drop\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter_pickup \u001b[38;5;241m=\u001b[39m mlam_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounter_pickup\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_motion_planner \u001b[38;5;241m=\u001b[39m \u001b[43mJointMotionPlanner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlam_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmotion_planner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_motion_planner\u001b[38;5;241m.\u001b[39mmotion_planner\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:410\u001b[0m, in \u001b[0;36mJointMotionPlanner.__init__\u001b[0;34m(self, mdp, params, debug)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Graph problem that returns optimal paths from \u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# starting positions to goal positions (without\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# accounting for orientations)\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoint_graph_problem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_graph_from_grid()\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_plans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate_all_plans\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/cesar/python-projects/Overcooked-coop-voice/overcooked_ai_py/planning/planners.py:493\u001b[0m, in \u001b[0;36mJointMotionPlanner._populate_all_plans\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     plan_key \u001b[38;5;241m=\u001b[39m (joint_start_state, joint_goal_state)\n\u001b[0;32m--> 493\u001b[0m     all_plans[plan_key] \u001b[38;5;241m=\u001b[39m (joint_action_list, end_statuses, plan_lengths)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_plans\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# M√©thode 1: √âvaluation avec AgentEvaluator (approche recommand√©e)\n",
    "print(\"üéÆ M√âTHODE 1: √âvaluation avec AgentEvaluator\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Cr√©er l'√©valuateur avec un horizon plus long pour permettre la compl√©tion des recettes\n",
    "env_params = {\"horizon\": 400}  # Horizon plus long\n",
    "agent_eval = AgentEvaluator.from_mdp(mdp, env_params, mlam_params=counter_params)\n",
    "\n",
    "# Cr√©er les agents GreedyAgent\n",
    "print(\"ü§ñ Cr√©ation et configuration des GreedyAgent...\")\n",
    "greedyagent1 = GreedyAgent()\n",
    "greedyagent2 = GreedyAgent()\n",
    "\n",
    "# Configuration avec gestion du temps\n",
    "print(\"‚öôÔ∏è Configuration MLAM (peut prendre du temps)...\")\n",
    "config_start = time.time()\n",
    "\n",
    "greedyagent1.set_mdp(mdp)\n",
    "greedyagent2.set_mdp(mdp)\n",
    "\n",
    "config_time = time.time() - config_start\n",
    "print(f\"‚úÖ Agents configur√©s en {config_time:.2f}s\")\n",
    "\n",
    "# Cr√©er la paire d'agents\n",
    "agent_pair = AgentPair(greedyagent1, greedyagent2)\n",
    "\n",
    "# √âvaluation avec multiple parties\n",
    "num_games = 3\n",
    "print(f\"üöÄ Lancement de {num_games} parties d'√©valuation...\")\n",
    "\n",
    "eval_results = agent_eval.evaluate_agent_pair(\n",
    "    agent_pair, \n",
    "    num_games=num_games, \n",
    "    native_eval=True,\n",
    "    info=True\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìà R√âSULTATS DE L'√âVALUATION:\")\n",
    "print(f\"   ‚è±Ô∏è Temps total: {total_time:.2f}s\")\n",
    "print(f\"   üéÆ Parties jou√©es: {num_games}\")\n",
    "\n",
    "# Analyser les r√©sultats\n",
    "if 'ep_rewards' in eval_results:\n",
    "    rewards = eval_results['ep_rewards']\n",
    "    print(f\"   üìä Scores: {rewards}\")\n",
    "    print(f\"   üìà Score moyen: {sum(rewards)/len(rewards):.2f}\")\n",
    "    print(f\"   üèÜ Meilleur score: {max(rewards)}\")\n",
    "    \n",
    "if 'ep_lengths' in eval_results:\n",
    "    lengths = eval_results['ep_lengths']\n",
    "    print(f\"   üî¢ Dur√©es (steps): {lengths}\")\n",
    "    print(f\"   ‚ö° Dur√©e moyenne: {sum(lengths)/len(lengths):.1f}\")\n",
    "\n",
    "# Afficher d'autres m√©triques si disponibles\n",
    "for key, value in eval_results.items():\n",
    "    if key not in ['ep_rewards', 'ep_lengths'] and isinstance(value, (list, tuple)) and len(value) > 0:\n",
    "        if isinstance(value[0], (int, float)):\n",
    "            print(f\"   üìã {key}: {value}\")\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc044fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 2: √âvaluation avec l'infrastructure OvercookedGame (comme dans app.py)\n",
    "print(\"\\nüéÆ M√âTHODE 2: √âvaluation avec OvercookedGame\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configurer le module game (n√©cessaire pour OvercookedGame)\n",
    "import game\n",
    "game._configure(\n",
    "    max_game_time=1000,\n",
    "    agent_dir=\"./static/assets/agents\"  # R√©pertoire des agents\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Configuration d'une partie avec deux GreedyAgent\n",
    "    game_config = {\n",
    "        'layouts': [layout_name.split('/')[-1]],  # Juste le nom du layout\n",
    "        'layouts_dir': layouts_dir,\n",
    "        'num_players': 2,\n",
    "        'gameTime': 60,  # 60 secondes\n",
    "        'playerZero': 'GreedyAgent',\n",
    "        'playerOne': 'GreedyAgent',\n",
    "        'mdp_params': {},\n",
    "        'curr_trial_in_game': 0,\n",
    "        'showPotential': False,\n",
    "        'randomized': False,\n",
    "        'planning_agent_id': 'GreedyAgent',\n",
    "        'config': {'completion_link': None}\n",
    "    }\n",
    "    \n",
    "    print(\"üì¶ Cr√©ation du jeu avec PlanningGame...\")\n",
    "    game_instance = PlanningGame(**game_config)\n",
    "    \n",
    "    print(\"‚úÖ Jeu cr√©√© avec succ√®s!\")\n",
    "    print(f\"   üèóÔ∏è Layout: {game_instance.curr_layout}\")\n",
    "    print(f\"   ‚öôÔ∏è MDP: {game_instance.mdp.width}x{game_instance.mdp.height}\")\n",
    "    print(f\"   ü§ñ Agents: {len(game_instance.npc_policies)} IA\")\n",
    "    \n",
    "    # Informations sur les agents\n",
    "    for agent_id, agent in game_instance.npc_policies.items():\n",
    "        print(f\"     - {agent_id}: {type(agent).__name__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur avec PlanningGame: {e}\")\n",
    "    print(\"üí° Cette m√©thode n√©cessite une configuration plus complexe pour les nouveaux layouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 3: √âvaluation de multiples layouts\n",
    "print(\"\\nüéÆ M√âTHODE 3: √âvaluation multiple layouts\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_multiple_layouts(layouts_directory, max_layouts=3, games_per_layout=2):\n",
    "    \"\"\"√âvalue plusieurs layouts avec des GreedyAgent\"\"\"\n",
    "    \n",
    "    # Trouver tous les layouts\n",
    "    layout_files = glob.glob(os.path.join(layouts_directory, \"*.layout\"))\n",
    "    layout_names = [os.path.basename(f).replace('.layout', '') for f in layout_files]\n",
    "    \n",
    "    if not layout_names:\n",
    "        print(f\"‚ùå Aucun layout trouv√© dans {layouts_directory}\")\n",
    "        return {}\n",
    "    \n",
    "    layout_names = layout_names[:max_layouts]\n",
    "    print(f\"‚úÖ {len(layout_names)} layouts trouv√©s: {layout_names}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for i, layout_name in enumerate(layout_names, 1):\n",
    "        print(f\"\\nüèóÔ∏è Layout {i}/{len(layout_names)}: {layout_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Charger le MDP\n",
    "            full_layout_path = f\"generation_cesar/{layout_name}\"\n",
    "            layout_mdp = OvercookedGridworld.from_layout_name(full_layout_path)\n",
    "            \n",
    "            # Analyser le layout\n",
    "            layout_elements = {\n",
    "                'tomato_dispensers': sum(row.count('T') for row in layout_mdp.terrain_mtx),\n",
    "                'onion_dispensers': sum(row.count('O') for row in layout_mdp.terrain_mtx),\n",
    "                'dish_dispensers': sum(row.count('D') for row in layout_mdp.terrain_mtx),\n",
    "                'pots': sum(row.count('P') for row in layout_mdp.terrain_mtx),\n",
    "                'serve_areas': sum(row.count('S') for row in layout_mdp.terrain_mtx)\n",
    "            }\n",
    "            \n",
    "            # V√©rifier si le layout est viable\n",
    "            viable = (layout_elements['tomato_dispensers'] > 0 and \n",
    "                     layout_elements['onion_dispensers'] > 0 and\n",
    "                     layout_elements['dish_dispensers'] > 0 and\n",
    "                     layout_elements['pots'] > 0 and\n",
    "                     layout_elements['serve_areas'] > 0)\n",
    "            \n",
    "            print(f\"   üìä √âl√©ments: T={layout_elements['tomato_dispensers']}, \"\n",
    "                  f\"O={layout_elements['onion_dispensers']}, \"\n",
    "                  f\"D={layout_elements['dish_dispensers']}, \"\n",
    "                  f\"P={layout_elements['pots']}, \"\n",
    "                  f\"S={layout_elements['serve_areas']}\")\n",
    "            print(f\"   {'‚úÖ' if viable else '‚ùå'} Layout {'viable' if viable else 'non viable'}\")\n",
    "            \n",
    "            if not viable:\n",
    "                all_results[layout_name] = {\n",
    "                    'viable': False,\n",
    "                    'elements': layout_elements,\n",
    "                    'error': 'Layout manque d\\'√©l√©ments essentiels'\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # √âvaluation avec AgentEvaluator\n",
    "            start_eval = time.time()\n",
    "            \n",
    "            # Param√®tres pour ce layout\n",
    "            layout_counter_params = COUNTERS_MLG_PARAMS.copy()\n",
    "            if layout_mdp.counter_goals:\n",
    "                layout_counter_params[\"counter_goals\"] = layout_mdp.counter_goals\n",
    "                layout_counter_params[\"counter_drop\"] = layout_mdp.counter_goals\n",
    "                layout_counter_params[\"counter_pickup\"] = layout_mdp.counter_goals\n",
    "            \n",
    "            # Cr√©er l'√©valuateur\n",
    "            layout_env_params = {\"horizon\": 300}\n",
    "            layout_eval = AgentEvaluator.from_mdp(layout_mdp, layout_env_params, \n",
    "                                                mlam_params=layout_counter_params)\n",
    "            \n",
    "            # Cr√©er les agents\n",
    "            agent1 = GreedyAgent()\n",
    "            agent2 = GreedyAgent()\n",
    "            agent1.set_mdp(layout_mdp)\n",
    "            agent2.set_mdp(layout_mdp)\n",
    "            pair = AgentPair(agent1, agent2)\n",
    "            \n",
    "            # √âvaluer\n",
    "            print(f\"   üöÄ √âvaluation {games_per_layout} parties...\")\n",
    "            results = layout_eval.evaluate_agent_pair(pair, num_games=games_per_layout, native_eval=True)\n",
    "            \n",
    "            eval_time = time.time() - start_eval\n",
    "            \n",
    "            # Traiter les r√©sultats\n",
    "            layout_results = {\n",
    "                'viable': True,\n",
    "                'elements': layout_elements,\n",
    "                'eval_time': eval_time,\n",
    "                'games_played': games_per_layout\n",
    "            }\n",
    "            \n",
    "            if 'ep_rewards' in results:\n",
    "                rewards = results['ep_rewards']\n",
    "                layout_results.update({\n",
    "                    'scores': rewards,\n",
    "                    'avg_score': sum(rewards) / len(rewards),\n",
    "                    'max_score': max(rewards),\n",
    "                    'min_score': min(rewards)\n",
    "                })\n",
    "                print(f\"   üìà Scores: {rewards} (moy: {layout_results['avg_score']:.1f})\")\n",
    "            \n",
    "            if 'ep_lengths' in results:\n",
    "                lengths = results['ep_lengths']\n",
    "                layout_results.update({\n",
    "                    'lengths': lengths,\n",
    "                    'avg_length': sum(lengths) / len(lengths)\n",
    "                })\n",
    "                print(f\"   ‚è±Ô∏è Dur√©es: {lengths} (moy: {layout_results['avg_length']:.1f})\")\n",
    "            \n",
    "            print(f\"   ‚úÖ √âvaluation termin√©e en {eval_time:.2f}s\")\n",
    "            \n",
    "            all_results[layout_name] = layout_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur: {e}\")\n",
    "            all_results[layout_name] = {\n",
    "                'viable': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Lancer l'√©valuation multiple\n",
    "multi_results = evaluate_multiple_layouts(layouts_dir, max_layouts=3, games_per_layout=2)\n",
    "\n",
    "# R√©sum√© des r√©sultats\n",
    "print(f\"\\nüèÜ R√âSUM√â √âVALUATION MULTIPLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "viable_layouts = [name for name, data in multi_results.items() if data.get('viable', False)]\n",
    "print(f\"‚úÖ Layouts viables: {len(viable_layouts)}/{len(multi_results)}\")\n",
    "\n",
    "if viable_layouts:\n",
    "    # Classement par score moyen\n",
    "    ranked = [(name, data['avg_score']) for name, data in multi_results.items() \n",
    "              if data.get('viable') and 'avg_score' in data]\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nü•á CLASSEMENT PAR PERFORMANCE:\")\n",
    "    for i, (name, score) in enumerate(ranked, 1):\n",
    "        medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "        print(f\"   {medal} {name}: {score:.1f} points\")\n",
    "\n",
    "multi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8739351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 4: Analyse comportementale d√©taill√©e des GreedyAgent\n",
    "print(\"\\nüß† M√âTHODE 4: Analyse comportementale d√©taill√©e\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def detailed_behavioral_analysis(layout_mdp, num_games=1, horizon=200):\n",
    "    \"\"\"Analyse comportementale d√©taill√©e des GreedyAgent\"\"\"\n",
    "    \n",
    "    print(f\"üî¨ Analyse comportementale sur {num_games} partie(s)\")\n",
    "    \n",
    "    # Configuration\n",
    "    counter_params = COUNTERS_MLG_PARAMS.copy()\n",
    "    if layout_mdp.counter_goals:\n",
    "        counter_params[\"counter_goals\"] = layout_mdp.counter_goals\n",
    "        counter_params[\"counter_drop\"] = layout_mdp.counter_goals\n",
    "        counter_params[\"counter_pickup\"] = layout_mdp.counter_goals\n",
    "    \n",
    "    env_params = {\"horizon\": horizon}\n",
    "    evaluator = AgentEvaluator.from_mdp(layout_mdp, env_params, mlam_params=counter_params)\n",
    "    \n",
    "    # Agents\n",
    "    agent1 = GreedyAgent()\n",
    "    agent2 = GreedyAgent()\n",
    "    agent1.set_mdp(layout_mdp)\n",
    "    agent2.set_mdp(layout_mdp)\n",
    "    \n",
    "    # √âvaluation avec informations d√©taill√©es\n",
    "    start_time = time.time()\n",
    "    results = evaluator.evaluate_agent_pair(\n",
    "        AgentPair(agent1, agent2), \n",
    "        num_games=num_games, \n",
    "        native_eval=True\n",
    "    )\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Temps d'√©valuation: {eval_time:.2f}s\")\n",
    "    \n",
    "    # Analyse des r√©sultats\n",
    "    analysis = {\n",
    "        'evaluation_time': eval_time,\n",
    "        'games_analyzed': num_games,\n",
    "        'horizon_used': horizon\n",
    "    }\n",
    "    \n",
    "    # M√©triques de base\n",
    "    if 'ep_rewards' in results:\n",
    "        rewards = results['ep_rewards']\n",
    "        analysis.update({\n",
    "            'total_score': sum(rewards),\n",
    "            'average_score': sum(rewards) / len(rewards),\n",
    "            'score_variance': sum((r - analysis['average_score'])**2 for r in rewards) / len(rewards),\n",
    "            'consistency': analysis['average_score'] / (analysis.get('score_variance', 1) + 1)\n",
    "        })\n",
    "        print(f\"üìä Score total: {analysis['total_score']}\")\n",
    "        print(f\"üìà Score moyen: {analysis['average_score']:.2f}\")\n",
    "        print(f\"üìâ Variance: {analysis['score_variance']:.2f}\")\n",
    "        print(f\"üéØ Consistance: {analysis['consistency']:.2f}\")\n",
    "    \n",
    "    if 'ep_lengths' in results:\n",
    "        lengths = results['ep_lengths']\n",
    "        analysis.update({\n",
    "            'average_length': sum(lengths) / len(lengths),\n",
    "            'completion_rate': sum(1 for l in lengths if l < horizon) / len(lengths)\n",
    "        })\n",
    "        print(f\"‚è±Ô∏è Dur√©e moyenne: {analysis['average_length']:.1f} steps\")\n",
    "        print(f\"‚úÖ Taux de compl√©tion: {analysis['completion_rate']*100:.1f}%\")\n",
    "    \n",
    "    # Analyse des trajectoires si disponibles\n",
    "    if 'mdp_params' in results:\n",
    "        print(f\"üó∫Ô∏è Param√®tres MDP: {len(results['mdp_params'])} configurations\")\n",
    "    \n",
    "    if 'metadatas' in results:\n",
    "        metadatas = results['metadatas']\n",
    "        if metadatas and len(metadatas) > 0:\n",
    "            # Analyser les m√©tadonn√©es pour des insights comportementaux\n",
    "            print(f\"üîç M√©tadonn√©es disponibles: {len(metadatas)} entr√©es\")\n",
    "            \n",
    "            # Exemple d'analyse des m√©tadonn√©es\n",
    "            for i, metadata in enumerate(metadatas[:3]):  # Analyser les 3 premi√®res\n",
    "                if metadata and isinstance(metadata, dict):\n",
    "                    print(f\"   Partie {i+1}: {len(metadata)} √©v√©nements\")\n",
    "                    for key, value in list(metadata.items())[:5]:  # Premiers 5 √©l√©ments\n",
    "                        print(f\"     {key}: {value}\")\n",
    "    \n",
    "    # Calcul de m√©triques avanc√©es\n",
    "    if 'ep_rewards' in results and 'ep_lengths' in results:\n",
    "        rewards = results['ep_rewards']\n",
    "        lengths = results['ep_lengths']\n",
    "        \n",
    "        # Efficacit√© (score par step)\n",
    "        efficiency = [r/l if l > 0 else 0 for r, l in zip(rewards, lengths)]\n",
    "        analysis['average_efficiency'] = sum(efficiency) / len(efficiency)\n",
    "        print(f\"‚ö° Efficacit√© moyenne: {analysis['average_efficiency']:.3f} points/step\")\n",
    "        \n",
    "        # Performance temporelle\n",
    "        if lengths:\n",
    "            fast_games = sum(1 for l in lengths if l < horizon * 0.7)  # Termin√© en moins de 70% du temps\n",
    "            analysis['speed_performance'] = fast_games / len(lengths)\n",
    "            print(f\"üèÉ Performance rapide: {analysis['speed_performance']*100:.1f}% des parties\")\n",
    "    \n",
    "    return analysis, results\n",
    "\n",
    "# Analyse comportementale sur le layout principal\n",
    "behavioral_analysis, detailed_results = detailed_behavioral_analysis(mdp, num_games=2, horizon=300)\n",
    "\n",
    "print(f\"\\nüìã RAPPORT D'ANALYSE COMPORTEMENTALE:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for metric, value in behavioral_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {metric}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"   {metric}: {value}\")\n",
    "\n",
    "behavioral_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overcooked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
