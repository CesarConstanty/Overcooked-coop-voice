#!/usr/bin/env python3
"""
evaluation_layout_simple.py

Version simplifi√©e de l'√©valuateur de layouts pour √©viter les probl√®mes de compatibilit√©.
Utilise RandomAgent comme substitut et simule des temps de compl√©tion bas√©s sur la structure.
"""

import os
import glob
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Optional

from overcooked_ai_py.agents.benchmarking import AgentEvaluator
from overcooked_ai_py.agents.agent import RandomAgent, AgentPair
from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld
from overcooked_ai_py.planning.planners import NO_COUNTERS_PARAMS


class SimpleLayoutEvaluator:
    """
    Classe simplifi√©e pour l'√©valuation rapide de layouts.
    Utilise des agents plus simples pour √©viter les probl√®mes de compatibilit√©.
    """
    
    def __init__(self, layouts_directory: str = "./overcooked_ai_py/data/layouts/generation_cesar/", 
                 horizon: int = 800, num_games_per_layout: int = 3):
        """
        Initialise l'√©valuateur simplifi√©.
        
        Args:
            layouts_directory: R√©pertoire contenant les fichiers .layout
            horizon: Nombre maximum de steps par partie
            num_games_per_layout: Nombre de parties √† jouer par layout
        """
        self.layouts_directory = layouts_directory
        self.horizon = horizon
        self.num_games_per_layout = num_games_per_layout
        self.results = {}
        
        print(f"üéÆ √âVALUATEUR SIMPLIFI√â DE LAYOUTS OVERCOOKED")
        print(f"üìÅ R√©pertoire: {layouts_directory}")
        print(f"‚è±Ô∏è Horizon: {horizon} steps")
        print(f"üéØ Parties par layout: {num_games_per_layout}")
        print(f"ü§ñ Utilisation d'agents RandomAgent pour √©viter les blocages")
    
    def discover_layouts(self) -> List[str]:
        """D√©couvre tous les fichiers .layout dans le r√©pertoire."""
        layout_files = glob.glob(os.path.join(self.layouts_directory, "*.layout"))
        layout_names = [os.path.basename(f).replace('.layout', '') for f in layout_files]
        layout_names.sort()
        
        print(f"‚úÖ {len(layout_names)} layouts d√©couverts: {layout_names}")
        return layout_names
    
    def analyze_layout_structure(self, mdp: OvercookedGridworld) -> Dict:
        """Analyse la structure d'un layout."""
        elements = {
            'width': mdp.width,
            'height': mdp.height,
            'tomato_dispensers': sum(row.count('T') for row in mdp.terrain_mtx),
            'onion_dispensers': sum(row.count('O') for row in mdp.terrain_mtx),
            'dish_dispensers': sum(row.count('D') for row in mdp.terrain_mtx),
            'pots': sum(row.count('P') for row in mdp.terrain_mtx),
            'serve_areas': sum(row.count('S') for row in mdp.terrain_mtx),
            'players': len(mdp.start_player_positions),
            'walls': sum(row.count('X') for row in mdp.terrain_mtx),
            'counters': sum(row.count('-') for row in mdp.terrain_mtx)
        }
        
        # V√©rifier la viabilit√©
        viable = (
            elements['tomato_dispensers'] > 0 and 
            elements['onion_dispensers'] > 0 and
            elements['dish_dispensers'] > 0 and
            elements['pots'] > 0 and
            elements['serve_areas'] > 0 and
            elements['players'] >= 2
        )
        
        elements['viable'] = viable
        elements['complexity_score'] = (
            elements['tomato_dispensers'] + elements['onion_dispensers'] + 
            elements['dish_dispensers'] + elements['pots'] + elements['serve_areas']
        )
        
        return elements
    
    def analyze_recipes(self, mdp: OvercookedGridworld) -> Dict:
        """Analyse les recettes demand√©es."""
        recipes_info = {
            'recipes': [],
            'num_recipes': 0,
            'total_orders': 0,
            'requires_onions': False,
            'requires_tomatoes': False
        }
        
        if hasattr(mdp, 'start_all_orders') and mdp.start_all_orders:
            recipes_info['total_orders'] = len(mdp.start_all_orders)
            
            for order in mdp.start_all_orders:
                if 'ingredients' in order:
                    ingredients = order['ingredients']
                    recipes_info['recipes'].append({
                        'ingredients': ingredients,
                        'onion_count': ingredients.count('onion'),
                        'tomato_count': ingredients.count('tomato'),
                        'value': order.get('value', None)
                    })
                    
                    if 'onion' in ingredients:
                        recipes_info['requires_onions'] = True
                    if 'tomato' in ingredients:
                        recipes_info['requires_tomatoes'] = True
            
            recipes_info['num_recipes'] = len(set(str(r['ingredients']) for r in recipes_info['recipes']))
        
        return recipes_info
    
    def estimate_completion_time(self, structure: Dict, recipes: Dict) -> Dict:
        """
        Estime le temps de compl√©tion bas√© sur la structure du layout.
        Utilis√© comme substitut quand l'√©valuation compl√®te √©choue.
        """
        # Calcul bas√© sur la complexit√© du layout
        base_time = 200  # Temps de base
        
        # Facteurs qui influencent le temps
        distance_factor = (structure['width'] + structure['height']) * 5
        ingredient_factor = recipes['total_orders'] * 50
        complexity_factor = structure['complexity_score'] * 30
        
        # P√©nalit√©s pour les layouts difficiles
        if structure['pots'] < recipes['total_orders']:
            complexity_factor += 100  # Pas assez de casseroles
        
        if structure['onion_dispensers'] < 1 and recipes['requires_onions']:
            complexity_factor += 200  # Manque d'oignons
            
        if structure['tomato_dispensers'] < 1 and recipes['requires_tomatoes']:
            complexity_factor += 200  # Manque de tomates
        
        estimated_time = base_time + distance_factor + ingredient_factor + complexity_factor
        
        # Simulation de plusieurs parties avec variation
        games_times = []
        for i in range(self.num_games_per_layout):
            variation = np.random.normal(0, estimated_time * 0.1)  # 10% de variation
            game_time = max(100, estimated_time + variation)  # Minimum 100 steps
            game_time = min(self.horizon, game_time)  # Maximum horizon
            games_times.append(int(game_time))
        
        # Simulation de scores (bas√© sur compl√©tion)
        scores = []
        for game_time in games_times:
            if game_time < self.horizon * 0.8:  # Compl√©tion r√©ussie
                score = recipes['total_orders'] * 20  # 20 points par recette
            else:
                score = max(0, recipes['total_orders'] * 10 - (game_time - 400))  # Score partiel
            scores.append(max(0, score))
        
        return {
            'estimated_times': games_times,
            'estimated_scores': scores,
            'estimated_completion_rate': sum(1 for t in games_times if t < self.horizon * 0.8) / len(games_times),
            'average_time': np.mean(games_times),
            'method': 'structural_estimation'
        }
    
    def evaluate_single_layout(self, layout_name: str) -> Dict:
        """√âvalue un seul layout avec m√©thode simplifi√©e."""
        print(f"\nüèóÔ∏è √âvaluation: {layout_name}")
        print("-" * 50)
        
        start_time = time.time()
        
        try:
            # Charger le MDP
            full_layout_path = f"generation_cesar/{layout_name}"
            mdp = OvercookedGridworld.from_layout_name(full_layout_path)
            
            # Analyser la structure
            structure_analysis = self.analyze_layout_structure(mdp)
            print(f"üìä Structure: {structure_analysis['width']}x{structure_analysis['height']}, "
                  f"T={structure_analysis['tomato_dispensers']}, "
                  f"O={structure_analysis['onion_dispensers']}, "
                  f"P={structure_analysis['pots']}, "
                  f"S={structure_analysis['serve_areas']}")
            
            # Analyser les recettes
            recipes_info = self.analyze_recipes(mdp)
            print(f"üç≤ Recettes: {recipes_info['num_recipes']} types, "
                  f"Total: {recipes_info['total_orders']} commandes")
            for recipe in recipes_info['recipes'][:3]:
                print(f"   - {recipe['ingredients']} (valeur: {recipe.get('value', 'inconnue')})")
            
            # V√©rifier la viabilit√©
            if not structure_analysis['viable']:
                print("‚ùå Layout non viable - √©l√©ments manquants")
                return {
                    'layout_name': layout_name,
                    'viable': False,
                    'structure': structure_analysis,
                    'recipes': recipes_info,
                    'error': 'Layout manque d\'√©l√©ments essentiels',
                    'evaluation_time': time.time() - start_time
                }
            
            print("‚úÖ Layout viable")
            
            # Tentative d'√©valuation avec RandomAgent
            try:
                print("ü§ñ Tentative d'√©valuation avec RandomAgent...")
                env_params = {"horizon": self.horizon}
                mlam_params = NO_COUNTERS_PARAMS.copy()  # Param√®tres simplifi√©s
                mlam_params["max_iter"] = 2  # R√©duire les it√©rations
                
                evaluator = AgentEvaluator.from_mdp(mdp, env_params, mlam_params=mlam_params)
                
                evaluation_results = evaluator.evaluate_random_pair(
                    num_games=self.num_games_per_layout,
                    native_eval=True
                )
                
                print("‚úÖ √âvaluation RandomAgent r√©ussie")
                real_evaluation = True
                
            except Exception as e:
                print(f"‚ö†Ô∏è √âvaluation RandomAgent √©chou√©e: {e}")
                print("üßÆ Utilisation de l'estimation structurelle...")
                evaluation_results = self.estimate_completion_time(structure_analysis, recipes_info)
                real_evaluation = False
            
            eval_time = time.time() - start_time
            
            # Traiter les r√©sultats
            results = {
                'layout_name': layout_name,
                'viable': True,
                'structure': structure_analysis,
                'recipes': recipes_info,
                'timing': {
                    'total_evaluation_time': eval_time,
                },
                'games_played': self.num_games_per_layout,
                'evaluation_method': 'real_agent' if real_evaluation else 'structural_estimation'
            }
            
            # Analyser les scores et temps
            if real_evaluation and 'ep_rewards' in evaluation_results:
                rewards = evaluation_results['ep_rewards']
                if hasattr(rewards, 'tolist'):
                    rewards = rewards.tolist()
                if isinstance(rewards, list) and len(rewards) > 0 and isinstance(rewards[0], list):
                    rewards = [item for sublist in rewards for item in sublist]
                rewards = [float(r) for r in rewards if isinstance(r, (int, float))]
                
                if rewards:
                    results['scores'] = {
                        'raw_scores': rewards,
                        'average_score': np.mean(rewards),
                        'max_score': max(rewards),
                        'min_score': min(rewards)
                    }
                    print(f"üìà Points: {rewards} (moy: {results['scores']['average_score']:.1f})")
            elif not real_evaluation:
                # Utiliser les scores estim√©s
                rewards = evaluation_results['estimated_scores']
                results['scores'] = {
                    'raw_scores': rewards,
                    'average_score': np.mean(rewards),
                    'max_score': max(rewards),
                    'min_score': min(rewards)
                }
                print(f"üìà Points estim√©s: {rewards} (moy: {results['scores']['average_score']:.1f})")
            
            # Analyser les temps de compl√©tion
            if real_evaluation and 'ep_lengths' in evaluation_results:
                lengths = evaluation_results['ep_lengths']
                if hasattr(lengths, 'tolist'):
                    lengths = lengths.tolist()
                if isinstance(lengths, list) and len(lengths) > 0 and isinstance(lengths[0], list):
                    lengths = [item for sublist in lengths for item in sublist]
                lengths = [int(l) for l in lengths if isinstance(l, (int, float))]
            else:
                # Utiliser les temps estim√©s
                lengths = evaluation_results['estimated_times']
            
            if lengths:
                completed_games = [l for l in lengths if l < self.horizon]
                
                results['completion'] = {
                    'raw_lengths': lengths,
                    'average_length': np.mean(lengths),
                    'completed_games_count': len(completed_games),
                    'completion_rate': len(completed_games) / len(lengths),
                    'average_completion_time': np.mean(completed_games) if completed_games else None,
                    'fastest_completion': min(completed_games) if completed_games else None,
                    'slowest_completion': max(completed_games) if completed_games else None
                }
                
                print(f"‚è±Ô∏è Dur√©es: {lengths} steps")
                print(f"‚úÖ Parties compl√©t√©es: {len(completed_games)}/{len(lengths)} ({results['completion']['completion_rate']*100:.1f}%)")
                
                if completed_games:
                    print(f"üèÅ Temps de compl√©tion moyen: {results['completion']['average_completion_time']:.1f} steps")
                    print(f"üöÄ Plus rapide: {results['completion']['fastest_completion']} steps")
                    print(f"üêå Plus lent: {results['completion']['slowest_completion']} steps")
                    
                    # M√âTRIQUE PRINCIPALE pour le classement
                    results['primary_metric'] = results['completion']['average_completion_time']
                    results['primary_metric_name'] = "Temps de compl√©tion moyen (steps)"
                else:
                    print(f"‚ùå AUCUNE RECETTE COMPL√âT√âE")
                    results['primary_metric'] = self.horizon + 100
                    results['primary_metric_name'] = "Temps de compl√©tion (p√©nalis√© - aucune compl√©tion)"
            
            print(f"‚úÖ √âvaluation termin√©e en {eval_time:.1f}s")
            return results
            
        except Exception as e:
            error_time = time.time() - start_time
            print(f"‚ùå Erreur lors de l'√©valuation: {e}")
            return {
                'layout_name': layout_name,
                'viable': False,
                'error': str(e),
                'evaluation_time': error_time
            }
    
    def evaluate_all_layouts(self) -> Dict:
        """√âvalue tous les layouts d√©couverts."""
        layout_names = self.discover_layouts()
        
        if not layout_names:
            print("‚ùå Aucun layout trouv√© dans le r√©pertoire sp√©cifi√©")
            return {}
        
        print(f"\nüöÄ D√âBUT √âVALUATION DE {len(layout_names)} LAYOUTS")
        print("=" * 60)
        
        start_time = time.time()
        
        for i, layout_name in enumerate(layout_names, 1):
            print(f"\n[{i}/{len(layout_names)}] {layout_name}")
            layout_result = self.evaluate_single_layout(layout_name)
            self.results[layout_name] = layout_result
        
        total_time = time.time() - start_time
        
        # G√©n√©rer un rapport de synth√®se
        self.generate_summary_report(total_time)
        
        return self.results
    
    def generate_summary_report(self, total_evaluation_time: float):
        """G√©n√®re un rapport de synth√®se des √©valuations."""
        print(f"\nüèÜ RAPPORT DE SYNTH√àSE")
        print("=" * 60)
        
        # Statistiques g√©n√©rales
        total_layouts = len(self.results)
        viable_layouts = [name for name, data in self.results.items() if data.get('viable', False)]
        failed_layouts = [name for name, data in self.results.items() if not data.get('viable', False)]
        
        print(f"üìä Layouts analys√©s: {total_layouts}")
        print(f"‚úÖ Layouts viables: {len(viable_layouts)}")
        print(f"‚ùå Layouts √©chou√©s: {len(failed_layouts)}")
        print(f"‚è±Ô∏è Temps total: {total_evaluation_time:.1f}s ({total_evaluation_time/60:.1f}min)")
        
        if failed_layouts:
            print(f"\n‚ùå Layouts √©chou√©s: {failed_layouts}")
        
        if viable_layouts:
            # Classement par temps de compl√©tion
            completion_layouts = [(name, self.results[name]['primary_metric'], 
                                 self.results[name]['completion']['completion_rate']) 
                                for name in viable_layouts 
                                if 'primary_metric' in self.results[name]]
            
            if completion_layouts:
                completion_layouts.sort(key=lambda x: x[1])
                
                print(f"\nüèÅ CLASSEMENT PAR TEMPS DE COMPL√âTION (OBJECTIF PRINCIPAL):")
                for i, (name, time_metric, completion_rate) in enumerate(completion_layouts, 1):
                    medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i:2d}."
                    method = self.results[name].get('evaluation_method', 'inconnue')
                    if time_metric < self.horizon:
                        print(f"   {medal} {name}: {time_metric:.0f} steps ({completion_rate*100:.0f}% r√©ussite) [{method}]")
                    else:
                        print(f"   {medal} {name}: √âCHEC ({completion_rate*100:.0f}% r√©ussite) [{method}]")
        
        print(f"\nüí° Les √©valuations marqu√©es [structural_estimation] sont des estimations")
        print(f"   bas√©es sur la structure du layout en cas d'√©chec de l'√©valuation compl√®te.")
    
    def save_results(self, filename: str = "layout_evaluation_results_simple.json"):
        """Sauvegarde les r√©sultats dans un fichier JSON."""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"üíæ R√©sultats sauvegard√©s dans {filename}")


def main():
    """Fonction principale pour lancer l'√©valuation simplifi√©e."""
    print("üéÆ √âVALUATEUR SIMPLIFI√â DE LAYOUTS OVERCOOKED")
    print("üß† Exp√©riences en sciences cognitives")
    print("=" * 50)
    
    # Configuration
    layouts_dir = "./overcooked_ai_py/data/layouts/generation_cesar/"
    
    # V√©rifier que le r√©pertoire existe
    if not os.path.exists(layouts_dir):
        print(f"‚ùå R√©pertoire {layouts_dir} non trouv√©")
        return
    
    # Cr√©er l'√©valuateur
    evaluator = SimpleLayoutEvaluator(
        layouts_directory=layouts_dir,
        horizon=800,  # Horizon r√©duit pour √©viter les timeouts
        num_games_per_layout=3  # Moins de parties pour plus de rapidit√©
    )
    
    # Lancer l'√©valuation
    results = evaluator.evaluate_all_layouts()
    
    # Sauvegarder les r√©sultats
    evaluator.save_results("layout_evaluation_results_simple.json")
    
    print(f"\nüéØ √âvaluation termin√©e! {len(results)} layouts analys√©s.")
    print("üíæ R√©sultats sauvegard√©s dans layout_evaluation_results_simple.json")


if __name__ == "__main__":
    main()
